<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Atomicity]]></title>
    <url>%2F2019%2F08%2F31%2FAtomicity%2F</url>
    <content type="text"><![CDATA[Atomicity of read and write a file lib pip install atomicwrites 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208import contextlib # 上下文管理器import ioimport osimport sysimport tempfiletry: import fcntl # 与linux下的文件锁头文件&lt;fcntl.h&gt;一个作用except ImportError: fcntl = None__version__ = '1.3.0'PY2 = sys.version_info[0] == 2 # 获取python版本text_type = unicode if PY2 else str # noqa # 忽略代码质量警告def _path_to_unicode(x): # 文件路径的编码转换 if not isinstance(x, text_type): return x.decode(sys.getfilesystemencoding()) return xDEFAULT_MODE = "wb" if PY2 else "w" # 默认操作模式_proper_fsync = os.fsync # 强制将文件描述符为fd的文件写入硬盘# 主要定义_replace_atomic和_move_atomic函数if sys.platform != 'win32': # Unix平台 if hasattr(fcntl, 'F_FULLFSYNC'): def _proper_fsync(fd): # https://lists.apple.com/archives/darwin-dev/2005/Feb/msg00072.html # https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man2/fsync.2.html # https://github.com/untitaker/python-atomicwrites/issues/6 fcntl.fcntl(fd, fcntl.F_FULLFSYNC) # 完全同步 def _sync_directory(directory): # 同步文件夹 # Ensure that filenames are written to disk fd = os.open(directory, 0) try: _proper_fsync(fd) finally: os.close(fd) def _replace_atomic(src, dst): os.rename(src, dst) _sync_directory(os.path.normpath(os.path.dirname(dst))) def _move_atomic(src, dst): os.link(src, dst) os.unlink(src) src_dir = os.path.normpath(os.path.dirname(src)) dst_dir = os.path.normpath(os.path.dirname(dst)) _sync_directory(dst_dir) if src_dir != dst_dir: _sync_directory(src_dir)else: # Win32平台 from ctypes import windll, WinError _MOVEFILE_REPLACE_EXISTING = 0x1 _MOVEFILE_WRITE_THROUGH = 0x8 _windows_default_flags = _MOVEFILE_WRITE_THROUGH def _handle_errors(rv): if not rv: raise WinError() def _replace_atomic(src, dst): _handle_errors(windll.kernel32.MoveFileExW( _path_to_unicode(src), _path_to_unicode(dst), _windows_default_flags | _MOVEFILE_REPLACE_EXISTING )) def _move_atomic(src, dst): _handle_errors(windll.kernel32.MoveFileExW( _path_to_unicode(src), _path_to_unicode(dst), _windows_default_flags ))# 再裹了一层def replace_atomic(src, dst): # 重写文件 ''' Move ``src`` to ``dst``. If ``dst`` exists, it will be silently overwritten. Both paths must reside on the same filesystem for the operation to be atomic. ''' return _replace_atomic(src, dst)def move_atomic(src, dst): # 更改文件 ''' Move ``src`` to ``dst``. There might a timewindow where both filesystem entries exist. If ``dst`` already exists, :py:exc:`FileExistsError` will be raised. Both paths must reside on the same filesystem for the operation to be atomic. ''' return _move_atomic(src, dst)class AtomicWriter(object): # 主要功能class ''' A helper class for performing atomic writes. Usage:: with AtomicWriter(path).open() as f: f.write(...) :param path: The destination filepath. May or may not exist. :param mode: The filemode for the temporary file. This defaults to `wb` in Python 2 and `w` in Python 3. :param overwrite: If set to false, an error is raised if ``path`` exists. Errors are only raised after the file has been written to. Either way, the operation is atomic. If you need further control over the exact behavior, you are encouraged to subclass. ''' def __init__(self, path, mode=DEFAULT_MODE, overwrite=False, **open_kwargs): if 'a' in mode: raise ValueError( 'Appending to an existing file is not supported, because that ' 'would involve an expensive `copy`-operation to a temporary ' 'file. Open the file in normal `w`-mode and copy explicitly ' 'if that\'s what you\'re after.' ) if 'x' in mode: raise ValueError('Use the `overwrite`-parameter instead.') if 'w' not in mode: raise ValueError('AtomicWriters can only be written to.') self._path = path self._mode = mode self._overwrite = overwrite self._open_kwargs = open_kwargs def open(self): ''' Open the temporary file. ''' return self._open(self.get_fileobject) @contextlib.contextmanager # 上下文管理 with def _open(self, get_fileobject): f = None # make sure f exists even if get_fileobject() fails try: success = False with get_fileobject(**self._open_kwargs) as f: yield f self.sync(f) self.commit(f) success = True finally: if not success: try: self.rollback(f) except Exception: pass def get_fileobject(self, suffix="", prefix=tempfile.template, dir=None, **kwargs): '''Return the temporary file to use.''' if dir is None: dir = os.path.normpath(os.path.dirname(self._path)) descriptor, name = tempfile.mkstemp(suffix=suffix, prefix=prefix, dir=dir) # io.open() will take either the descriptor or the name, but we need # the name later for commit()/replace_atomic() and couldn't find a way # to get the filename from the descriptor. os.close(descriptor) kwargs['mode'] = self._mode kwargs['file'] = name return io.open(**kwargs) def sync(self, f): # 负责在提交之前清除尽可能多的文件缓存 '''responsible for clearing as many file caches as possible before commit''' f.flush() _proper_fsync(f.fileno()) def commit(self, f): # 将临时文件移动到目标位置 '''Move the temporary file to the target location.''' if self._overwrite: replace_atomic(f.name, self._path) else: move_atomic(f.name, self._path) def rollback(self, f): # 清除临时资源 '''Clean up all temporary resources.''' os.unlink(f.name)def atomic_write(path, writer_cls=AtomicWriter, **cls_kwargs): # 回调函数 ''' Simple atomic writes. This wraps :py:class:`AtomicWriter`:: with atomic_write(path) as f: f.write(...) :param path: The target path to write to. :param writer_cls: The writer class to use. This parameter is useful if you subclassed :py:class:`AtomicWriter` to change some behavior and want to use that new subclass. Additional keyword arguments are passed to the writer class. See :py:class:`AtomicWriter`. ''' return writer_cls(path, **cls_kwargs).open() temfile code using temp file find have temp file if have, so pre step err and del pre temp file, else continue next step create a new temp file and write to temp file change file name when write over123456789101112131415161718192021def write_to_file(path, file_name, content): """ path: the path to the folder where the file is located file_name: will be created file name content: type is str, will be write content """ import os try: if os.path.exists(os.path.join(path, "temp.txt")): print("have temp file in path, because program interrupted in pre \ step on running.") os.remove(os.path.join(path, "temp.txt")) with open(os.path.join(path, "temp.txt")) as f: f.write(content) # make sure that all data is on disk # see https://stackoverflow.com/questions/7433057/is-rename-without-fsync-safe f.flush() os.fsync(f.fileno()) os.rename(os.path.join(path, "temp.txt"), os.path.join(path, file_name)) except Exception as err: print("Error: ", err) Multi-thread lock-free simultaneous reading and writing of an object12345678910111213141516171819202122232425// Many Buffertemplate &lt;typename DataType&gt;class Var &#123; public: void Var (DataType data) &#123; p = new DataType(); &#125; void ~Var() &#123; delete *p; &#125; void write(DataType data) &#123; DataType* temp = new DataType(); *temp = data; // write time is long swap(temp, p); &#125; DataType* read() &#123; return p; &#125; private: DataType* p; // read&#125;;]]></content>
      <categories>
        <category>C++</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python直接根据URL直接调用远程代码]]></title>
    <url>%2F2019%2F08%2F05%2FPython%E7%9B%B4%E6%8E%A5%E6%A0%B9%E6%8D%AEURL%E7%9B%B4%E6%8E%A5%E8%B0%83%E7%94%A8%E8%BF%9C%E7%A8%8B%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#!/usr/bin/env python3# -*- coding:utf-8 -*-# =============================================================================# File Name: load_code.py# Author: DaiDai# Mail: daidai4269@aliyun.com# Created Time: Fri 02 Aug 2019 04:41:19 PM CST# ============================================================================="""TODO: proxyexample:from load_code import LoadCodeLoadCode("xxx/xxx") # py code url, xxx is a url"""import osimport sysimport requestsif sys.version &lt; '3': VERSION = 2else: VERSION = 3GITHUB = "https://raw.githubusercontent.com/"GITEE = "" # TODOGITLAB = "" # TODOclass LoadCode(object): def __init__(self, url): url = os.path.join(GITHUB, url) code = requests.get(url).text if VERSION == 3: # for print exec("from __future__ import print_function") exec(code)if __name__ == "__main__": pass]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3.x load module path error]]></title>
    <url>%2F2019%2F07%2F21%2FPython_load_module_path_config%2F</url>
    <content type="text"><![CDATA[Program execute display like this error: ImportError: No module named &#39;xxx&#39; also you used like this code: from .xxx import xxx or from ..xxx import xxx. Example: module/ __init__.py A/ __init__.py a.py12def func(): print("A.a") B/ __init__.py b.py1234from ..A.a import funcfunc() main.py 1from module.B import * This code will display error when running python3 b.py. I think maybe Python3 Interpreter not support this grammar.And code will display error when running python3 main.py. if you use sys.paths.append(&quot;../&quot;) or sys.paths.append(&quot;./&quot;) like this code, still a mistake. Because program when load module abspath is main.py file, not b.py file. And C++ /C language is not such. Solve Method __file__ return current file path os.path.abspath(path) return file abspath os.path.dirname() return folder abspath os.path.join() join path load upward one level catalog path: os.path.abspath(os.path.join(os.path.dirname(__file__), &quot;../&quot;))load upward two level catalog path: os.path.abspath(os.path.join(os.path.dirname(__file__), &quot;../../&quot;))]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Header File]]></title>
    <url>%2F2019%2F07%2F13%2FC%2B%2B_header_file%2F</url>
    <content type="text"><![CDATA[*.h file include state variable(header file can define variable, but should be do it) state class / function / templete const / extern inline function exclude using namespace std; or namespace xxx {}, should be used std::cout define anything hello.hpp 123456#ifndef HELLO_H#define HELLO_Hvoid hello();#endif // HELLO_H hello.cpp 12345#include &lt;iostream&gt;void hello() &#123; std::cout &lt;&lt; "hello" &lt;&lt; std::endl;&#125; main.cpp 12345#include "hello.hpp"int main() &#123; hello();&#125; bash 123&gt;&gt; g++ -std=c++11 main.cpp hello.cpp&gt;&gt; ./a.outhello]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Summary]]></title>
    <url>%2F2019%2F07%2F04%2FC%2B%2B_Summary%2F</url>
    <content type="text"><![CDATA[Basic Grammar annotation // /* note */ variable, constant type bool, char, int, float, double, wchar_t, void decorate: signed, unsigned, long, short, const, volatile, restrict define global, local storage class: auto, register, static, extern, mutable, thread_local (C++ 11) operator arithmetic: +, -, *, /, %, ++, -- relational: ==, !=, &gt;, &lt;, &gt;=, &lt;= logical: &amp;&amp;, ||, ! bitwise: , &amp;, |, ^, ~, &lt;&lt;, &gt;&gt; assignment: , =, +=, -=, *=, /=, %=, &lt;&lt;=, &gt;&gt;=, &amp;=, ^=, |= miscellaneous: , sizeof, Condition ? X : Y, ,, ., -&gt;, Cast: (const_cast&lt;type&gt; (expr), dynamic_cast&lt;type&gt; (expr), reinterpret_cast&lt;type&gt; (expr), static_cast&lt;type&gt; (expr)), &amp;, * loop: for, while, do while; control sentence: break, continue, goto judge: if, if else, switch, ? : function: define, statement, call funciton by (value, pointer, quote) Lambda function: [capture](parameters) -&gt; return_type {body} number, array, string pointer: (Null pointer, pointer arithmetic operation, pointer array, a pointer to a pointer, pointer parameter to function, return pointer from function), quote: (quote parameter to function, return quote from function) date, time data struct: struct, typedef, enum OOP class member function, call decorate: (public, private, protected), constructor function, destructive function, copy constructor function, friend function, inline function, static function, pointer to class, this pointer inherit: derived class, multiple inheritance function overloading, operation overloading polymorphic: virtual function, pure virtual function error deal: try catch, throw dynamic memory: (new, delete), new: 1d array, 2d array, 3d array,object stack: all variables inside the function occupy stack memory heap: this is program not used memory, it can be used to allocate memory dynamically when the program is running. namespace: using, namespace, using namespace template: template function, template class pretreatment: #define, parametric macros, conditional compliastion: #ifdef #endif #if, # and ##, macros predefined: __LINE__, __FILE__, __DATE__, __TIME__ thread: #include &lt;thread&gt; Library STL: vector, set, etc I/O: #include &lt;filesystem&gt; network: . process: fork() Other Makefile CMake GDB cpplint.py Google C++ code style]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Grammar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReadSourceCode——howdoi]]></title>
    <url>%2F2019%2F06%2F22%2F%E8%AF%BB%E6%BA%90%E7%A0%81%E2%80%94%E2%80%94howdoi%2F</url>
    <content type="text"><![CDATA[实现思路用户执行.py文件 -&gt; 获取命令行输入的参数 -&gt; 获取搜索关键词 –&gt; 通过爬虫找到 stackoverflow 上的答案(html 格式) –&gt; 对 html 进行解析拿到答案 关键词 必须从终端获取，这一步可以通过 Python 自带的包 argparse 实现。 爬虫 而爬虫部分，则使用包 requests ，其中 url 采用的是 google 搜索的 url 解析 html 采用的工具是 pyquery，它可以让使用者像使用 jquery 一样解析 html 代码。 代理 缓存 直接将内容存在本地的文件夹 颜色输出 代码解析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436#!/usr/bin/env python######################################################## howdoi - instant coding answers via the command line# written by Benjamin Gleitzman (gleitz@mit.edu)# inspired by Rich Jones (rich@anomos.info)#######################################################from __future__ import print_function # python2.x和python3.x中print括号的兼容import gcgc.disable() # noqa: E402 # 关闭gcimport argparse # 获取命令行参数import osimport appdirs # 用户数据目录import re # 正则from cachelib import FileSystemCache, NullCache # 缓存import requests # 用于http请求import sysfrom . import __version__# 用于控制台彩色高亮格式化输出from pygments import highlightfrom pygments.lexers import guess_lexer, get_lexer_by_namefrom pygments.formatters.terminal import TerminalFormatterfrom pygments.util import ClassNotFoundfrom pyquery import PyQuery as pq # 用于网页解析from requests.exceptions import ConnectionErrorfrom requests.exceptions import SSLError# 兼容Python2.x和Python3.x的库# Handle imports for Python 2 and 3if sys.version &lt; '3': import codecs from urllib import quote as url_quote from urllib import getproxies # Handling Unicode: http://stackoverflow.com/a/6633040/305414 def u(x): return codecs.unicode_escape_decode(x)[0]else: from urllib.request import getproxies from urllib.parse import quote as url_quote def u(x): return x# rudimentary standardized 3-level log outputdef _print_err(x): print("[ERROR] " + x)_print_ok = print # noqa: E305def _print_dbg(x): print("[DEBUG] " + x) # noqa: E302if os.getenv('HOWDOI_DISABLE_SSL'): # Set http instead of https SCHEME = 'http://' VERIFY_SSL_CERTIFICATE = Falseelse: SCHEME = 'https://' VERIFY_SSL_CERTIFICATE = TrueSUPPORTED_SEARCH_ENGINES = ('google', 'bing')URL = os.getenv('HOWDOI_URL') or 'stackoverflow.com' # 设置目标问答网站# 浏览器UA，用于伪造浏览器请求，防止网站对脚本请求进行屏蔽USER_AGENTS = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:11.0) Gecko/20100101 Firefox/11.0', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:22.0) Gecko/20100 101 Firefox/22.0', 'Mozilla/5.0 (Windows NT 6.1; rv:11.0) Gecko/20100101 Firefox/11.0', ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/536.5 (KHTML, like Gecko) ' 'Chrome/19.0.1084.46 Safari/536.5'), ('Mozilla/5.0 (Windows; Windows NT 6.1) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.46' 'Safari/536.5'), )SEARCH_URLS = &#123; 'bing': SCHEME + 'www.bing.com/search?q=site:&#123;0&#125;%20&#123;1&#125;', 'google': SCHEME + 'www.google.com/search?q=site:&#123;0&#125;%20&#123;1&#125;'&#125;BLOCK_INDICATORS = ( 'form id="captcha-form"', 'This page appears when Google automatically detects requests coming from your computer ' 'network which appear to be in violation of the &lt;a href="//www.google.com/policies/terms/"&gt;Terms of Service')BLOCKED_QUESTION_FRAGMENTS = ( 'webcache.googleusercontent.com',)STAR_HEADER = u('\u2605')ANSWER_HEADER = u('&#123;2&#125; Answer from &#123;0&#125; &#123;2&#125;\n&#123;1&#125;') # 格式化答案输出NO_ANSWER_MSG = '&lt; no answer given &gt;'CACHE_EMPTY_VAL = "NULL"CACHE_DIR = appdirs.user_cache_dir('howdoi')CACHE_ENTRY_MAX = 128if os.getenv('HOWDOI_DISABLE_CACHE'): # 设置缓存文件路径 cache = NullCache() # works like an always empty cacheelse: cache = FileSystemCache(CACHE_DIR, CACHE_ENTRY_MAX, default_timeout=0)howdoi_session = requests.session()class BlockError(RuntimeError): passdef _random_int(width): bres = os.urandom(width) if sys.version &lt; '3': ires = int(bres.encode('hex'), 16) else: ires = int.from_bytes(bres, 'little') return iresdef _random_choice(seq): return seq[_random_int(1) % len(seq)]def get_proxies(): # 获取代理 proxies = getproxies() filtered_proxies = &#123;&#125; for key, value in proxies.items(): if key.startswith('http'): if not value.startswith('http'): filtered_proxies[key] = 'http://%s' % value else: filtered_proxies[key] = value return filtered_proxiesdef _get_result(url): # 获取结果 try: return howdoi_session.get(url, headers=&#123;'User-Agent': _random_choice(USER_AGENTS)&#125;, proxies=get_proxies(), verify=VERIFY_SSL_CERTIFICATE).text except requests.exceptions.SSLError as e: _print_err('Encountered an SSL Error. Try using HTTP instead of ' 'HTTPS by setting the environment variable "HOWDOI_DISABLE_SSL".\n') raise edef _add_links_to_text(element): hyperlinks = element.find('a') for hyperlink in hyperlinks: pquery_object = pq(hyperlink) href = hyperlink.attrib['href'] copy = pquery_object.text() if (copy == href): replacement = copy else: replacement = "[&#123;0&#125;](&#123;1&#125;)".format(copy, href) pquery_object.replace_with(replacement)def get_text(element): ''' return inner text in pyquery element ''' _add_links_to_text(element) try: return element.text(squash_space=False) except TypeError: return element.text()def _extract_links_from_bing(html): html.remove_namespaces() return [a.attrib['href'] for a in html('.b_algo')('h2')('a')]def _extract_links_from_google(html): return [a.attrib['href'] for a in html('.l')] or \ [a.attrib['href'] for a in html('.r')('a')]def _extract_links(html, search_engine): if search_engine == 'bing': return _extract_links_from_bing(html) return _extract_links_from_google(html)def _get_search_url(search_engine): return SEARCH_URLS.get(search_engine, SEARCH_URLS['google'])def _is_blocked(page): for indicator in BLOCK_INDICATORS: if page.find(indicator) != -1: return True return Falsedef _get_links(query): # 获取google搜索结果中的连接 search_engine = os.getenv('HOWDOI_SEARCH_ENGINE', 'google') search_url = _get_search_url(search_engine) result = _get_result(search_url.format(URL, url_quote(query))) if _is_blocked(result): _print_err('Unable to find an answer because the search engine temporarily blocked the request. ' 'Please wait a few minutes or select a different search engine.') raise BlockError("Temporary block by search engine") html = pq(result) return _extract_links(html, search_engine)def get_link_at_pos(links, position): if not links: return False if len(links) &gt;= position: link = links[position - 1] else: link = links[-1] return linkdef _format_output(code, args): # 代码格式化输出函数 渲染 if not args['color']: return code lexer = None # try to find a lexer using the StackOverflow tags # or the query arguments for keyword in args['query'].split() + args['tags']: try: lexer = get_lexer_by_name(keyword) break except ClassNotFound: pass # no lexer found above, use the guesser if not lexer: try: lexer = guess_lexer(code) except ClassNotFound: return code return highlight(code, lexer, TerminalFormatter(bg='dark'))def _is_question(link): for fragment in BLOCKED_QUESTION_FRAGMENTS: if fragment in link: return False return re.search(r'questions/\d+/', link)def _get_questions(links): # 获取问题连接 return [link for link in links if _is_question(link)]def _get_answer(args, links): # 获取答案 link = get_link_at_pos(links, args['pos']) if not link: return False if args.get('link'): return link cache_key = link page = cache.get(link) if not page: page = _get_result(link + '?answertab=votes') cache.set(cache_key, page) html = pq(page) first_answer = html('.answer').eq(0) instructions = first_answer.find('pre') or first_answer.find('code') args['tags'] = [t.text for t in html('.post-tag')] if not instructions and not args['all']: text = get_text(first_answer.find('.post-text').eq(0)) elif args['all']: texts = [] for html_tag in first_answer.items('.post-text &gt; *'): current_text = get_text(html_tag) if current_text: if html_tag[0].tag in ['pre', 'code']: texts.append(_format_output(current_text, args)) else: texts.append(current_text) text = '\n'.join(texts) else: text = _format_output(get_text(instructions.eq(0)), args) if text is None: text = NO_ANSWER_MSG text = text.strip() return textdef _get_links_with_cache(query): cache_key = query + "-links" res = cache.get(cache_key) if res: if res == CACHE_EMPTY_VAL: res = False return res links = _get_links(query) if not links: cache.set(cache_key, CACHE_EMPTY_VAL) question_links = _get_questions(links) cache.set(cache_key, question_links or CACHE_EMPTY_VAL) return question_linksdef _get_instructions(args): # 解析 question_links = _get_links_with_cache(args['query']) if not question_links: return False only_hyperlinks = args.get('link') star_headers = (args['num_answers'] &gt; 1 or args['all']) answers = [] initial_position = args['pos'] spliter_length = 80 answer_spliter = '\n' + '=' * spliter_length + '\n\n' for answer_number in range(args['num_answers']): current_position = answer_number + initial_position args['pos'] = current_position link = get_link_at_pos(question_links, current_position) answer = _get_answer(args, question_links) if not answer: continue if not only_hyperlinks: answer = format_answer(link, answer, star_headers) answer += '\n' answers.append(answer) return answer_spliter.join(answers)def format_answer(link, answer, star_headers): # 格式化回答 if star_headers: return ANSWER_HEADER.format(link, answer, STAR_HEADER) return answerdef _clear_cache(): # 清除缓存 global cache # 使用全部变量 if not cache: cache = FileSystemCache(CACHE_DIR, CACHE_ENTRY_MAX, 0) # 创建缓存 return cache.clear()def howdoi(args): # 脚本主函数 args['query'] = ' '.join(args['query']).replace('?', '') cache_key = str(args) res = cache.get(cache_key) if res: # 缓存命中 return res try: res = _get_instructions(args) if not res: res = 'Sorry, couldn\'t find any help with that topic\n' cache.set(cache_key, res) # 高速缓存 return res except (ConnectionError, SSLError): return 'Failed to establish network connection\n'def get_parser(): # 获取用户输入的命令行参数 parser = argparse.ArgumentParser(description='instant coding answers via the command line') parser.add_argument('query', metavar='QUERY', type=str, nargs='*', help='the question to answer') parser.add_argument('-p', '--pos', help='select answer in specified position (default: 1)', default=1, type=int) parser.add_argument('-a', '--all', help='display the full text of the answer', action='store_true') parser.add_argument('-l', '--link', help='display only the answer link', action='store_true') parser.add_argument('-c', '--color', help='enable colorized output', action='store_true') parser.add_argument('-n', '--num-answers', help='number of answers to return', default=1, type=int) parser.add_argument('-C', '--clear-cache', help='clear the cache', action='store_true') parser.add_argument('-v', '--version', help='displays the current version of howdoi', action='store_true') parser.add_argument('-e', '--engine', help='change search engine for this query only', dest='search_engine', nargs="?", default='google', const='bing') #google if -e not specified, bing if -e specified without positional arg. return parserdef command_line_runner(): # 启动函数 parser = get_parser() # 获取参数 args = vars(parser.parse_args()) # 获取参数的属性和属性值的字典对象 if args['version']: _print_ok(__version__) # 打印版本 return if args['clear_cache']: if _clear_cache(): # 清除缓存 _print_ok('Cache cleared successfully') else: _print_err('Clearing cache failed') return if not args['query']: # 用户没用输入选项的处理 parser.print_help() return if os.getenv('HOWDOI_COLORIZE'): args['color'] = True if args['search_engine'] != 'google': # 搜索引擎设置 assert args['search_engine'] in SUPPORTED_SEARCH_ENGINES os.environ['HOWDOI_SEARCH_ENGINE'] = args['search_engine'] utf8_result = howdoi(args).encode('utf-8', 'ignore') if sys.version &lt; '3': # Python版本为2时转换编码 print(utf8_result) else: # Write UTF-8 to stdout: https://stackoverflow.com/a/3603160 sys.stdout.buffer.write(utf8_result) # 输出结果 # close the session to release connection howdoi_session.close() # 关闭http连接if __name__ == '__main__': command_line_runner() Link howdoi]]></content>
      <categories>
        <category>ReadSourceCode</category>
      </categories>
      <tags>
        <tag>Read-Python-Code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu搭梯子]]></title>
    <url>%2F2019%2F05%2F25%2FUbuntu%E6%90%AD%E6%A2%AF%E5%AD%90%2F</url>
    <content type="text"><![CDATA[使用pip安装安装shadowsocks123sudo apt-get updatesudo apt-get install python3-pippip install shadowsocks 设置shadowsocks 执行vim ~/.shadowsocks.json 然后在shadowsocks.json里面添加配置信息，如： 123456789&#123; "server":"my_server_ip", "local_address": "127.0.0.1", "local_port": 1080, "server_port": my_server_port, "password": "my_password", "timeout": 300, "method": "aes-256-cfb"&#125; 启动sslocal1sslocal -c ~/.shadowsocks.json 浏览器代理 安装插件SwitchyOmega 点击进入，点击下载SwitchyOmega_Chromium.crx文件 将文件后缀名改为.zip 新建文件夹SwitchyOmega_Chromium，解压到这个文件夹 Chrome 打开chrome://extensions/，选择开发者模式，把插件托进去安装 配置 Proxy Server填写shadowsocks.json配置中的local_address Port填写shadowsocks.json配置中的local_port 左边Apply changes保存 配置Auto Switch Rule list rules的Profile填proxy Default的Profile填[Direct] Rule List Format选择AutoProxy Rule List URL填写gfwlist的规则: https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 下载规则文件Download Profile Now 左边Apply changes保存 启用 SwitchyOmega 启用 SwitchyOmega 插件，选择 Auto Switch 模式就可以了。 测试启动 后端启动：sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d start 后端停止：sslocal -c /home/xx/Software/ShadowsocksConfig/shadowsocks.json -d stop 参考链接 Linux安装配置Shadowsocks客户端及开机自动启动 科学上网 | Ubuntu使用shadowsocks翻墙]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>VPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Momenta-Internship]]></title>
    <url>%2F2019%2F04%2F11%2Fmomenta_internship%2F</url>
    <content type="text"><![CDATA[用工卡记录下： 公司大部分都是 T 480s 加 Ubuntu。 这里就要吐槽下了： 桌面版本的使用了几天，终于见识到了流传已久的内部错误。 Ubuntu的显卡驱动好无语，风扇要起飞了。 安装显卡驱动也是个坑，搞得我又重装了一次系统。 还有zsh虽然好用，但有时候也是个坑。 很多办公软件不支持，deepin不是很稳定，安装后其他软件未必安装的上，坑。 又得学vim，这个学习成本有些高。 各种坑，都想申请mac了。。。 现在基本是 Ubuntu(desktop system) + Docker(development environment) + Vim(coding) + Tmux(terminal) + Oh-My-Zsh(zsh or shell or bash) + Git(version) + VScode(review code) + Pycharm(move brick) sshfs mount etc. 公司的展厅： 苏州的太湖： 周六晚溜岗去了苏州大学（晚上12：00多了，没进去）。早上去的渔洋公园，从大门进没买票，这个可以的。 自己的域名要到期了，当时图便宜买的.xyz的，现在续费好贵。下次还是买.cn的吧。 kaggle的比赛也勉强10%，后面都没有时间做了，后面要加油了。 太晚了，瞎记录下吧 Update Time: 4.15 am 1:49 周六晚上公司的21楼和24楼还有这么多同事在加班： Town Hall:]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 性能优化]]></title>
    <url>%2F2019%2F03%2F23%2FPython%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Python 性能优化简介主要优化方法： 代码质量 代码性能分析 cProfile 测试CPU运行 timeit 测试函数 memoryprofiler 监控 Python 代码的内存使用 lineprofiler 逐行性能分析 解释器 Cpython Psyco JIT解释器 PyPi Cython Jython ccfi C / C++ 扩展 ctypes SWIG 其他 Numba GPU加速 Python和C++互相调用 C++ 和 Python 忙等待文件夹内容 Python运行 C++停；C++停的时候唤醒Python Ctypes 示例123456789101112#include &lt;stdio.h&gt;int add_int(int, int);float add_float(float, float);int add_int(int num1, int num2) &#123; return num1 + num2;&#125;float add_float(float num1, float num2) &#123; return num1 + num2;&#125; 接下来将C文件编译为.so文件(windows下为DLL)。下面操作会生成adder.so文件 12# for Mac$ gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c 现在在你的Python代码中来调用它 1234567891011121314151617from ctypes import *# load the shared object fileadder = CDLL('./adder.so')# find sum of integersres_int = adder.add_int(4, 5)print("Sum of 4 and 5 = " + str(res_int))# find sum of floatsa = c_float(5.5)b = c_float(4.1)add_float = adder.add_floatadd_float.restype = c_floatprint("Sum of 5.5 and 4.1 = ", str(add_float(a, b))) 输出如下 123(base) ➜ test-ctypes python3 test.py Sum of 4 and 5 = 9Sum of 5.5 and 4.1 = 9.600000381469727 link ctypes — A foreign function library for Python The chapter 15 of “Python CookBook” book. 极客学院 第十五章：C语言扩展 | python3-cookbook]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-Logging]]></title>
    <url>%2F2019%2F03%2F23%2FPython-Logging%2F</url>
    <content type="text"><![CDATA[Python With Logging logging有两种使用方法： 基础使用 高级用法 基础使用logging使用场景 Python内置的logging模块，为我们提供了现成的高效好用的日志解决方案。但是，不是所有的场景都需要使用logging模块，下面是Python官方推荐的使用方法： 要执行的任务 执行这个任务最好的工具 普通情况下，在控制台显示输出 print() 报告正常程序操作过程中发生的事件 logging.info() (or logging.debug()) 发出有关特定事件的警告 warnings.warn() or logging.warning() 有区别 报告错误 弹出异常 在不引发异常的情况下报告错误 logging.error(), logging.exception() or logging.critical() 日志级别，按事件严重程度由低到高排列： 级别 使用范围 DEBUG 详细信息，常用于调试 INFO 程序正常运行过程中产生的一些信息 WARNING 警告用户，虽然程序还在正常工作，但有可能发生错误 ERROR 由于更严重的问题，程序已不能执行一些功能了 CRITICAL 严重错误，程序已不能继续运行 范例要把日志输出到文件内，就不能使用上面的方法了，但是logging模块同样给我们提供了一个相对便捷的手段，那就是logging.basicConfig()方法。重新进入解释器环境，执行下面的代码： 12345import logginglogging.basicConfig(filename='example.log',level=logging.DEBUG)logging.debug('This message should go to the log file')logging.info('So should this')logging.warning('And this, too') 然后打开本地的example.log文件，可以看到下面的日志消息： 123DEBUG:root:This message should go to the log fileINFO:root:So should thisWARNING:root:And this, too 可以通过下面的方法来获取用户输入的日志级别参数： 1234numeric_level = getattr(logging, loglevel.upper(), None)if not isinstance(numeric_level, int): raise ValueError('Invalid log level: %s' % loglevel)logging.basicConfig(level=numeric_level, ...) 附加时间信息：要在日志内容中附加时间信息，可以在format字符串中添加%(asctime)s。 123import logginglogging.basicConfig(format='%(asctime)s %(message)s')logging.warning('is when this event was logged.') 输出结果： 12010-12-12 11:41:42,612 is when this event was logged. 高级用法 如果只是简单地使用logging，那么使用上面介绍的方法就可以了，如果要深度定制logging，那么就需要对它有更深入的了解。下面的内容才是基本的logging模块的使用方法。logging模块采用了模块化设计，主要包含四种组件： 组件 名称 作用 Loggers 日志器 提供应用程序代码能直接使用的接口 Handlers 处理器 将记录器产生的日志发送至目的地 Filters 过滤器 提供更好的粒度控制，决定哪些日志会被输出 Formatters 格式器 设置日志内容的组成结构和消息字段 日志流 日志事件信息在loggers和handlers中的逻辑流程如下图所示： 下面是同时向屏幕和文件进行日志输出的流程： Loggers记录器logging模块的日志功能是基于Logger类实现的。我们可以通过下面的方法获取一个Logger类的实例（建议以模块名命名logger实例）。 logger = logging.getLogger(__name__) Logger是一个树形层级结构，在使用debug()，info()，warn()，error()，critical()等方法之前必须先创建一个Logger的实例，即创建一个记录器，如果没有显式的进行创建，则默认创建一个root logger，并应用默认的日志级别(WARN)，默认的处理器Handler(StreamHandler，即将日志信息打印在标准输出上)，和默认的格式化器Formatter，就像我们在前面举的那些例子一样。 logger对象有三重功能。首先，提供应用程序调用的接口；其次，决定日志记录的级别；最后，将日志内容传递到相关联的handlers中。 总结logger对象的用法，可以分成两类：配置和消息发送。 下面是最常用的配置方法： Logger.setLevel()：设置日志记录级别 Logger.addHandler()和Logger.removeHandler()：为logger对象添加或删除handler处理器对象。 Logger.addFilter()和Logger.removeFilter()：为为logger对象添加或删除filter过滤器对象。 配置好logger对象后，就可以使用下面的方法创建日志消息了： Logger.debug(), Logger.info(), Logger.warning(), Logger.error(), and Logger.critical()：创建对应级别的日志，但不一定会被记录。 Logger.exception()：创建一个类似Logger.error()的日志消息。不同的是Logger.exception()保存有一个追踪栈。该方法只能在异常handler中调用。 Logger.log()：显式的创建一条日志，是前面几种方法的通用方法。 注意，getLogger()方法返回一个logger对象的引用，并以你提供的name参数命名，如果未提供名字，那么默认为‘root’。使用同样的name参数，多次调用getLogger()，将返回同样的logger对象。 Handlers处理器Handlers对象是日志信息的处理器、分发器。它们将日志分发到不同的目的地。比如有时候我们希望将所有的日志都记录在本地文件内，将error及其以上级别的日志发送到标准输出stdout，将critical级别的日志以邮件的方法发送给管理员。这就需要同时有三个独立的handler，分别负责一个方向的日志处理。 logging模块使用较多的handlers有两个，StreamHandler和FileHandler。 StreamHandler 标准输出stdout（如显示器）分发器。 创建方法: sh = logging.StreamHandler(stream=None) FileHandler 将日志保存到磁盘文件的处理器。 创建方法: fh = logging.FileHandler(filename, mode=&#39;a&#39;, encoding=None, delay=False) handlers对象有下面的方法： setLevel()：和logger对象的一样，设置日志记录级别。那为什么要设置两层日志级别呢？logger对象的日志级别是全局性的，对所有handler都有效，相当于默认等级。而handlers的日志级别只对自己接收到的logger传来的日志有效，进行了更深一层的过滤。 setFormatter()：设置当前handler对象使用的消息格式。 addFilter() 和 removeFilter()：配置或删除一个filter过滤对象 logging模块内置了下面的handler处理器，从字面上你就能看出它们的大概用途： StreamHandlerFileHandlerBaseRotatingHandlerRotatingFileHandlerTimedRotatingFileHandlerSocketHandlerDatagramHandlerSMTPHandlerSysLogHandlerNTEventLogHandlerHTTPHandlerWatchedFileHandlerQueueHandlerNullHandler FormattersFormatter对象用来最终设置日志信息的顺序、结构和内容。其构造方法为： ft = logging.Formatter.__init__(fmt=None, datefmt=None, style=’%’)如果不指定datefmt，那么它默认是%Y-%m-%d %H:%M:%S样式的。 style参数默认为百分符%，这表示前面的fmt参数应该是一个%(&lt;dictionary key&gt;)s格式的字符串，而可以使用的logging内置的keys，如下表所示： Filter过滤器Handlers和Loggers可以使用Filters来完成比日志级别更复杂的过滤。比如我们定义了filter = logging.Filter(&#39;a.b.c&#39;)，并将这个Filter添加到了一个Handler上，则使用该Handler的Logger中只有名字带a.b.c前缀的Logger才能输出其日志。 创建方法: filter = logging.Filter(name=&#39;&#39;) 例如： 12filter = logging.Filter('mylogger.child1.child2') fh.addFilter(filter) 则只会输出下面格式的日志，注意其用户名： 12342017-09-27 16:27:46,227 - mylogger.child1.child2 - DEBUG - logger1 debug message2017-09-27 16:27:46,227 - mylogger.child1.child2 - DEBUG - logger1 debug message2017-09-27 16:27:46,227 - mylogger.child1.child2 - DEBUG - logger1 debug message2017-09-27 16:27:46,227 - mylogger.child1.child2 - DEBUG - logger1 debug message 配置日志模块 有三种配置logging的方法： 创建loggers、handlers和formatters，然后使用Python的代码调用上面介绍过的配置函数。 创建一个logging配置文件，然后使用fileConfig()方法读取它。 创建一个配置信息字典然后将它传递给dictConfig()方法。 下面的例子采用了第一种方法： 123456789101112131415161718192021222324252627#simple_logging_module.pyimport logging# 创建logger记录器logger = logging.getLogger('simple_example')logger.setLevel(logging.DEBUG)# 创建一个控制台处理器，并将日志级别设置为debug。ch = logging.StreamHandler()ch.setLevel(logging.DEBUG)# 创建formatter格式化器formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')# 将formatter添加到ch处理器ch.setFormatter(formatter)# 将ch添加到loggerlogger.addHandler(ch)# 然后就可以开始使用了！logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') 在命令行中运行上面的代码，输出结果如下： 123456$ python simple_logging_module.py2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message2005-03-19 15:10:26,620 - simple_example - INFO - info message2005-03-19 15:10:26,695 - simple_example - WARNING - warn message2005-03-19 15:10:26,697 - simple_example - ERROR - error message2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message 下面是使用第二种方法，logging配置文件的方式： 12345678910111213141516# simple_logging_config.pyimport loggingimport logging.configlogging.config.fileConfig('logging.conf') # 读取config文件# 创建logger记录器logger = logging.getLogger('simpleExample')# 使用日志功能logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') 其中的logging.conf配置文件内容如下： 12345678910111213141516171819202122232425262728[loggers]keys=root,simpleExample[handlers]keys=consoleHandler[formatters]keys=simpleFormatter[logger_root]level=DEBUGhandlers=consoleHandler[logger_simpleExample]level=DEBUGhandlers=consoleHandlerqualname=simpleExamplepropagate=0[handler_consoleHandler]class=StreamHandlerlevel=DEBUGformatter=simpleFormatterargs=(sys.stdout,)[formatter_simpleFormatter]format=%(asctime)s - %(name)s - %(levelname)s - %(message)sdatefmt= 在命令行中执行代码，结果如下： 123456$ python simple_logging_config.py2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message2005-03-19 15:38:55,979 - simpleExample - INFO - info message2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message2005-03-19 15:38:56,055 - simpleExample - ERROR - error message2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message Python官方更推荐第三种新的配置方法，类字典形式的配置信息，因为Python的字典运用形式多样，操作灵活。比如，你可以通过JSON格式保存字典，或者YAML格式保存信息，然后读取成字典。当然，你也可以直接在Python代码里编写传统的带有配置信息的字典。一切都是基于键值对形式的就OK。 下面的例子就是基于YAML配置文件的日志。logging.conf.yaml配置文件内容如下： 123456789101112131415161718version: 1formatters: simple: format: &apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;handlers: console: class: logging.StreamHandler level: DEBUG formatter: simple stream: ext://sys.stdoutloggers: simpleExample: level: DEBUG handlers: [console] propagate: noroot: level: DEBUG handlers: [console] 这里要先通过pip安装yaml模块： 1pip install pyyaml yaml模块的使用很简单，使用open()方法打开一个yaml文件对象，然后使用yaml的load()方法将文件内容读成一个Python的字典对象。最后我们根据这个字典对象，使用logging.conf的dictConfig()方法，获取配置信息。如下代码所示： 12345678910111213141516171819import loggingimport logging.configimport yaml# 通过yaml文件配置loggingf = open("logging.conf.yaml")dic = yaml.load(f)f.close()logging.config.dictConfig(dic)# 创建loggerlogger = logging.getLogger('simpleExample')# 输出日志logger.debug('debug message')logger.info('info message')logger.warn('warn message')logger.error('error message')logger.critical('critical message') 输出结果： 123452017-09-27 17:41:09,241 - simpleExample - DEBUG - debug message2017-09-27 17:41:09,242 - simpleExample - INFO - info message2017-09-27 17:41:09,242 - simpleExample - WARNING - warn message2017-09-27 17:41:09,242 - simpleExample - ERROR - error message2017-09-27 17:41:09,242 - simpleExample - CRITICAL - critical message link The official documentation 刘江的博客及教程 | logging logging | tencent Logging HOWTO Logging Cookbook]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Logging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单调栈]]></title>
    <url>%2F2019%2F02%2F12%2F%E5%8D%95%E8%B0%83%E6%A0%88%2F</url>
    <content type="text"><![CDATA[单调栈：栈中从栈底到栈顶的数都是递增（减）的，为了维护这种结构在插入比当前栈顶大的数的时候都需要先将栈顶的数弹出，这样我们就能够知道弹出的这个数两边比它大的数了 下面使用Leetcode两道题举例： 84. Largest Rectangle in Histogram题意给定n个非负整数代表柱状图的条高，其中每个条的宽度为1，求柱状图中最大矩形的面积。上面是一个柱状图，其中每个条形的宽度为1，给定高度=[2,1,5,6,2,3]。最大的矩形显示在阴影区域，其面积为10个单位。实例：12Input: [2,1,5,6,2,3]Output: 10 思路 如果height是升序的，例如：[1，2，3]，那么就是比较（1 3）vs（2 2）vs（3 1），也就是max（height[i] (len(height) - 1)） 我们要找到最大的长方形面积，可以按照这种方式来找：对于数组中每个元素代表的高度，找到以当前这个高度为高的最大的长方形面积，那么遍历所有的高度即可。我们直接暴力求解的话，就是O(n^2)的复杂度。 有了上述思路后，我们在遍历数组的时候维护一个栈,这个栈中的元素从底向上按照高度值递增。如果遇到当前bar的高度比栈顶元素低，那么就出栈直到栈顶元素低于当前bar的高度。关键就在这里，当我们将第i个元素弹出栈的时候，我们就可以计算以heights[i]为高的最大长方形的面积。在遍历完数组之后，栈内元素仍然需要继续弹出，最后所有元素都会依次出栈,意味着计算完了所有可能的最大面积,就可以得到结果了。 再来分析一下这个算法的可行性：每一个元素都要入栈一次，出栈一次。入栈的时候的是遍历访问到它的时候，那出栈的时候意味着什么呢。在这里元素出栈意味着，我们已经计算了以它的高度为高的最大长方形面积。结合栈内元素的单调性，栈顶元素所对应的bar一定比出栈元素对应的bar小，所以以出栈元素对应的bar为高的长方形无法往左边延展。结合代码，我们已经判断过当前处理的第i个元素所对应的bar也比出栈元素对应的bar小，所以长方形无法往右边延展。这个元素和左右边界之间如果还有空隙，那么这些空隙里所存在的bar，一定是因为维护栈的单调性而被弹出了。也就是说，如果这些bar存在，那么一定比这个出栈元素所对应的bar高。既然这些bar的高度更高，那么就可以被纳入这个最大长方形面积的计算中，也就不影响当前出栈元素的最大长方形面积的计算。以上我们就证明了，当我们将第i个元素弹出栈的时候，我们计算了以heights[i]为高的最大长方形的面积。 模拟一下过程： 先加入一个0，方便最后可以全部弹栈出来。栈变成：[2，1，5，6，3，-1] 2进栈，1比栈顶小，对2进行出栈，max_space = 2 2被替换为1进栈，1继续进栈，这时栈为[1，1] 5，6都是非降的，继续进栈，栈为[1，1，5，6] 遇到3，是一个降序点；开始出栈，6出栈，对应space=61；5出栈对应space=52；下一个1比3小，不需要出栈。然后将5、6的弹栈后的空位压栈为3，这是栈为[1，1，3，3，3] 下一步遇到0，开始依次出栈，得到area=31，32，33，14，1*5。 遍历结束。整个过程中max_space=10 代码1234567891011121314# Runtime: 60 ms, faster than 97.59% of Python3 online submissions for Largest Rectangle in Histogram.# Memory Usage: 9.3 MB, less than 41.22% of Python3 online submissions for Largest Rectangle in Histogram.class Solution: def largestRectangleArea(self, height): height.append(0) stack = [-1] # 添加-1是为了判断是不是进行到了最后一个 max_space = 0 for i in range(len(height)): while height[i] &lt; height[stack[-1]]: # 如果当前柱比栈顶柱要低，出栈，更新结果 h = height[stack.pop()] w = i - stack[-1] - 1 max_space = max(max_space, h * w) stack.append(i) return max_space 说明： height升序则入栈，否则出栈计算并更新结果 stack存储的是height的下标 栈变化如下： 1234567[-1][-1, 0][-1, 1][-1, 1, 2][-1, 1, 2, 3][-1, 1, 4][-1, 1, 4, 5] 85. Maximal Rectangle题意给定一个填充了0和1的二维二进制矩阵，找到只包含1的最大矩形并返回其面积。实例：12345678Input:[ [&quot;1&quot;,&quot;0&quot;,&quot;1&quot;,&quot;0&quot;,&quot;0&quot;], [&quot;1&quot;,&quot;0&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;], [&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;,&quot;1&quot;], [&quot;1&quot;,&quot;0&quot;,&quot;0&quot;,&quot;1&quot;,&quot;0&quot;]]Output: 6 思路 对于某一行，将它转换成以当前行为底的直方图，而且下一个行可以根据上一行的结果直接求得高度，所以遍历完成以后，就能得到最大的矩形。 进行一下模拟: 第一行，对应的高度为 1 0 1 0 0 第二行，对应的高度为 2 0 2 1 1 第三行，对应的高度为 3 1 3 2 2 第四行，对应的高度为 4 0 0 3 0 其实就是，对于每一行，如果它为1，就加上上一行的高度，否则高度就为0，求得高度以后，用上一个问题的解法直接求得面积并且更新结果就可以了。 代码123456789101112131415161718# Runtime: 80 ms, faster than 96.81% of Python3 online submissions for Maximal Rectangle.# Memory Usage: 7.6 MB, less than 52.71% of Python3 online submissions for Maximal Rectangle.class Solution: def maximalRectangle(self, matrix): if not matrix: return 0 height = [0] * (len(matrix[0]) + 1) ans = 0 for row in matrix: for i in range(len(matrix[0])): height[i] = height[i] + 1 if row[i] == '1' else 0 stack = [-1] for i in range(len(matrix[0]) + 1): while height[i] &lt; height[stack[-1]]: h = height[stack.pop()] w = i - 1 - stack[-1] ans = max(ans, h * w) stack.append(i) return ans]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主要排序算法]]></title>
    <url>%2F2019%2F02%2F06%2FSort-Interview%2F</url>
    <content type="text"><![CDATA[对比 算法 稳定性 平均时间复杂度 最优时间复杂度 最坏时间复杂度 空间复杂度 备注 快速排序 x NlogN NlogN N^2 logN 归并排序 √ NlogN NlogN NlogN N 堆排序 x NlogN NlogN NlogN 1 无法利用局部性原理 快速排序 快速排序使用分治法策略来把一个序列分为两个子序列 步骤： 从数列中挑出一个元素，称为“基准” 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分割结束之后，该基准就处于数列的中间位置。这个称为分割操作。 递归地把小于基准值元素的子数列和大于基准值元素的子数列排序。 123456789101112131415161718192021def quick_sort(array): def sort_recursion(array, left, right): if left &gt;= right: return # 递归停止 i, j = left, right flag = array[left] # 取首位为对比标志值 while i &lt; j: # 找flag的最终位置 while i &lt; j and array[j] &gt;= flag: # 用j向左扫描找小于flag的记录 j -= 1 if i &lt; j: array[i] = array[j] # 小记录移到左边 i += 1 while i &lt; j and array[i] &lt;= flag: # 用i向右扫描找大于flag的记录 i += 1 if i &lt; j: array[j] = array[i] # 大记录移到右边 j -= 1 array[i] = flag # 将对比的标志位存入其最终位置 sort_recursion(array, left, i - 1) # 左边 sort_recursion(array, i + 1, right) # 右边 sort_recursion(array, 0, len(array) - 1) 归并排序 指的是将两个已经排序的序列合并成一个序列的操作 递归法（自顶向下）： 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列 设定两个指针，最初位置分别为两个已经排序序列的起始位置 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置 重复步骤3直到某一指针到达序列尾 将另一序列剩下的所有元素直接复制到合并序列尾 12345678910111213141516171819202122def merge_sort(array): def merge(left, right): # 归并 tmp = [] i, j = 0, 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: tmp.append(left[i]) i += 1 else: tmp.append(right[j]) j += 1 tmp += left[i:] tmp += right[j:] return tmp length = len(array) if length &lt; 2: # 递归中止 return array else: mid = length // 2 left, right = merge_sort(array[:mid]), merge_sort(array[mid:]) # 递归 return merge(left, right) # 归并 堆排序 指利用堆这种数据结构所设计的一种排序算法。堆是一个近似完全二叉树的结构，并同时满足堆积的性质：即子节点的键值或索引总是小于（或者大于）它的父节点。 在堆的数据结构中，堆中的最大值总是位于根节点（在优先队列中使用堆的话堆中的最小值位于根节点）。堆中定义以下几种操作： 最大堆调整：将堆的末端子节点作调整，使得子节点永远小于父节点 创建最大堆：将堆中的所有数据重新排序 堆排序：移除位在第一个数据的根节点，并做最大堆调整的递归运算 12345678910111213141516171819202122def heap_sort(array): def sift_down(start, end): root = start while True: child = 2 * root + 1 if child &gt; end: break if child + 1 &lt;= end and array[child] &lt; array[child + 1]: # 找出两个child中较大的那个 child += 1 if array[root] &lt; array[child]: # 最大堆小于较大的child，交换 array[root], array[child] = array[child], array[root] root = child # 正在调整的节点设置为root else: break # 无序调整的时候退出 # 创建最大堆 for start in range(len(array) // 2 - 1, -1, -1): sift_down(start, len(array) - 1) # 堆排序 for end in range(len(array) - 1, 0, -1): array[0], array[end] = array[end], array[0] sift_down(0, end - 1) return array link 数据结构和算法教程 | 易百教程 CS-Notes | CyC2018 《算法》- 红宝书]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Backtrack]]></title>
    <url>%2F2019%2F02%2F04%2F%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[是暴力搜索法中的一种 回溯法采用试错的思想，它尝试分步的去解决一个问题。在分步解决问题的过程中，当它通过尝试发现现有的分步答案不能得到有效的正确的解答的时候，它将取消上一步甚至是上几步的计算，再通过其它的可能的分步解答再次尝试寻找问题的答案。回溯法通常用最简单的递归方法来实现，在反复重复上述的步骤后可能出现两种情况： 找到一个可能存在的正确的答案 在尝试了所有可能的分步方法后宣告该问题没有答案 经典例题八皇后问题 很多人都知道 8皇后问题，即在8X8格的国际象棋上摆放八个皇后，使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上。一种可能的放置方法法如下： 解题步骤： 在第n行寻找可以插入的位置，中间涉及到位置合法性的判断 如果没有可以插入的位置，返回 如果有可以插入的位置， 插入数据。此时再判断是否已经是最后一行，如果是，打印输出返回；反之继续对下一行数据进行试探处理。 Python代码： 123456789101112def eight_queen_question(arr=[None] * 8, n=0): if n == len(arr): print(arr) return for col in range(len(arr)): arr[n], flag = col, True # 表示把第n行的皇后放在col列上 for row in range(n): if arr[row] == col or abs(col - arr[row]) == n - row: # 判断是否跟前面的皇后冲突 flag = False break if flag: eight_queen_question(arr, n+1) 46. Permutations 给定一个不同整数的集合，返回所有可能的排列。举例： 12345678910Input: [1,2,3]Output:[ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 代码： 12345678910111213141516# 回溯class Solution: def permute(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ tmp = [] def backtrack(nums, tmp, arr): if len(arr) == len(nums): tmp.append(arr) else: for i in range(len(nums)): if nums[i] not in arr: backtrack(nums, tmp, arr + [nums[i]]) backtrack(nums, tmp, []) return tmp 47. Permutations II给定一组可能包含重复项的数字，返回所有可能的唯一排列。举例： 1234567Input: [1,1,2]Output:[ [1,1,2], [1,2,1], [2,1,1]] 代码： 1234567891011121314151617# 回溯# time:276 ms space:6.7 MBclass Solution: def permuteUnique(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ def dfs(nums, cur, res): if not nums: res.add(cur) else: for i in range(len(nums)): dfs(nums[:i] + nums[i + 1:], cur + (nums[i], ), res) res = set() dfs(nums, (), res) return list(res) 51. N-Queens N皇后之谜是把N皇后放在一个N×N棋盘上，这样就不会有两个皇后互相攻击。给定一个整数n，将所有不同的解返回到n-queens难题。每一种解决办法都有一个单单的单子配置，而在哪里都有的。举例： 12345678910111213Input: 4Output: [ [&quot;.Q..&quot;, // Solution 1 &quot;...Q&quot;, &quot;Q...&quot;, &quot;..Q.&quot;], [&quot;..Q.&quot;, // Solution 2 &quot;Q...&quot;, &quot;...Q&quot;, &quot;.Q..&quot;]]解释：有两个不同的解决方案，4皇后难题，如上图所示。 代码： 12345678910111213141516171819202122232425class Solution: def solveNQueens(self, n): """ :type n: int :rtype: List[List[str]] """ self.result = [] self.col = set() self.left = set() self.right = set() self.dfs(n, 0, []) return self.result def dfs(self, n, row, curstate): if row &gt;= n: return self.result.append(curstate) for col in range(n): if col in self.col or ((row + col) in self.left) or ((row - col) in self.right): # 砍树枝 continue self.col.add(col) self.left.add(col + row) self.right.add(row - col) self.dfs(n, row + 1, curstate + ['.'*col+'Q'+'.'*(n-col-1)]) # 逐行绘制 self.col.remove(col) self.left.remove(col + row) self.right.remove(row - col) link Backtracking N皇后问题]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Backtrack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Socket-网络套接字]]></title>
    <url>%2F2019%2F02%2F02%2FSocket-Python3%2F</url>
    <content type="text"><![CDATA[Socket又称”套接字”，应用程序通常通过”套接字”向网络发出请求或者应答网络请求，使主机间或者一台计算机上的进程间可以通讯。 Python 提供了两个基本的 socket 模块： Socket 它提供了标准的BSD Socket API。 SocketServer 它提供了服务器重心，可以简化网络服务器的开发。 Socket 类型 socket 类型 描述 socket.AF_UNIX 用于同一台机器上的进程通信（既本机通信） socket.AF_INET 用于服务器与服务器之间的网络通信 socket.AF_INET6 基于IPV6方式的服务器与服务器之间的网络通信 socket.SOCK_STREAM 基于TCP的流式socket通信 socket.SOCK_DGRAM 基于UDP的数据报式socket通信 socket.SOCK_RAW 原始套接字，普通的套接字无法处理ICMP、IGMP等网络报文，而SOCK_RAW可以；其次SOCK_RAW也可以处理特殊的IPV4报文；此外，利用原始套接字，可以通过IP_HDRINCL套接字选项由用户构造IP头 socket.SOCK_SEQPACKET 可靠的连续数据包服务 创建TCP Socket：1sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 创建UDP Socket：1sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) Socket 函数 TCP发送数据时，已建立好TCP链接，所以不需要指定地址，而UDP是面向无连接的，每次发送都需要指定发送给谁。 服务器与客户端不能直接发送列表，元素，字典等带有数据类型的格式，发送的内容必须是字符串数据。 服务器端 Socket 函数 Socket 函数 描述 s.bind(address) 将套接字绑定到地址，在AF_INET下，以tuple(host, port)的方式传入，如s.bind((host, port)) s.listen(backlog) 开始监听TCP传入连接，backlog指定在拒绝链接前，操作系统可以挂起的最大连接数，该值最少为1，大部分应用程序设为5就够用了 s.accept() 接受TCP链接并返回（conn, address），其中conn是新的套接字对象，可以用来接收和发送数据，address是链接客户端的地址。 客户端 Socket 函数 Socket 函数 描述 s.connect(address) 链接到address处的套接字，一般address的格式为tuple(host, port)，如果链接出错，则返回socket.error错误 s.connect_ex(address) 功能与s.connect(address)相同，但成功返回0，失败返回errno的值 公共 Socket 函数 Socket 函数 描述 s.recv(bufsize[, flag]) 接受TCP套接字的数据，数据以字符串形式返回，buffsize指定要接受的最大数据量，flag提供有关消息的其他信息，通常可以忽略 s.send(string[, flag]) 发送TCP数据，将字符串中的数据发送到链接的套接字，返回值是要发送的字节数量，该数量可能小于string的字节大小 s.sendall(string[, flag]) 完整发送TCP数据，将字符串中的数据发送到链接的套接字，但在返回之前尝试发送所有数据。成功返回None，失败则抛出异常 s.recvfrom(bufsize[, flag]) 接受UDP套接字的数据u，与recv()类似，但返回值是tuple(data, address)。其中data是包含接受数据的字符串，address是发送数据的套接字地址 s.sendto(string[, flag], address) 发送UDP数据，将数据发送到套接字，address形式为tuple(ipaddr, port)，指定远程地址发送，返回值是发送的字节数 s.close() 关闭套接字 s.getpeername() 返回套接字的远程地址，返回值通常是一个tuple(ipaddr, port) s.getsockname() 返回套接字自己的地址，返回值通常是一个tuple(ipaddr, port) s.setsockopt(level, optname, value) 设置给定套接字选项的值 s.getsockopt(level, optname[, buflen]) 返回套接字选项的值 s.settimeout(timeout) 设置套接字操作的超时时间，timeout是一个浮点数，单位是秒，值为None则表示永远不会超时。一般超时期应在刚创建套接字时设置，因为他们可能用于连接的操作，如s.connect() s.gettimeout() 返回当前超时值，单位是秒，如果没有设置超时则返回None s.fileno() 返回套接字的文件描述 s.setblocking(flag) 如果flag为0，则将套接字设置为非阻塞模式，否则将套接字设置为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关的文件 Socket 编程思想TCP 服务器1、创建套接字，绑定套接字到本地IP与端口12s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.bind() 2、开始监听链接1s.listen() 3、进入循环，不断接受客户端的链接请求12While True: s.accept() 4、接收客户端传来的数据，并且发送给对方发送数据12s.recv()s.sendall() 5、传输完毕后，关闭套接字1s.close() TCP 客户端1、创建套接字并链接至远端地址12s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect() 2、链接后发送数据和接收数据12s.sendall()s.recv() 3、传输完毕后，关闭套接字 范例服务端12345678910111213141516171819202122232425# -*- coding: utf-8 -*-'''file-name:server.py'''# 加载socket包import socket# 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()# 设置端口号port = 8000# 绑定端口号s.bind((host, port))# 设置最大连接数，超过后排队s.listen(1)while True: # 建立客户端连接 clientsocket, addr = s.accept() print("连接地址: %s" % str(addr)) msg='欢迎访问socket服务！'+ "\r\n" clientsocket.send(msg.encode('utf-8')) clientsocket.close() 客户端123456789101112131415161718192021# -*- coding: utf-8 -*-'''file-name:client.py'''# 加载socket包import socket# 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()# 设置端口号port = 8000# 连接服务，指定主机和端口s.connect((host, port))# 接收小于 1024 字节的数据msg = s.recv(1024)# 关系socket连接s.close()# 输出访问内容print(msg.decode('utf-8')) 步骤12# shell 1&gt; python server.py 123# shell 2&gt; python client.py欢迎访问socket服务！ 视频 如果视频不能播放，请移步链接]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce:面向大型集群的简化数据处理]]></title>
    <url>%2F2019%2F02%2F01%2FMapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce: Simplified Data Processing on Large Clusters MapReduce既是一种编程模型，也是一种与之关联的、用于处理和产生大数据集的实现。 用户要特化一个map程序去处理key/value对，并产生中间key/value对的集合，以及一个reduce程序去合并有着相同key的所有中间key/value对。 编程模型 MapReduce库的使用者用两个函数来表示这个过程：map和reduce。 123456789101112131415// 考虑一个问题：统计一个很大的文档集合中每个单词出现的次数。使用者能写出与下面的伪代码相似的代码：map(String key,String value): // key: 文档名 // value: 文档内容 for each word w in value: EmitIntermediate(w,"1");reduce(Stringkey, Iterator values): // key: 一个单词 // value: 计数值列表 int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result));// map函数将每个单词与出现次数一同输出（本例中简单的输出“1”）。reduce函数将针对某个特定词输出的次数都合并相加。 举例： URL访问频次统计：map函数处理网页请求的日志，对每个URL输出〈URL, 1〉。reduce函数将相同URL的所有值相加并输出〈URL, 总次数〉对。 实现 上图展示了我们的实现中MapReduce操作的整体流程。当用户程序调用MapReduce函数时，会发生下面一系列动作（上图中的标号与下面列表顺序相同）： 用户程序中的MapReduce库首先将输入文件切分为M块，每块的大小从16MB到64MB（用户可通过一个可选参数控制此大小）。然后MapReduce库会在一个集群的若干台机器上启动程序的多个副本。 程序的各个副本中有一个是特殊的——主节点，其它的则是工作节点。主节点将M个map任务和R个reduce任务分配给空闲的工作节点，每个节点一项任务。 被分配map任务的工作节点读取对应的输入区块内容。它从输入数据中解析出key/value对，然后将每个对传递给用户定义的map函数。由map函数产生的中间key/value对都缓存在内存中。 缓存的数据对会被周期性的由划分函数分成R块，并写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给reduce工作节点。 当一个reduce工作节点得到了主节点的这些位置通知后，它使用RPC调用去读map工作节点的本地磁盘中的缓存数据。当reduce工作节点读取完了所有的中间数据，它会将这些数据按中间key排序，这样相同key的数据就被排列在一起了。同一个reduce任务经常会分到有着不同key的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序。 reduce工作节点遍历排序好的中间数据，并将遇到的每个中间key和与它关联的一组中间value传递给用户的reduce函数。reduce函数的输出会写到由reduce划分过程划分出来的最终输出文件的末尾。 当所有的map和reduce任务都完成后，主节点唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码中。 技巧 划分函数 顺序保证 合并函数 输入和输出类型 边界效应 略过坏记录 本地执行 状态信息 计数器 结论 约束这个编程模型令并行和分布式计算，以及令这些计算可容错，变得简单了 网络带宽是一种稀缺资源。我们系统中的很多优化都因此针对减少通过网络发送的数据总量：局部性优化允许我们从本地磁盘读，同时将中间文件写入本地磁盘也节省了网络带宽 备用执行可以用于减小缓慢的机器的影响，及应对机器失败和数据丢失 通俗来说 MapReduce讲的就是分而治之的程序处理理念，把一个复杂的任务划分为若干个简单的任务分别来做。这里重点思想在于并行计算。 举例：统计一篇文章中“的”字的频率，当这篇文章及其长，使用单线程从头到尾计算快还是使用多线程（先将文章分为n段，然后开n个进程，同时计算“的”在各段出现的次数，最后求和）快这里就很明显了吧！ link MapReduce | Wiki 《机器学习实战》 | 第十五章、大数据与MapReduce 《MapReduce: Simplified Data Processing on Large Clusters》]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python定时报警]]></title>
    <url>%2F2019%2F01%2F29%2FPython%E5%AE%9A%E6%97%B6%E6%8A%A5%E8%AD%A6%2F</url>
    <content type="text"><![CDATA[当训练ML模型或者运维等等，等候时间长，浪费时间。所以我一般都是下午和晚上改模型和写代码，凌晨和早上跑代码或者训练模型。但是Python毕竟是动态语言，代码中的bug插件未必很完善。为了防止模型在深夜训练到一半的时候出bug，但有不想一直守在电脑前，下面的方法就比较管用了。 发出报警声音 Mac下: 12while True: print("\a") Windows下: 123import winsound # 这里的lib是python自有的winsound.Beep(2015, 3000)# winsound.Beep(frequency, duration) # 参数是频率和持续时间毫秒数 发送邮件]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Code-Skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-数学]]></title>
    <url>%2F2019%2F01%2F27%2FML-Math%2F</url>
    <content type="text"><![CDATA[linkai-start | math]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ML-Match]]></title>
    <url>%2F2019%2F01%2F21%2FML-Match%2F</url>
    <content type="text"><![CDATA[“样本和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已！” 思路流程 1-数据探索 1-1-可视化 2-数据预处理 2-1-无量纲化 2-2-对定量特征二值化 2-3-对定性特征哑编码 2-4-缺失值计算 2-5-数据变换 3-特征工程 3-1-特征选择 3-1-1-过滤式选择（Filter） 3-1-2-包裹式选择（Wrapper） 3-1-3-嵌入式选择（Embedded） 3-2-降维 3-2-1-主成分分析法（PCA） 3-2-2-线性判别分析法（LDA） 4-模型选择 4-1-模型训练 4-2-交叉验证 5-集成学习 堆叠 6-管道 1-数据探索 工具：Pandas、Matplotlib、Seaborn、Numpy、Jupyter Notebook EDA (Exploratory Data Analysis)，对数据进行探索性的分析 1-1-可视化 常见图表和方法： 查看目标变量的分布：绘制变量之间两两的分布和相关度图表 变量：用 Box Plot 来直观地查看它的分布 坐标数据：用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在 分类问题：将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助 绘制变量之间两两的分布和相关度图表 2-数据预处理 工具：Jupyter Notebook、Sklearn、Pandas、Numpy、Matplotlib 存在问题： 不属于同一量纲 信息冗余 定性特征不能直接使用 将定性特征转换为定量特征 存在缺失值 信息利用率低 2-1-无量纲化无量纲化使不同规格的数据转换到同一规格。 标准化：标准化需要计算特征的均值和标准差：$$x^{‘} = \frac{x - \bar{x}}{S}$$ 区间缩放法：区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放：$$x^{‘} = \frac{x - Min}{Max - Min}$$ 归一化：归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”：$$x^{‘} = \frac{x}{\sqrt{\sum_j^m x[j]^2}}$$ 2-2-对定量特征二值化 定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0： $$x^{‘} =\left{\begin{matrix} 1, x &gt; threshold \ 0, x &lt;= threshold\end{matrix}\right.$$ 2-3-对定性特征哑编码2-4-缺失值计算2-5-数据变换 常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的 3-特征工程 工具：Sklearn、Pandas、Numpy、Matplotlib、Jupyter Notebook或vscode 3-1-特征选择3-1-1-过滤式选择（Filter）先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。 方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征 相关系数法：使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值 卡方检验：经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：$$x^2 = \sum \frac{(A - E)^2}{E}$$ 互信息法：经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：$$I(X;Y) = \sum_{x \epsilon X} \sum_{y \epsilon Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$$ 3-1-2-包裹式选择（Wrapper）选择直接把最终将要使用的学习器的性能作为特征子集的评价标准。 递归特征消除法：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 3-1-3-嵌入式选择（Embedded）将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。 基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维 基于树模型的特征选择法：树模型中GBDT也可用来作为基模型进行特征选择 3-2-降维高维情形下经常会碰到数据样本稀疏、距离计算困难的问题（称为 “维数灾难”），解决方法就是降维。有很多方法，下面是主要的两种方法： 3-2-1-主成分分析法（PCA）3-2-2-线性判别分析法（LDA）4-模型选择 工具：vscode、conda、boost相关（xgboost/catboost/lightgbm） 常用算法： Gradient Boosting Random Forest Extra Randomized Trees SVM Linear Regression Logistic Regression Neural Networks 4-1-模型训练4-2-交叉验证5-集成学习 常见的 Ensemble 方法： Bagging Boosting Blending Stacking 从理论上讲，Ensemble 要成功，有两个要素： Base Model 之间的相关性要尽可能的小 Base Model 之间的性能表现不能差距太大 6-管道 管道：管道是保持数据处理和建模代码组织的简单方法。具体来说，管道束预处理和建模步骤，这样您就可以像使用单个步骤一样使用整个束。 优点： 整洁的代码 较少的bug 更容易重构 模型测试有更多选项 作用： 模块化 | 特征变换：只需写很少的代码就能将新的 Feature 更新到训练集中 自动化 | 网格搜索：只要预先设定好使用的 Model 和参数的候选，就能自动搜索并记录最佳的 Model 自动化 | 集成学习：每个一段时间将现有最好的 K 个 Model 拿来做 Ensemble 提供两个脑图，以供参考 Sklearn类 功能 StandardScaler 无量纲化 MinMaxScaler 无量纲化 Normalizer 归一化 Binarizer 二值化 OneHotEncoder 哑编码 Imputer 缺失值计算 PolynomialFeatures 多项式数据转换 FunctionTransformer 自定义单元数据转换 VarianceThreshold Filter SelectKBest Filter RFE Wrapper SelectFromModel Embedded decomposition PCA lda LDA 链接 如何在 Kaggle 首战中进入前 10% 使用sklearn做单机特征工程 一个很好的可视化示例 一个很好的pipeline示例 一个很好的pipeline示例 轻而易举地击败 Kaggle 科技巨头都爱的Data Pipeline，如何自动化你的数据工作？]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>Match</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册]]></title>
    <url>%2F2019%2F01%2F20%2FPandas%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[这个不是全部API，而是大部分常用的，详细的API介绍还是移步官网 数据结构 系列(Series) | 1维 | 大小不变 数据帧(DataFrame) | 2维 | 大小可变的表结构与潜在的异质类型的列 面板(Panel) | 3维 | 大小可变数组 系列 s = pd.Series(data=ndarray/list/constants, index, dtype, copy=false/true) 创建 数据帧 pandas.DataFrame(data=ndarray/series/map/lists/dict/constant/DataFrame, index, columns, dtype, copy=false/true) 创建 df[&#39;col&#39;] 列选择 df[&#39;col&#39;] = df[&#39;col_1&#39;] + df[&#39;col_2&#39;] 列添加 del df[&#39;col&#39;] 或者 df.pop(&#39;col&#39;) 列删除 df.loc(&#39;index&#39;) 行选择 按标签 df.iloc[col_index_num] 行选择 按整数位置 df[start:end] 行切片 df.append(df_tmp) 附加行 df.drop(row_index_num) 删除行 面板 pandas.Panel(data=ndarray/series/map/lists/dict/constant/DataFrame, items=0, major_axis=1, minor_axis=2, dtype, copy=false/true) 创建（items：每个项目对应于内部包含的数据帧(DataFrame)；major_axis：它是每个数据帧(DataFrame)的索引(行)；minor_axis：它是每个数据帧(DataFrame)的列。） p[&#39;df_num&#39;] 选择数据 使用Item p.major_axis(row_num) 选择数据 行 p.minor_axis(col_num) 选择数据 列 基本功能 系列 s.axes 返回系列的标签列表 s.dtype 返回对象的数据类型 s.empty 返回布尔值，True表示对象是否为空 s.ndim 返回对象的维数 s.size 返回系列的长度 s.values 以数组形式返回系列中的实际数据值 s.head(n) 返回前n行 s.tail(n) 返回最后n行 数据帧 df.T 转置行和列 df.axes 返回行轴标签和列轴标签列表 df.dtypes 返回每列的数据类型 df.empty 返回布尔值，True表示对象是否为空 df.ndim 返回对象的维数 df.shape 返回表示维度的元组 df.size 返回元素数 df.values 将实际数据作为NDarray返回 df.head(n) 返回前n行 df.tail(n) 返回最后n行 描述性统计 df.sum(axis_n) 返回所请求轴的值的总和 df.mean() 返回平均值 df.std() 返回数字列的Bressel标准偏差 df.count() 返回非空观测数量 df.median() 返回所有值的中位数 df.mode() 返回值的模值 df.min() 返回所有值中的最小值 df.max() 所有值中的最大值 df.abs() 返回绝对值 df.prod() 返回数组元素的乘积 df.cumsum() 返回累计总和 df.cumprod() 返回累计乘积 df.describe(include=&#39;object/number/all&#39;) 返回有关列的统计信息的摘要（object：汇总字符串列；number：汇总数字列；all：将所有列汇总在一起。） 函数引用 df.pipe() 表格函数应用：可以通过将函数和适当数量的参数作为管道参数来执行自定义操作 df.apply() 行或列合理函数应用 df.applymap() 元素合理函数应用 重建索引 df.reindex(df_tmp, method=&#39;ffill/backfill/nearest&#39;, limit=n) 重建索引 df.rename(inplace=False/True) 重命名：方法允许基于一些映射(字典或者系列)或任意函数来重新标记一个轴 迭代 df.iteritems() 迭代(key，value)对 df.iterrows() 将行迭代为(索引，系列)对 df.itertuples() 以namedtuples的形式迭代行 排序 df.sort_index(axis=0/1, ascending=True/False) 按标签排序 df.sort_values(by, kind=&#39;mergeesort/heapsort/quicksort&#39;) 按值排序（by列名称；kind排序算法） 字符串和文本数据 lower() 将Series/Index中的字符串转换为小写 upper() 将Series/Index中的字符串转换为大写 len() 计算字符串长度 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符) split(&#39; &#39;) 用给定的模式拆分每个字符串 cat(sep=&#39; &#39;) 使用给定的分隔符连接系列/索引元素 get_dummies() 返回具有单热编码值的数据帧(DataFrame) contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False replace(a,b) 将值a替换为值b repeat(value) 重复每个元素指定的次数 count(pattern) 返回模式中每个元素的出现总数 startswith(pattern) 如果系列/索引中的元素以模式开始，则返回true endswith(pattern) 如果系列/索引中的元素以模式结束，则返回true find(pattern) 返回模式第一次出现的位置 findall(pattern) 返回模式的所有出现的列表 swapcase 变换字母大小写 islower() 检查系列/索引中每个字符串中的所有字符是否小写，返回布尔值 isupper() 检查系列/索引中每个字符串中的所有字符是否大写，返回布尔值 isnumeric() 检查系列/索引中每个字符串中的所有字符是否为数字，返回布尔值 选项和自定义 pd.get_option(param=&#39;display.max_rows/display.max_columns&#39;) 需要一个参数，并返回下面输出中给出的值 pd.set_option(param=&#39;display.max_rows/display.max_columns&#39;, value) 需要两个参数，并将该值设置为指定的参数值 pd.reset_option(param=&#39;display.max_rows/display.max_columns&#39;) 接受一个参数，并将该值设置为默认值 pd.describe_option(param=&#39;display.max_rows/display.max_columns&#39;) 打印参数的描述 pd.option_context() 上下文管理器用于临时设置语句中的选项 常用参数： display.max_rows 要显示的最大行数 display.max_columns 要显示的最大列数 display.expand_frame_repr 显示数据帧以拉伸页面 display.max_colwidth 显示最大列宽 display.precision 显示十进制数的精度 索引和选择数据 df.loc[&#39;row_start&#39;:&#39;row_end&#39;, &#39;col_start&#39;:&#39;col_end&#39;] 完成基于标签的索引 df.iloc[row_start:row_end, col_start:col_end] 获得纯整数索引 df.ix[row_start:row_end, col_start:col_end] 运算符进行选择和子集化对象的混合方法 使用符号 s.loc[indexer] Series df.loc[row_index,col_index] DataFrame p.loc[item_index,major_index, minor_index] Panel df.index 属性访问（可以使用属性运算符.来选择列） 统计函数 df.pct_change() 将每个元素与其前一个元素进行比较，并计算变化百分比 df.cov(df_tmp) 协方差 df.corr() 相关性（相关性显示了任何两个数值(系列)之间的线性关系） s.rank() 数据排名 窗口函数 df.rolling(window=n) n窗口大小的统计值（后加句点式统计函数） df.expanding(min_periods=n) n窗口大小的统计值（累计计算） df.ewm(com, span, halflife) n窗口大小的统计值 缺失数据 df.isnull() 检查缺失值，为空返回True df.notnull() 检查缺失值，不为空返回True 在求和数据时，NA将被视为0 df.fillna(n) 用标量值n替换NaN df.fillna(method=&#39;fill/backfill&#39;) 填写NA前进和后退（fill填充方法向前；backfill填充方法向后） 分组 obj.groupby(‘key’) 或 obj.groupby([‘key1’,’key2’]) 或 obj.groupby(key,axis=1) 将数据拆分成组 df.get_group(&#39;&#39;) 选择一个分组 df_group.transform(lambda) 转换（分组或列上的转换返回索引大小与被分组的索引相同的对象） df.groupby(&#39;&#39;).filter(lambda_func) 过滤（过滤根据定义的标准过滤数据并返回数据的子集） 合并/拼接 pd.merge(left, right, how=&#39;inner/outer/left/right&#39;, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True) （left一个DataFrame对象；right另一个DataFrame对象；on列(名称)连接，必须在左和右DataFrame对象中存在(找到)；left_on左侧DataFrame中的列用作键，可以是列名或长度等于DataFrame长度的数组；right_on 来自右的DataFrame的列作为键，可以是列名或长度等于DataFrame长度的数组；left_index如果为True，则使用左侧DataFrame中的索引(行标签)作为其连接键；how是left, right, outer以及inner之中的一个，默认为内inner；sort照字典顺序通过连接键对结果DataFrame进行排序） 级联 pd.concat(objs, axis=0, join=&#39;outer/inner&#39;, join_axes=None, ignore_index=False) 连接对象（objs是Series，DataFrame或Panel对象的序列或映射；axis是连接的轴；join如何处理其他轴上的索引；join_axes是Index对象的列表；ignore_index是否使用连接轴上的索引值） df.append(df_tmp) 使用附加连接 pd.datetime.now() 获取当前的日期和时间 pd.date_range(&quot;start&quot;, &quot;end&quot;, freq=&quot;&quot;).time 创建一个时间范围 pd.to_datetime() 转换为时间戳 日期功能 pd.date_range(&#39;data&#39;, periods=n, freq=&#39;&#39;) 创建一个日期范围（freq更改日期频率） 偏移别名 略 时间差 pd.Timedelta(&#39;&#39;) 是时间上的差异，以不同的单位来表示 分类数据 pandas.Categorical(values, categories, ordered) 分类构造函数 s.categories 获取对象的类别 obj.ordered 获取对象的顺序 s.cat.categories 重命名类别 Categorical.add.categories() 附加新类别 Categorical.remove_categories() 删除类别 当ordered==True和类别是相同时，所有比较(\==，!=，&gt;，&gt;=，&lt;，和&lt;=)分类数据到另一个分类系列 分类数据的比较 可视化 df.plot() 绘图 df.plot.bar(stacked=False/True) 竖直条形图 df.plot.barh(stacked=True) 水平条形图 df.plot.hist(bins=n) 直方图 df.plot.box() 箱型图 df.plot.area() 区域块图 df.plot.scatter(x=&#39;a&#39;, y=&#39;b&#39;) 散点图形 df.plot.pie(subplots=True) 饼状图 IO工具 pd.read_csv(index_col=[&#39;col_index&#39;], dtype={&#39;col_name&#39;: dtype}, names=[&#39;name&#39;], skiprows=n) 从csv文件中读取数据并创建一个DataFrame对象（dtype转换器；使用names参数指定标题的名称；skiprows跳过指定的行数） 稀疏数据 s.to_sparse() 稀疏Dtypes（稀疏数据应该具有与其密集表示相同的dtype；支持float64，int64和booldtypes） 链接 Pandas 官方文档 莫凡Python | pandas 易百教程 | Pandas教程 Pandas 中文文档 pandas:强大的Python数据分析工具包]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置ML环境]]></title>
    <url>%2F2019%2F01%2F19%2F%E9%85%8D%E7%BD%AEML%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[配置Anaconda简介Anaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。因为包含了大量的科学包，Anaconda 的下载文件比较大。这里就要说明下了，本人一直不喜欢conda的，毕竟太大了。一直在使用pip，但是又发现conda安装貌似还比pip好，pip的安装源不稳定和各种小问题吧。 安装 点击下载anaconda 安装anaconda，点点点就好。。。 anaconda安装完成后 删除原有的python3 切记pyhton2 不要删，unix系统有部分依赖于python2 直接删除python3安装的文件夹 删除软连接/usr/bin/ pip3 python3 找到anaconda的安装路径，我这里是/Users/daidai/anaconda3/bin 添加软连接 ln -s /Users/daidai/anaconda3/bin/python3 /usr/bin/python3 ln -s /Users/daidai/anaconda3/bin/pip /usr/bin/pip3 添加环境变量 sudo vi ~/.bash_profile 打开环境变量的设置文件 export PATH=&quot;/Users/daidai/anaconda3/bin:$PATH&quot; 注意填写自己的路径 source ~/.bash_profile 刷新 conda list 测试环境变量是否正确添加 Jupyter配置 安装Jupyter NbExtensions Configurator conda install -c conda-forge jupyter_contrib_nbextensions conda install -c conda-forge jupyter_nbextensions_configurator 选择插件 Collapsible headings 放下/收起notebook的某些内容 Notify Notify功能就能在任务处理完后及时向你发送通知 Codefolding 折叠代码 Table of Contents 自动生成目录 Autopep8 pep8标准 ExecuteTime 显示单元格的运行时间和耗时 启动 jupyter notebook，浏览器进入http://localhost:8888 安装库安装流行库 xgboost conda install py-xgboost LightGBM conda install lightgbm CatBoost pip3 install catboost 说明： 这里lightgbm、XGBoost、catboost都支持GPU的，我这里是安装CPU版本的方法。 还有Sklearn是不支持GPU计算的，为了提高计算速度；使用CPU训练时，设置n_jobs = -1就是使用多核并行计算。 服务器安装Jupyter Docker https://github.com/jupyterhub/binderhub https://github.com/sagemathinc/cocalc Web https://github.com/jupyterhub/jupyterhub Github 在线 微软 https://mybinder.org/ Google 参考链接 那些在线的Jupyter jupyter 官网 jupyter github-help]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph]]></title>
    <url>%2F2019%2F01%2F14%2FGraph%2F</url>
    <content type="text"><![CDATA[图的概念 图的分类： 有向图和无向图 有权图和无权图 连通图（从任意一个顶点都存在一条路径到达另一个任意顶点）和非连通图 图的表示 邻接矩阵（建议5000*5000以下） 边的数组 邻接表 链式前向星 DFS 和 BFS 定义图结构： 12345678graph = &#123; "A": ["B","C"], "B": ["A", "C", "D"], "C": ["A", "B", "D","E"], "D": ["B", "C", "E","F"], "E": ["C", "D"], "F": ["D"],&#125; 如图A的相邻元素为B、CB的相邻元素为A、C、DC的相邻元素为A、B、D、ED的相邻元素为B、C、E、FE的相邻元素为C、DF的相邻元素为D 广度优先搜索 队列实现：队列，入队列，出队列 BFS优先遍历当前节点的相邻节点，即若当前节点为A时，则继续遍历的节点为B和C；当A的所有相邻节点遍历完以后，再遍历A相邻节点B和C的所有相邻节点，以B为例，在遍历B的相邻节点时，由于A已被访问过，则需要标记为已访问，在遍历B的相邻节点时，不再需要访问A。以此类推，完成无向图的BFS。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding: utf-8 -*-def BFS(graph, vertex): # 使用列表作为队列 queue = [] # 将首个节点添加到队列中 queue.append(vertex) # 使用集合来存放已访问过的节点 looked = set() # 将首个节点添加到集合中表示已访问 looked.add(vertex) # 当队列不为空时进行遍历 while(len(queue) &gt; 0): # 从队列头部取出一个节点并查询该节点的相邻节点 temp = queue.pop(0) nodes = graph[temp] # 遍历该节点的所有相邻节点 for w in nodes: # 判断节点是否存在于已访问集合中,即是否已被访问过 if w not in looked: # 若未被访问,则添加到队列中,同时添加到已访问集合中,表示已被访问 queue.append(w) looked.add(w) print(temp, end=' ') print()if __name__ == "__main__": graph = &#123; "A": ["B", "C"], "B": ["A", "C", "D"], "C": ["A", "B", "D", "E"], "D": ["B", "C", "E", "F"], "E": ["C", "D"], "F": ["D"], &#125; # 由于无向图无根节点，则需要手动传入首个节点，此处以"A"为例 print("BFS", end="\t") BFS(graph, "A")'''# 结果BFS A B C D E F''' 深度优先搜索 栈和回溯方式实现：栈，压栈，出栈 递归方式实现 DFS优先遍历与当前节点0相邻的一个节点1，然后再访问与节点1相邻但与节点0不相邻的节点，即若当前节点为A，则继续遍历B或C，再访问与B或C节点相邻且与A节点不相邻的节点，即节点D或E，若没有未遍历过的相邻节点，则返回访问上一个有未被访问过相邻节点的节点进行访问，依此遍历整个图，完成无向图的DFS。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding: utf-8 -*-def DFS(graph, vertex): # 使用列表作为栈 stack = [] # 将首个元素添加到队列中 stack.append(vertex) # 使用集合来存放已访问过的节点 looked = set() # 将首个节点添加到集合中表示已访问 looked.add(vertex) # 当队列不为空时进行遍历 while len(stack) &gt; 0: # 从栈尾取出一个节点并查询该节点的相邻节点 temp = stack.pop() nodes = graph[temp] # 遍历该节点的所有相邻节点 for w in nodes: # 判断节点是否存在于已访问集合中,即是否已被访问过 if w not in looked: # 若未被访问,则添加到栈中,同时添加到已访问集合中,表示已被访问 stack.append(w) looked.add(w) print(temp, end=' ') print()if __name__ == "__main__": graph = &#123; "A": ["B", "C"], "B": ["A", "C", "D"], "C": ["A", "B", "D", "E"], "D": ["B", "C", "E", "F"], "E": ["C", "D"], "F": ["D"], &#125; # 由于无向图无根节点，则需要手动传入首个节点，此处以"A"为例 print("DFS", end="\t") DFS(graph, "A")'''# 结果DFS A C E D F B''' DFS对比BFS DFS BFS 用途 搜索全部解 搜索最短路 优劣 占用内存较小 占用内存较大 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# -*- coding: utf-8 -*-def BFS(graph, start, end): # 使用列表作为队列 queue = [] # 将首个节点添加到队列中 queue.append(start) # 使用集合来存放已访问过的节点 looked = set() # 将首个节点添加到集合中表示已访问 looked.add(start) # 当队列不为空时进行遍历 while(len(queue) &gt; 0): # 从队列头部取出一个节点并查询该节点的相邻节点 temp = queue.pop(0) nodes = graph[temp] # 遍历该节点的所有相邻节点 for w in nodes: # 判断节点是否存在于已访问集合中,即是否已被访问过 if w not in looked: # 若未被访问,则添加到队列中,同时添加到已访问集合中,表示已被访问 queue.append(w) looked.add(w) if w == end: print(temp, ' ', end) return print(temp, end=' ') print()def DFS(graph, start, end): # 使用列表作为栈 stack = [] # 将首个元素添加到队列中 stack.append(start) # 使用集合来存放已访问过的节点 looked = set() # 将首个节点添加到集合中表示已访问 looked.add(start) # 当队列不为空时进行遍历 while len(stack) &gt; 0: # 从栈尾取出一个节点并查询该节点的相邻节点 temp = stack.pop() nodes = graph[temp] # 遍历该节点的所有相邻节点 for w in nodes: # 判断节点是否存在于已访问集合中,即是否已被访问过 if w not in looked: # 若未被访问,则添加到栈中,同时添加到已访问集合中,表示已被访问 stack.append(w) looked.add(w) if w == end: print(temp, ' ', end) return print(temp, end=' ') print()if __name__ == "__main__": graph = &#123; "A": ["B", "C"], "B": ["A", "C", "D"], "C": ["A", "B", "D", "E"], "D": ["B", "C", "E", "F"], "E": ["C", "D"], "F": ["D"], &#125; # 由于无向图无根节点，则需要手动传入首个节点，此处以"A"为例 print("BFS", end="\t") BFS(graph, "A", "F") print("-----") print("DFS", end="\t") DFS(graph, "A", "F")'''# 结果BFS A B C D F-----DFS A C E D F''']]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[libsvm简介]]></title>
    <url>%2F2019%2F01%2F14%2Flibsvm%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[介绍 LIBSVM 是支持向量分类的集成软件，（C-SVC，nu-SVC），回归（epsilon-SVR，nu-SVR）和分布估计（one-class SVM）。支持多级分类。 SVC nu-SVC SVR nu-SVR Working Set Selection Using Second Order Informationfor Training Support Vector Machines libsvm包括： 不同的SVM公式化表述 高效的多类分类 模型选择的交叉验证 概率估计 各种内核（包括预计算的内核矩阵） 不平衡数据的加权支持向量机 C++和Java源 GUI显示支持向量机分类和回归 支持多种语言Python、R、Matlab等 能够产生交叉验证精度轮廓的自动模型选择。 支持向量机的实用指南1 介绍 这里只适合新手快速学习得到较好的结果，但不是最优的结果。 一个分类器通常把数据分为训练集合测试集。训练集中的每一个实例都包含一个目标值和属性。SVM的目标是生成一个模型（基于训练数据），该模型在给定测试数据属性的的情况下预测测试数据的目标值。 给定实例标签对的训练集$(x_i, y_i), i = 1, …, l$，需要解决以下优化问题： $$\min_{w, b, \xi} \frac{1}{2} W^TW + C\sum^l_{i =1}\xi_i$$ $$从属于 y_i(W^T \phi(X_i) + b) \geqslant 1 - \xi_i, \qquad (1)$$ $$\xi_i \geqslant 0.$$ 这里训练向量$X_i$通过函数映射到一个更高的维度空间$\phi$。SVM在高维空间中找到具有最大边缘的线性分离超平面。$C &gt; 0$是错误项的惩罚参数。此外，$K(X_I, X_j) \equiv \phi (X_i)^T \phi(X_i)$是被叫做核函数。尽管研究人员正在提出新的内核，初学者可能会在SVM书籍中找到下面四个基本内核： 线性：$K(X_i, X_j) = X_i^TX^j$ 多项式：$K(X_i, X_j) = (\gamma X_i^TX_j + r)^d, \gamma &gt; 0$ 径向基函数 （RBF）：$K(X_i, X_j) = \exp(-\gamma \left | X_i - X_j \right |^2), \gamma &gt; 0$ S型函数：$K(X_i, X_j) = \tanh(\gamma X_i^TX_j + r)$这里$\gamma,r$和$d$是核参数。 1.2 拟议程序 许多初学者现在使用以下步骤： 将数据转换为SVM包的格式 随机尝试一些内核和参数 测试 我们建议初学者首先尝试以下步骤： 将数据转换为SVM包的格式 对数据进行简单的缩放 考虑RBF内核 $K(X, Y) = e^{-\gamma \left | X - Y \right |^2}$ 使用交叉验证确定最佳参数$C$和$\gamma$ 使用最佳参数$C$和$\gamma$训练整个训练集 测试 我们将在下面的章节中详细讨论这个过程。 2 数据预处理2.1 分类特征 SVM要求每个数据实例都表示为实数向量。因此，如果存在分类属性，我们首先必须将它们转换为n数值数据。我们建议使用$m$个数来代表$m$类属性。只有一个数字是一，其他的是零。例如，三类属性例如{red, green, blue}能用(1, 0, 0)、(0, 1, 0)和(1, 0, 0)来表示。我们的经验表明，如果一个属性中的值的数目不太大，这种编码可能比使用单个数字更稳定。 2.2 缩放 应用SVM前的缩放非常重要。 Part 2 of Sarle’s Neural NetworksFAQ Sarle (1997说明了这一点的重要性，以及大多数考虑事项也适用于SVM。缩放的主要优点是避免较大数值范围内的属性占较小数值范围内的属性。另一个优点是在计算过程中避免了数值计算。因为核值通常依赖于特征向量的内积。例如：线性核和多项式核、大属性值可能会导致数值问题。我们建议将每个属性线性地调整为范围[-1, +1]和[0, 1]。 当然，我们必须使用相同的方法来衡量培训和测试数据。例如，假设我们从[-10, +10]到[-1, +1]。如果测试数据的属性在范围内[-11, +8]，我们必须将测试数据扩展到[-1.1, +0.8]。 3 模型选择 虽然第一节中只提到四种常见的核，但我们必须首先确定哪种谷粒。然后选择惩罚参数和内核参数。 3.1 RBF 核 在选择核函数时通常RBF核是作为第一选择，这个核函数将样本非线性的映射到了一个更高维的空间，所以，不同于线性核，RBF核可以处理类标签和属性非线性相关的情况。此外，Keerthi and Lin (2003)提出线性核其实是RBF核的一种特殊情况，因为具有惩罚系数$\tilde{C}$的线性核和具有参数$(C, \gamma)$的RBF核有相同的性能。另外，sigmois核在特定的参数下也会表现得与RBF核类似(Lin and Lin, 2003)。 第二个原因就是超参数的个数会影响模型选择的复杂度，而多项式核具有比RBF核更多的超参数。 最后，RBF核具有更少的数值计算困难。关键的一点就是RBF核的值域固定在$0 &lt; K_{ij} \leqslant 1$，相比之下多项式核的值在次数非常大的时候可能会取到无穷$(\gamma X_i^TX_j + r &gt; 1)$或者零$(\gamma X_i^TX_j + r &lt; 1)$。另外，我们必须要注意sigmoid核在一些参数下是无效的（即不是两个向量的内积）(Vapnik, 1995)。 这里还有一些RBF核不合适使用的情况，特别是当特征的数量特别大时，这时可能更好使用线性核。我们将在附录C进行详细讨论。 3.2 交叉验证和梯度搜索 在RBF核和中主要有两个参数：$C$和$\gamma$，我们先前并不知道对于给定的问题$C$和$\gamma$最佳的取值是多少，所以我们有必要进行模型选择（参数搜索）。我们的目标是找到一对足够好的$(C, \gamma)$使得分类器可以准确的预测未知的数据（即测试集），请注意我们不必要去追求很高的训练准确率（即训练器可以准确预测那些标签已知的训练数据）。综上所述，一个常用的策略就是将数据集分成两部分，其中一个被认为是未知的（另一个被认为已知的用来训练模型），从这个“未知”的数据集中获得的预测精度可以准确的反应分类器在分类一个独立的数据集的性能，此过程的改进版本就被成为交叉验证。 对于v-折交叉验证，我们首先将训练集分割成v个大小相同的子数据集，接下来用其中v-1个子数据集训练的分类器来测试剩下的那个子数据集。因此，训练集中的每个样本都被预测过一次，交叉验证的精度就等于正确分类的样本所占训练集样本总数的百分比。 交叉验证还可以防止过拟合问题，如图1所示通过一个二分类问题来阐述这个问题，其中实心圆圈和三角形是训练数据，而空心圆和三角形是测试数据。在图1a和1b中由于过拟合，分类器测试的准确度并不理想，如果我们把在图1a和1b中的训练和测试数据看成是交叉验证中的训练集和验证集，结果还是不太好，另一方面，图1c和1d所示的分类器没有过拟合并且在获得更好的交叉验证结果的同时测试精度也提高了。 我们推荐利用交叉验证的方法通过梯度搜索来确定$C$和$\gamma$，通过对$(C, \gamma)$不同的组合进行测试并选择其中具有最大交叉验证精度的组合，我们发现对$C$和$\gamma$使用指数增长是一个实用的方法来找到满意的参数（例如，$C = 2^{-5}, 2^{-3}, …, 2^{15}, \gamma = 2^{-15}, 2^{-13}, …, 2^3$）。 梯度搜索是一种简单直接但是似乎稍显天真的方法，事实上，这里还有一些更加先进，可以节省更多计算开销的方法，例如，估计交叉验证率。然而，这里有两个原因来说明我们为什么还是更喜欢这个简单的梯度搜索方法。 第一，从心理上，我们使用那些通过近似或启发式来避免进行全局搜索的方法可能会觉得不靠谱。另一个原因是通过梯度搜索来寻找满意参数所需的计算时间并不会比那些先进的方法更多，因为这里只有两个参数需要确定。另外，梯度搜索可以很容易实现并行化，因为每次交叉验证都是独立的，而很多先进的算法，比如，walking along a path是一个迭代的过程，很难实现并行化。 不过完成一个完整梯度搜索仍然是十分耗时的，我们建议在开始的时候使用粗略的梯度，在找到一个最佳的梯度区间后，再在该区间进行精细的梯度搜索。为了说明这个过程，我们通过german问题来做一个实验，在进行缩放后，我们一开始使用一个粗略的梯度（如图2所示）找到最佳的$(C, \gamma)$取值是$(2^3, 2^{-5})$——拥有77.5%的交叉验证精度。接下来我们在$(2^3, 2^{-5})$附近进行精细的提度搜索（如图3所示）并且在$(2^{3.25}, 2^{-5.25})$获得了最佳的交叉验证精度77.6%。再找到了最佳的$(C, \gamma)$对后，重新训练整个训练集得到最终的分类器模型。 以上的方法在上千或者更多的数据大小时工作效果良好，而对于超大型数据集来说，一个可行的方法是随机选择数据集的一个子集，在该子集上进行粗略的梯度搜索，再在整个数据集上进行精细的梯度搜索。 4 讨论 以上提到的方法可能在一些情况下表现不会足够好，所以其他一些技术，例如，特征选择还是需要的，不够这些内容已经超越本指导的范畴了。我们的实验证实了这些处理方法对于那些没有很多特征的数据效果显著。当需要处理成千上万的属性时，在将数据传递给SVM之前或许需要先对它们进行挑选。 B 扩展训练和测试数据中的常见错误C 何时使用线性而不是RBF内核C.1 实例数远小于特征数C.2 实例和特征数都很大C.3 实例数远大于特征数 Reference Link LIBSVM – A Library for Support Vector Machines LIBSVM Tools guide.pdf-(支持向量分类的实用指南) LIBSVM新手指导——《A Practical Guide to Support Vector Classiﬁcation》翻译 支持向量机分类入门实用指南 github libsvm libsvm中文使用文档 实用libsvm入门笔记(A Practical Guide to Support Vector Classication)]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP Alorithm]]></title>
    <url>%2F2019%2F01%2F14%2Fkmp%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[朴素匹配 设待匹配串长度为$n$，模版串长度为$m$。 朴素匹配复杂度为$O(m(n - 1))$ 12345678910111213141516171819# 暴力字符串匹配算法def match_str(string_, pattern_): ''' input: string_: 待匹配串 pattern_: 模板串 output: &gt;= 0: 找到，并返回首次匹配位置 &lt; 0: 未找到 ''' for i in range(len(string_) - len(pattern_)): j = 0 while j &lt; len(pattern_): if string_[i + j] != pattern_[j]: break j += 1 if j == len(pattern_): return i return -1 KMP算法 这里就要讲讲部分匹配表，又称为失配函数，作用是让算法无需多次匹配S中的任何字符。 计算部分匹配表 上图中的部分匹配表为： 对应字符 a b a b b 下标 0 1 2 3 4 部分匹配值 0 0 1 2 0 部分匹配值就是”前缀”和”后缀”的最长的共有元素的长度。以ababb为例： a的前缀和后缀都为空集，共有元素的长度为0 ab前缀为[a]，后缀为[b]，共有元素的长度为0 aba前缀为[a, ab]，后缀为[a, ba]，共有元素为[a]，长度为1 abab前缀为[a, ab, aba]，后缀为[b, ab, bab]，共有元素为[ab]，长度为2 ababb前缀为[a, ab, aba, abab]，后缀为[b, bb, abb, babb]，共有元素的长度为0 “部分匹配”的实质是，有时候，字符串头部和尾部会有重复。当在匹配值出不匹配时，模板串后移的位数 = 已匹配的字符数 - 对应的部分匹配值。 12345678910111213141516# 计算模板串的部分匹配表def pmt(pattern_): ''' input: pattern_: 模板串 output: dfa: 部分匹配表数组 ''' dfa = [] for j in range(len(pattern_)): max_, tmp = 0, pattern_[:j + 1] for i in range(len(tmp)): if tmp[:i] == tmp[len(tmp) - i:] and max_ &lt; i: max_ = i dfa.append(max_) return dfa 全部代码： 123456789101112131415161718192021222324252627282930313233343536373839# kmp算法def kmp(string_, pattern_): ''' input: string_: 待匹配串 pattern_: 模板串 output: &gt;= 0: 找到，并返回首次匹配位置 &lt; 0: 未找到 ''' # 计算模板串的部分匹配表 def pmt(pattern_): ''' input: pattern_: 模板串 output: dfa: 部分匹配表数组 ''' dfa = [] for j in range(len(pattern_)): max_, tmp = 0, pattern_[:j + 1] for i in range(len(tmp)): if tmp[:i] == tmp[len(tmp) - i:] and max_ &lt; i: max_ = i dfa.append(max_) return dfa dfa = pmt(pattern_) # 计算部分匹配表 j, i, len_string_, len_pattern_ = 0, 0, len(string_), len(pattern_) while j &lt; len_string_: if string_[j] == pattern_[i]: if i == len_pattern_ - 1: return j - len_pattern_ + 1 j, i = j + 1, i + 1 elif i &gt; 0: # 状态转移 i = dfa[i - 1] else: j += 1 return -1 时间复杂度$O(n)$ Cpp完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;using namespace std;// 计算模板串部分匹配表vector&lt;int&gt; pmt(string pat) &#123; vector&lt;int&gt; dfa; for (int j = 0; j &lt; pat.size(); j++) &#123; int max = 0; for (int i = 0; i &lt; j + 1; i++) &#123; vector&lt;string&gt; start, end; for (int n = 0; n &lt; i; n++) &#123; string tmp; tmp = pat[n]; start.push_back(tmp); tmp = pat[j + 1 - i]; end.push_back(tmp); &#125; if (start == end &amp;&amp; max &lt; i) &#123; max = i; &#125; &#125; dfa.push_back(max); &#125; return dfa;&#125;// kmp 算法int kmp(string str, string pat) &#123; vector&lt;int&gt; dfa = pmt(pat); // 计算部分匹配表 int j, i = 0; int len_str = str.size(); int len_pat = pat.size(); while (j &lt; len_str) &#123; if (str[j] == pat[i]) &#123; if (i == len_pat - 1) &#123; return j - len_pat + 1; &#125; j++; i++; &#125; else if (i &gt; 0) &#123; // 状态转移 i = dfa[i - 1]; &#125; else &#123; j++; &#125; &#125; return -1;&#125;// 主函数int main() &#123; string str = "123234562476qvregerv"; string pat = "76qv"; cout &lt;&lt; kmp(str, pat) &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用最小二乘法拟合椭圆]]></title>
    <url>%2F2019%2F01%2F12%2F%E5%88%A9%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%8B%9F%E5%90%88%E6%A4%AD%E5%9C%86%2F</url>
    <content type="text"><![CDATA[数学推导这里$x_i$和$y_i$分别就是$i$点的$x$轴坐标和$y$轴坐标。这里是对$i$个点进行拟合。 在原始测得的N（N≥5）组数据（$x_i$，$y_i$），（$i=1,2,3,…,N$）中，根据椭圆方程通式和最小二乘法原理，求目标函数 $$F(A, B, C, D, E) = \sum^N_{i = 1}(x^2_i + Ax_iy_i + By_i^2 + Cx_i + Dy_i + E)^2$$ 的最小值来确定参数A、B、C、D和E。令F（A，B，C，D，E）对各个参数的偏导数均为零，得到以下方程组： $$\begin{bmatrix} \sum^N_{i = 1} x_i^2y_i^2 &amp; \sum^N_{i = 1} x_iy_i^3 &amp; \sum^N_{i = 1} x_i^2y_i &amp; \sum^N_{i = 1} x_iy_i^2 &amp; \sum^N_{i = 1} x_iy_i \ \sum^N_{i = 1} x_iy_i^3 &amp; \sum^N_{i = 1} y_i^4 &amp; \sum^N_{i = 1} x_iy_i^2 &amp; \sum^N_{i = 1} y_i^3 &amp; \sum^N_{i = 1} y_i^2 \ \sum^N_{i = 1} x_i^2y_i &amp; \sum^N_{i = 1} x_iy_i^2 &amp; \sum^N_{i = 1} x_i^2 &amp; \sum^N_{i = 1} x_iy_i &amp; \sum^N_{i = 1} x_i \ \sum^N_{i = 1} x_iy_i^2 &amp; \sum^N_{i = 1} y_i^3 &amp; \sum^N_{i = 1} x_iy_i &amp; \sum^N_{i = 1} y_i^2 &amp; \sum^N_{i = 1} y_i \ \sum^N_{i = 1} x_iy_i &amp; \sum^N_{i = 1} y_i^2 &amp; \sum^N_{i = 1} x_i &amp; \sum^N_{i = 1} y_i &amp; N \\end{bmatrix}\begin{bmatrix} A \ B \ C \ D \ E \\end{bmatrix} =\begin{bmatrix} \sum^N_{i = 1} x_i^3y_i \ \sum^N_{i = 1} x_i^2y_i^2 \ \sum^N_{i = 1} x_i^3 \ \sum^N_{i = 1} x_i^2y_i \ \sum^N_{i = 1} x_i^2 \\end{bmatrix}$$ 可以写为 $$M\begin{bmatrix} A \ B \ C \ D \ E \\end{bmatrix} =N$$ 则$\begin{bmatrix} A \ B \ C \ D \ E \end{bmatrix} = N \backslash M$即可得到拟合系数。 然后求解参数$x_0, y_0, a, b, q$ $x_0 = \frac{B·E - 2·C·D}{4·A·C - B^2}$$y_0 = \frac{B·D - 2·A·E}{4·A·C - B^2}$$a = \sqrt{\frac{2·A·x_0^2 + 2·C·y_0^2 + 2·B·x_0·y_0 - 2}{A + C + \sqrt{(A - C)^2 + B^2}}}$$b = \sqrt{\frac{2·A·x_0^2 + 2·C·y_0^2 + 2·B·x_0·y_0 - 2}{A + C - \sqrt{(A - C)^2 + B^2}}}$$q = \frac{1}{2} \arctan \frac{B}{A - C}$ 求解得到椭圆的中心点坐标$x_0$和$y_0$、椭圆的长轴$a$和短轴$b$、倾斜的弧度$q$ 论文可以参考论文Direct Least Squares Fitting of Ellipses 算法思想： 算法通过最小化约束条件4ac-b^2 = 1，最小化距离误差。利用最小二乘法进行求解，首先引入拉格朗日乘子算法获得等式组，然后求解等式组得到最优的拟合椭圆。 算法的优点： a、椭圆的特异性，在任何噪声或者遮挡的情况下都会给出一个有用的结果； b、不变性，对数据的Euclidean变换具有不变性，即数据进行一系列的Euclidean变换也不会导致拟合结果的不同； c、对噪声具有很高的鲁棒性； d、计算高效性。 算法原理：]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell输入输出重定向和管道命令]]></title>
    <url>%2F2019%2F01%2F12%2FShell%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E5%92%8C%E7%AE%A1%E9%81%93%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[输入/输出重定向command &gt; file 将输出重定向到file123456789➜ command touch log.txt➜ command lslog.txt➜ command cat log.txt➜ command ps &gt; log.txt➜ command cat log.txt PID TTY TIME CMD17928 pts/0 00:00:01 zsh21688 pts/0 00:00:00 ps command &lt; file 将输入重定向到file1234➜ command wc -l command.txt1 command.txt➜ command wc -l &lt; command.txt1 command &gt;&gt; file 将输出以追加的方式重定向到file1234567891011121314➜ command lslog.txt➜ command cat log.txt PID TTY TIME CMD17928 pts/0 00:00:01 zsh21688 pts/0 00:00:00 ps➜ command ps &gt;&gt; log.txt➜ command cat log.txt PID TTY TIME CMD17928 pts/0 00:00:01 zsh21688 pts/0 00:00:00 ps PID TTY TIME CMD17928 pts/0 00:00:01 zsh21821 pts/0 00:00:00 ps &lt;&lt; tag 将输入重定向到一个交互式Shell脚本或程序1234command &lt;&lt; delimiter documentdelimiter# 作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command 123456➜ command wc -l &lt;&lt; EOFheredoc&gt; helloheredoc&gt; worldheredoc&gt; !heredoc&gt; EOF3 n &gt;&amp; m 将输出文件 m 和 n 合并。 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息 command &gt; file 将 stdout 重定向到 file 12➜ command &gt; file 2&gt;&amp;1# 将 stdout 和 stderr 合并后重定向到 file n &lt;&amp; m 将输入文件 m 和 n 合并。 command &lt; file 将stdin 重定向到 file 12➜ command &lt; file1 2&lt;&amp;0# 将 stdin 和 stderr 合并后重定向到 file 管道命令command_1; command_2; command_3; 顺序执行多条命令12➜ command touch file.txt; ls;file.txt command &amp; 表示任务在后台执行12345678➜ test_code cat csdn_spider.py#!/usr/bin/env python# coding=utf-8。。。➜ test_code python csdn_spider.py &amp;[1] 19020➜ test_code ps -aux | grep pythonroot 15076 0.3 0.9 60224 17444 ? SN 2018 155:29 python3 csdn_spider.py command_1 &amp;&amp; command_2 表示前一条命令执行成功后才会执行下一条命令 前一条命令执行成功 123456➜ test_code lshello.py tensorboard.py test.py test.py.tar.gz➜ test_code cat file &amp;&amp; rm filecat: file: No such file or directory➜ test_code lshello.py tensorboard.py test.py test.py.tar.gz 前一条命令执行失败 123456789➜ test_code lshello.py tensorboard.py test.py test.py.tar.gz➜ test_code cat hello.py &amp;&amp; rm hello.py#!/usr/bin/env python# coding=utf-8print('hello')➜ test_code lstensorboard.py test.py test.py.tar.gz command_1 | command_2 表示管道，上一条命令的输出作为下一条命令的参数123456➜ test_code ps PID TTY TIME CMD17928 pts/0 00:00:00 zsh19839 pts/0 00:00:00 ps➜ test_code ps | grep zsh17928 pts/0 00:00:00 zsh command_1 || command_2 表示上一条命令执行失败后，才会执行下一条命令 上一条命令执行失败 12345➜ command ls➜ command cat file.txt || touch file.txtcat: file.txt: No such file or directory➜ command lsfile.txt 上一条命令执行成功 12345➜ command lsfile.txt➜ command cat file.txt || rm file.txt➜ command lsfile.txt 其他运算符(command_1; command_2; command_3) 执行圆括号内一组命令12345678➜ command lscommand.txt➜ command ls; (ls; ps)command.txtcommand.txt PID TTY TIME CMD17928 pts/0 00:00:02 zsh25249 pts/0 00:00:00 ps {command_1; command_2; command_3;} 执行大括号内一组命令12345678910111213➜ command lscommand.txt➜ command ls; &#123;ls; ps&#125;command.txtcommand.txt PID TTY TIME CMD17928 pts/0 00:00:02 zsh25283 pts/0 00:00:00 ps➜ command ls; &#123;ls; ps;&#125; | grep zshcommand.txt17928 pts/0 00:00:02 zsh➜ command &#123;ls; ps;&#125; | grep zsh17928 pts/0 00:00:02 zsh 这里()和{}作用相近，但是又有区别。 区别： () 括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用 {} 在当前shell顺序执行命令，括号中的变量可以被脚本余下的部分使用 在Shell交互式命令行中区别不大，可以忽略；在bash脚本中区别较大。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++_STL]]></title>
    <url>%2F2019%2F01%2F05%2Fcpp_STL%2F</url>
    <content type="text"><![CDATA[为了面试刷题，学习了下cpp，然后cpp的stl文档感觉不太好，自己总结了下！ 通用库I/O库 &lt;iostream&gt;数学函数 &lt;cstdlib&gt; abs(n) 返回n的绝对值 pow(a, b) 返回a的b次方 sqrt(n) 返回n的平方根 字符串库 &lt;string&gt;1sting str; // 定义str 容量 str.empty() 若str为空则返回1，否则返回0 str.size() 返回str的长度 操作 str.clear() 清除str str.insert(n, tmp) 在字符串str的第n位插入tmp str.erase(begin, end, tmp) 移除str的begin到end之间的tmp字符 str.push_back(tmp) 将tmp附加到str结尾 str.pop_back() 从字符串str移除末尾字符 str.append(tmp) 后附tmp到str末尾 str.replace(n, num, tmp) str从第n字符开始num个字符替换为tmp 查找 str.find(tmp, n) 在str的第n个字符开始搜索tmp，返回首次匹配的位置 str.rfind(tmp, n) 在str的第n个字符开始搜索tmp，返回最后匹配的位置 容器库Vector库 &lt;vector&gt;1234vector&lt;type&gt; vec(n); // 定义vec的类型是type，大小为nvector&lt;type&gt; vec(n); // 定义vec的类型是type，赋值为1， 2， 3vector&lt;int&gt; arr(2);vector&lt;double&gt; arr&#123;1, 2, 3&#125;; 容量 vec.empty() 如果vec非空则返回0，否则返回1 vec.size() 返回vec中的元素个数 修改器 vec.clear() 清除vec vec.insert() vec.erase(begin, end) 移除begin到end中的元素 vec.erase(begin) 移除begin以后的元素 vec.push_back(tmp) 将tmp后附到vec尾部 vec.resize(n) 重置vec可以容纳n和元素 Set库 &lt;set&gt;1set&lt;type&gt; set_; 容量 set.empty() 如果set为空返回0，否则返回1 set.size() 返回集合中元素的个数 修改器 set.clear() 清除set set.insert(tmp) set中插入tmp set.erase(tmp) set中删除tmp 查找 123set&lt;int&gt;::iterator iter;iter = set.find(tmp); // 在set中查找tmp，找到返回tmp的值cout &lt;&lt; *iter &lt;&lt; endl; // 返回值 Map库 &lt;map&gt;12map&lt;key-type, value-type&gt; dict; // 定义dict及其键值类型map&lt;int, string&gt; dict; 容量 dict.empty() 如果dict为空返回0，否则返回1 dict.size() 返回map中元素的个数 修改器 dict.clear() 删除dict dict.insert(pair&lt;int, string&gt;(key, value)) 在dict中插入key-value dict.erase(key) 删除key 查找 123456auto search = dict.find(1);if (search != dict.end()) &#123; cout &lt;&lt; true &lt;&lt; endl; // 找到，打印1&#125; else &#123; cout &lt;&lt; false &lt;&lt; endl; // 未找到，打印0&#125; 算法库 &lt;algorithm&gt;排序操作 sort(arr.begin(), arr.end()); 对arr进行升序排序 二分搜索操作 binary_search(arr.begin(), arr.end(), tmp) 对已排序序列arr，进行二分查找tmp，找到返回1，否则返回0]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coder基础知识体系]]></title>
    <url>%2F2018%2F12%2F08%2Fcoder%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[为什么要阅读长篇小说，因为中短篇小说就像用针扎你，而长篇小说就像把你装进一个沙袋里吊起来，从四面八方用狼牙棒打你，酣畅淋漓。 知识体系 计算机基础理论 计算机模型：内存/IO/时钟/CPU…… 算法 专项技术领域： 数据挖掘 数据管理 智能推荐 搜索 …… 语言与工具 语言与相关体系 开发工具，分析工具，代码管理工具 HTML/CSS/JS/Ajax 常用框架与第三方类库 调试与测试 调试方法和哲学 定位问题 BUG管理工具 单元测试 集成测试 性能测试 安全测试 兼容性测试与方法 JS/Ajax测试与方法 服务层测试 Web层测试 网络与系统 TCP/IP协议与模型，HTTP/SMTP等协议 Linux系统，网络分析工具，系统分析工具 容量，流量与负载均衡 应用部署、规范、规划 安全 监控与故障分析 磁盘与存储 Shell DNS与域名 缓存，反向代理 图片服务器（海量小文件） 需求挖掘与分析 需求文档格式 需求访谈 需求分析方法，需求分析工具 领域知识与经验 系统分析与设计 UML语言与模型 分析模式 设计模式，领域驱动 系统分析文档格式 系统设计文档格式 功能性需求与非功能性需求 数据与系统 数据库 可伸缩策略，扩展策略，备份，容灾，性能，安全，高可用…… 数据设计与范式，SQL/NoSQL，Cache，分布式文件 架构设计 架构模式，典型互联网公司架构演进历史 架构原则，常用策略 架构设计方法 非功能性理解 扩展性 伸缩性 稳定性 一致性 性能 吞吐量 容量预测与规划 架构体系与相关技术 过程与管理 分析过程 研发过程 评审过程 测试过程 发布过程 回滚过程 文档管理 知识管理 项目管理 ~你学过的每一样东西，你遭受的每一次苦难，都会在你一生中的某个时候派上用场。——佩内洛普·菲兹杰拉德 《离岸》 Everything that you’ve learnt and all the hardships you’ve suffered will all come in handy at some point in your life.]]></content>
      <categories>
        <category>Think</category>
      </categories>
      <tags>
        <tag>Think</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拟合散点方程]]></title>
    <url>%2F2018%2F11%2F25%2F%E6%8B%9F%E5%90%88%E6%95%A3%E7%82%B9%E6%96%B9%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多点拟合 leastsq的doc https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html scipy的GitHub看拟合问题 https://github.com/scipy/scipy/issues?utf8=%E2%9C%93&amp;q=scipy.optimize.leastsq 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181def leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-8, xtol=1.49012e-8, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None): """ Minimize the sum of squares of a set of equations. :: x = arg min(sum(func(y)**2,axis=0)) y Parameters ---------- func : callable should take at least one (possibly length N vector) argument and returns M floating point numbers. It must not return NaNs or fitting might fail. x0 : ndarray The starting estimate for the minimization. args : tuple, optional Any extra arguments to func are placed in this tuple. Dfun : callable, optional A function or method to compute the Jacobian of func with derivatives across the rows. If this is None, the Jacobian will be estimated. full_output : bool, optional non-zero to return all optional outputs. col_deriv : bool, optional non-zero to specify that the Jacobian function computes derivatives down the columns (faster, because there is no transpose operation). ftol : float, optional Relative error desired in the sum of squares. xtol : float, optional Relative error desired in the approximate solution. gtol : float, optional Orthogonality desired between the function vector and the columns of the Jacobian. maxfev : int, optional The maximum number of calls to the function. If `Dfun` is provided then the default `maxfev` is 100*(N+1) where N is the number of elements in x0, otherwise the default `maxfev` is 200*(N+1). epsfcn : float, optional A variable used in determining a suitable step length for the forward- difference approximation of the Jacobian (for Dfun=None). Normally the actual step length will be sqrt(epsfcn)*x If epsfcn is less than the machine precision, it is assumed that the relative errors are of the order of the machine precision. factor : float, optional A parameter determining the initial step bound (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``. diag : sequence, optional N positive entries that serve as a scale factors for the variables. Returns ------- x : ndarray The solution (or the result of the last iteration for an unsuccessful call). cov_x : ndarray Uses the fjac and ipvt optional outputs to construct an estimate of the jacobian around the solution. None if a singular matrix encountered (indicates very flat curvature in some direction). This matrix must be multiplied by the residual variance to get the covariance of the parameter estimates -- see curve_fit. infodict : dict a dictionary of optional outputs with the key s: ``nfev`` The number of function calls ``fvec`` The function evaluated at the output ``fjac`` A permutation of the R matrix of a QR factorization of the final approximate Jacobian matrix, stored column wise. Together with ipvt, the covariance of the estimate can be approximated. ``ipvt`` An integer array of length N which defines a permutation matrix, p, such that fjac*p = q*r, where r is upper triangular with diagonal elements of nonincreasing magnitude. Column j of p is column ipvt(j) of the identity matrix. ``qtf`` The vector (transpose(q) * fvec). mesg : str A string message giving information about the cause of failure. ier : int An integer flag. If it is equal to 1, 2, 3 or 4, the solution was found. Otherwise, the solution was not found. In either case, the optional output variable 'mesg' gives more information. Notes ----- "leastsq" is a wrapper around MINPACK's lmdif and lmder algorithms. cov_x is a Jacobian approximation to the Hessian of the least squares objective function. This approximation assumes that the objective function is based on the difference between some observed target data (ydata) and a (non-linear) function of the parameters `f(xdata, params)` :: func(params) = ydata - f(xdata, params) so that the objective function is :: min sum((ydata - f(xdata, params))**2, axis=0) params The solution, `x`, is always a 1D array, regardless of the shape of `x0`, or whether `x0` is a scalar. """ x0 = asarray(x0).flatten() n = len(x0) if not isinstance(args, tuple): args = (args,) shape, dtype = _check_func('leastsq', 'func', func, x0, args, n) m = shape[0] if n &gt; m: raise TypeError('Improper input: N=%s must not exceed M=%s' % (n, m)) if epsfcn is None: epsfcn = finfo(dtype).eps if Dfun is None: if maxfev == 0: maxfev = 200*(n + 1) with _MINPACK_LOCK: retval = _minpack._lmdif(func, x0, args, full_output, ftol, xtol, gtol, maxfev, epsfcn, factor, diag) else: if col_deriv: _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (n, m)) else: _check_func('leastsq', 'Dfun', Dfun, x0, args, n, (m, n)) if maxfev == 0: maxfev = 100 * (n + 1) with _MINPACK_LOCK: retval = _minpack._lmder(func, Dfun, x0, args, full_output, col_deriv, ftol, xtol, gtol, maxfev, factor, diag) errors = &#123;0: ["Improper input parameters.", TypeError], 1: ["Both actual and predicted relative reductions " "in the sum of squares\n are at most %f" % ftol, None], 2: ["The relative error between two consecutive " "iterates is at most %f" % xtol, None], 3: ["Both actual and predicted relative reductions in " "the sum of squares\n are at most %f and the " "relative error between two consecutive " "iterates is at \n most %f" % (ftol, xtol), None], 4: ["The cosine of the angle between func(x) and any " "column of the\n Jacobian is at most %f in " "absolute value" % gtol, None], 5: ["Number of calls to function has reached " "maxfev = %d." % maxfev, ValueError], 6: ["ftol=%f is too small, no further reduction " "in the sum of squares\n is possible.""" % ftol, ValueError], 7: ["xtol=%f is too small, no further improvement in " "the approximate\n solution is possible." % xtol, ValueError], 8: ["gtol=%f is too small, func(x) is orthogonal to the " "columns of\n the Jacobian to machine " "precision." % gtol, ValueError], 'unknown': ["Unknown error.", TypeError]&#125; info = retval[-1] # The FORTRAN return value if info not in [1, 2, 3, 4] and not full_output: if info in [5, 6, 7, 8]: warnings.warn(errors[info][0], RuntimeWarning) else: try: raise errors[info][1](errors[info][0]) except KeyError: raise errors['unknown'][1](errors['unknown'][0]) mesg = errors[info][0] if full_output: cov_x = None if info in [1, 2, 3, 4]: from numpy.dual import inv perm = take(eye(n), retval[1]['ipvt'] - 1, 0) r = triu(transpose(retval[1]['fjac'])[:n, :]) R = dot(r, perm) try: cov_x = inv(dot(transpose(R), R)) except (LinAlgError, ValueError): pass return (retval[0], cov_x) + retval[1:-1] + (mesg, info) else: return (retval[0], info) 以上为大家常用python数据分析库拟合曲线方程的方法。但是这里可以看到计算得到的方程并不是最优解而是牛顿迭代不断逼近的解。（SO：调库要谨慎！）直线拟合 首先scipy.optimize.leastsq可以直接调用就行。然后我看网上很多雷同的博客用的这个库直接调用的，自己测试后对于此库个人保留怀疑态度。所以下面是自己写的一个对于直线方程的拟合函数 1234567891011121314151617# 这里用的方程是y=k*x+b# 输入的是列表list，分别是x和y的值# 输出的是k和b的值def least_square(x, y): if len(x) != len(y): return False aver_x = sum(x) / len(x) aver_y = sum(y) / len(y) # 分母和分子初始化 denominator = 0 molecule = 0 for i in range(0, len(x)): denominator += (x[i] - aver_x) * (y[i] - aver_y) molecule += pow((x[i] - aver_x), 2) k = denominator / molecule b = aver_y - k * aver_x return k, b 推导过程 直线方程为$y = kx + b$ 先求k为$k = \frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^n_{i=1}(x_i-\bar{x})^2}$ 再求b为$b = \bar{y} - k\bar{x}$ 推导过程参考wiki 针对椭圆的最小二乘法拟合 平面上任意位置的一个椭圆，其中心坐标为（x0，y0），半长轴a，半短轴b，长轴偏角为θ，方程为$x^2 + Axy + By^2 + Cx + Dy + E = 0$ 转换为标准椭圆方程$\frac{(x-x_0)^2}{a^2} + \frac{(y-y_0)^2}{b^2} = 1$时，参数计算为： $x_0 = \frac{2BC-AD}{A^2-4B}$ $y_0 = \frac{2D-AD}{A^2-4B}$ $a = \sqrt{\frac{2(ACD-BC^2-D^2+4BE-A^2E)}{(A^2-4B)(B+\sqrt{A^2+(1-B)^2}+1)}}$ $b = \sqrt{\frac{2(ACD-BC^2-D^2+4BE-A^2E)}{(A^2-4B)(B-\sqrt{A^2+(1-B)^2}+1)}}$ $\theta = \arctan(\sqrt{\frac{a^2-b^2B}{a^2B-b^2}})$ 在原始测得的N（N≥5）组数据（xi，yi），（i=1,2,3,…,N）中，根据椭圆方程通式和最小二乘法原理，求目标函数$F(A, B, C, D, E) = \sum^N_{i=1}(x_i^2 + Ax_iy_i + By_i^2 + Cx_i + Dy_i + E)^2$的最小值来确定参数A、B、C、D和E的值 令F（A，B，C，D，E）对各个参数的偏导数均为零，得到以下方程组： $$\begin{bmatrix}\sum^N_{i=1}x_i^2y_i^2 &amp; \sum^N_{i=1}x_i^2y_i^3 &amp; \sum^N_{i=1}x_i^2y_i &amp; \sum^N_{i=1}x_iy_i^2 &amp; \sum^N_{i=1}x_iy_i \\sum^N_{i=1}x_iy_i^3 &amp; \sum^N_{i=1}y_i^4 &amp; \sum^N_{i=1}x_iy_i^2 &amp; \sum^N_{i=1}y_i^3 &amp; \sum^N_{i=1}y_i^2 \\sum^N_{i=1}x_i^2y_i &amp; \sum^N_{i=1}x_iy_i^2 &amp; \sum^N_{i=1}x_i^2 &amp; \sum^N_{i=1}x_iy_i &amp; \sum^N_{i=1}x_i \\sum^N_{i=1}x_iy_i^2 &amp; \sum^N_{i=1}y_i^2 &amp; \sum^N_{i=1}x_iy_i &amp; \sum^N_{i=1}y_i^2 &amp; \sum^N_{i=1}y_i \\sum^N_{i=1}x_iy_i &amp; \sum^N_{i=1}y_i^2 &amp; \sum^N_{i=1}x_i &amp; \sum^N_{i=1}y_i &amp; N \\end{bmatrix}\begin{bmatrix}A \B \C \D \E \\end{bmatrix} =\begin{bmatrix}\sum^N_{i=1}x_i^3y_i \\sum^N_{i=1}x_i^2y_i^2 \\sum^N_{i=1}x_i^3 \\sum^N_{i=1}x_i^2y_i \\sum^N_{i=1}x_i^2 \\end{bmatrix}$$ 求解此线性方程组可解出A、B、C、D和E，代入第二个方程即可解得拟合的椭圆的参数 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding: utf-8 -*-import numpy as npdef solve_tuoyuan(x,y): # a*x**2 + b*x*y + c*y**2 + d*x + e*y + f x0,y0 = x.mean(),y.mean() D1=np.array([(x-x0)**2,(x-x0)*(y-y0),(y-y0)**2]).T D2=np.array([x-x0,y-y0,np.ones(y.shape)]).T S1=np.dot(D1.T,D1) S2=np.dot(D1.T,D2) S3=np.dot(D2.T,D2) T=-1*np.dot(np.linalg.inv(S3),S2.T) M=S1+np.dot(S2,T) M=np.array([M[2]/2,-M[1],M[0]/2]) lam,eigen=np.linalg.eig(M) cond=4*eigen[0]*eigen[2]-eigen[1]**2 A1=eigen[:,cond&gt;0] A=np.vstack([A1,np.dot(T,A1)]).flatten() A3=A[3]-2*A[0]*x0-A[1]*y0 A4=A[4]-2*A[2]*y0-A[1]*x0 A5=A[5]+A[0]*x0**2+A[2]*y0**2+A[1]*x0*y0-A[3]*x0-A[4]*y0 A[3]=A3; A[4]=A4; A[5]=A5 return Adef normal_style(paras): paras=paras/paras[5] A,B,C,D,E=paras[:5] # 椭圆中心 x0=(B*E-2*C*D)/(4*A*C-B**2) y0=(B*D-2*A*E)/(4*A*C-B**2) # 长短轴 a= 2*np.sqrt((2*A*(x0**2)+2*C*(y0**2)+2*B*x0*y0-2)/(A+C+np.sqrt(((A-C)**2+B**2)))) b= 2*np.sqrt((2*A*(x0**2)+2*C*(y0**2)+2*B*x0*y0-2)/(A+C-np.sqrt(((A-C)**2+B**2)))) # 长轴倾角 q=0.5 * np.arctan(B/(A-C)) # normal_style return x0,y0,a,b,qdef tuoyuan(y,x,p): # 用来计算 return p[0]*x**2+p[1]*x*y+p[2]*y**2+p[3]*x + p[4]*y + p[5] 参考源 最小二乘法拟合椭圆——MATLAB和Qt-C++实现 github omfPython/fit_ellipse.py 最小二乘法拟合圆公式推导及vc实现[r]]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汽车状态分类器]]></title>
    <url>%2F2018%2F10%2F27%2F%E6%B1%BD%E8%BD%A6%E7%8A%B6%E6%80%81%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这里使用的是UCI的汽车评估数据集 数据集的标签 buying maint doors persons lug_boot safety class vhigh vhigh 2 2 small low unacc high high 3 4 med med acc med med 4 more big high good low low 5more vgood 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# -*- coding:utf-8 -*-import numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltimport pandas as pdfrom urllib.request import urlretrieve# 获取数据class Preprocessing: # 数据预处理 def load_data(download=True): # 下载数据集 if download: data_path, _ = urlretrieve("http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data", "car.csv") print("Downloaded to car.csv") # 使用pandas查看数据结构 col_names = ["buying", "maint", "doors", "persons", "lug_boot", "safety", "class"] data = pd.read_csv("car.csv", names=col_names) return data # 数据类型转化 def covert_2_onehot(data): data = pd.get_dummies(data, prefix=data.columns) # 将类型转化为0、1数组 return datadata = Preprocessing.load_data()new_data = Preprocessing.covert_2_onehot(data)# 准备训练数据# 训练集测试集7：3new_data = new_data.values.astype(np.float32) # 转换数据类型np.random.shuffle(new_data) # 打乱数据集sep = int(0.7*len(new_data))train_data = new_data[:sep]test_data = new_data[sep:]# 建立网络模型tf_input = tf.placeholder(tf.float32, [None, 25], "input") # 函数参数tfx = tf_input[:, :21]tfy = tf_input[:, 21:]l1 = tf.layers.dense(tfx, 128, tf.nn.relu, name="l1") # 第一层、使用relu激活函数l2 = tf.layers.dense(l1, 128, tf.nn.relu, name="l2") # 第二层、使用relu激活函数out = tf.layers.dense(l2, 4, name="l3") # 第三层、没有使用激活函数prediction = tf.nn.softmax(out, name="prediction") # softmax函数loss = tf.losses.softmax_cross_entropy(onehot_labels=tfy, logits=out) # 损失函数accuracy = tf.metrics.accuracy(labels=tf.argmax(tfy, axis=1), predictions=tf.argmax(out, axis=1),)[1] # 计算预准率opt = tf.train.GradientDescentOptimizer(learning_rate=0.1) # 学习率为固定值train_op = opt.minimize(loss) # 最小化的目标变量sess = tf.Session() # 会话sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())) # 初始化参数# 训练网络accuracies, steps = [], []for t in range(10000): # 训练 batch_index = np.random.randint(len(train_data), size=32) sess.run(train_op, &#123;tf_input: train_data[batch_index]&#125;) if t % 50 == 0: # 每训练50次测试一次 # 测试 acc_, pred_, loss_ = sess.run([accuracy, prediction, loss], &#123;tf_input: test_data&#125;) accuracies.append(acc_) steps.append(t) print("Step: ", t, "| Accurate: ", acc_, "| Loss: ", loss_) # 可视化 plt.ion() fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4)) ax1.cla() for c in range(4): bp = ax1.bar(c+0.1, height=sum((np.argmax(pred_, axis=1) == c)), width=0.2, color='red') bt = ax1.bar(c-0.1, height=sum((np.argmax(test_data[:, 21:], axis=1) == c)), width=0.2, color='blue') ax1.set_xticks(range(4), ["accepted", "good", "unaccepted", "very good"]) ax1.legend(handles=[bp, bt], labels=["prediction", "target"]) ax1.set_ylim((0, 400)) ax2.cla() ax2.plot(steps, accuracies, label="accuracy") ax2.set_ylim(ymax=1) ax2.set_ylabel("accuracy") plt.pause(0.01) plt.ioff() plt.show() plt.close() 结果：]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>UCI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程、线程和协程——python]]></title>
    <url>%2F2018%2F10%2F14%2F%E8%BF%9B%E7%A8%8B%E3%80%81%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%8D%8F%E7%A8%8B%E2%80%94%E2%80%94python%2F</url>
    <content type="text"><![CDATA[&nbsp;协程 协程，又称微线程，纤程 协程是一种用户态的轻量级线程（协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此，协程能保留上一次调用时的状态，即每次过程重入时进入上一次离开时所处逻辑流的位置。） 优点 高并发 缺点 无法利用多核资源(协程需要和进程配合才能运行在多CPU上) 进行阻塞操作时会阻塞掉整个程序 import Gevent python关于协程的第三方库（貌似对win不支持） 迭代器1234567891011list_ = [1, 2, 3]it = iter(list_)print(it)print(next(it))print(next(it))# result# &lt;list_iterator object at 0x1073b8320&gt;# 1# 2 12345678list_ = [1, 2, 3]it = iter(list_)for i in it: print(i, end='\t')# result# 1 2 3 生成器12345678910111213141516import syslist_ = [1, 2, 3]it = iter(list_)while True: try: print(next(it)) except StopIteration: sys.exit()# result# 1# 2# 3 1234567891011121314151617181920212223242526272829# coding=utf-8import sysdef triangles(): result = [1] while True: yield result for i in range(1, len(result)): result[i] = result[i] + prepare[i-1] result.append(1) prepare = result[:]if __name__ == '__main__': f = triangles() for i in range(6): print(f.__next__())# result# [1]# [1, 1]# [1, 2, 1]# [1, 3, 3, 1]# [1, 4, 6, 4, 1]# [1, 5, 10, 10, 5, 1] 多线程使用1234567891011121314151617import threading # 引入线程import timedef func_add(): # 目标函数 print('this is a thread')num = threading.active_count() # 获取已经激活的线程数print(num)inf = threading.enumerate() # 获取所有线程信息print(inf)act = threading.current_thread() # 查看正在运行的线程print(act)t_1 = threading.Thread(target=func_add, ) # 添加线程,target表示线程要完成的任务print(t_1)t_1.start() # 让线程开始工作 join功能 阻塞主线程,即在子线程未返回的时候,主线程等待其返回然后再继续执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import threadingimport timeprint('no join')# 不加join的结果def func_job_1(): print(1) time.sleep(1) print(2)t_2 = threading.Thread(target=func_job_1, )t_2.start()print(3)# result# 1# 3# 2time.sleep(2)print('join')# 加join的结果def func_job_3(): print('3 start') time.sleep(1) print('3 end') def func_job_4(): print('4 start') print('4 end') if __name__ == '__main__': # no join # t_3 = threading.Thread(target=func_job_3, ) # t_4 = threading.Thread(target=func_job_4, ) # t_3.start() # t_4.start() # print('all end') # join t_3 = threading.Thread(target=func_job_3, ) t_4 = threading.Thread(target=func_job_4, ) # 1221布局 t_3.start() t_4.start() t_4.join() t_3.join() print('all end') 存储线程结果12345678910111213141516171819202122232425import threadingimport timefrom queue import Queue # 队列def func_add(list_, queue_): # 目标函数 sum_ = 0 for i, val in enumerate(list_): sum_ += val queue_.put(sum_) # 多线程调用的函数不能用return返回值queue_ = Queue() # queue_中存放返回值，代替return的返回值threads_ = []data = [[1, 1], [2, 2], [3, 3]]for i in range(3): # 定义三个线程 t_ = threading.Thread(target=func_add, args=(data[i], queue_)) t_.start() threads_.append(t_) # 把每个线程添加到线程列表中for t_ in threads_: # 把所有线程join到主线程 t_.join()result = []for i in range(3): result.append(queue_.get()) # 把队列中的值依次拿出print(result) # 打印结果 GIL效率分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import threadingimport timedef func_add(list_): # 目标函数 sum_ = 0 for i, val in enumerate(list_): sum_ += valdef mutithreading_add(list_, threads, add_num, flag_join): # 多线程函数 for i in range(add_num): for i in range(threads): t_ = threading.Thread(target=func_add, args=(list_, )) t_.start() if flag_join is True: t_.join()def func_print(str_, n): # 目标函数 for i in range(n): print(str_, end='') print() def mutithreading_print(str_, n, threads, flag_join): for i in range(threads): num = int(n / threads) # 每个线程打印的次数 t_ = threading.Thread(target=func_print, args=(str_, num, )) t_.start() if flag_join is True: t_.join()if __name__ == '__main__': # 计算密集型 print('计算密集型') # 单线程 time_1 = time.time() mutithreading_add([1, 2, 3, 4, 5], 1, 500, True) # 1个线程，计算500次，加join time_2 = time.time() mutithreading_add([1, 2, 3, 4, 5], 1, 500, False) # 1个线程，计算500次，不加join time_3 = time.time() print('单线程 加join', '\t', time_2-time_1) print('单线程 不加join', '\t', time_3-time_2) # 多线程 time_1 = time.time() mutithreading_add([1, 2, 3, 4, 5], 10, 500, True) # 10个线程，计算500次，加join time_2 = time.time() mutithreading_add([1, 2, 3, 4, 5], 10, 500, False) # 1个线程，计算500次，不加join time_3 = time.time() print('多线程 加join', '\t', time_2-time_1) print('多线程 不加join', '\t', time_3-time_2) # I/O密集型 print('I/O密集型') # 单线程 time_1 = time.time() mutithreading_print('hello world', 5000, 1, True) # 1个线程，打印5000次，加join time_2 = time.time() mutithreading_print('hello world', 5000, 1, False) # 1个线程，打印5000次，不加join time_3 = time.time() print('单线程 加join', '\t', time_2-time_1) print('单线程 不加join', '\t', time_3-time_2) # 多线程 time_1 = time.time() mutithreading_print('hello world', 5000, 10, True) # 10个线程，打印5000次，加join time_2 = time.time() mutithreading_print('hello world', 5000, 10, False) # 1个线程，打印5000次，不加join time_3 = time.time() print('多线程 加join', '\t', time_2-time_1) print('多线程 不加join', '\t', time_3-time_2)# result# 计算密集型# 单线程 加join 0.0544428825378418# 单线程 不加join 0.02800607681274414# 多线程 加join 0.37989211082458496# 多线程 不加join 0.462191104888916# I/O密集型# 单线程 加join 0.004912853240966797# 单线程 不加join 0.005605220794677734# 多线程 加join 0.0066182613372802734# 多线程 不加join 0.005144834518432617 python的单线程基本都是比多线程快的，平常建议一直使用单线程 线程锁 lock = threading.Lock() 定义一个锁对象 lock.acquire() 将共享内存上锁 lock.release() 将锁打开 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108# coding=utf-8# 线程锁Lockimport threadingimport time# 不使用lockdef func_print_1(): # 目标函数 global number for i in range(5): time.sleep(0.5) number += 1 print('func 1', '\t', number)def func_print_2(): # 目标函数 global number for i in range(5): time.sleep(0.5) number += 10 print('func 2', '\t', number)def no_use_lock(): t_1 = threading.Thread(target=func_print_1, ) t_2 = threading.Thread(target=func_print_2, ) t_1.start() t_2.start() t_2.join() t_1.join()# 使用lockdef func_print_3(lock): # 目标函数 global number for i in range(5): lock.acquire() # 将共享内存上锁 time.sleep(0.5) number += 1 print('func 3', '\t', number) lock.release() # 将锁打开def func_print_4(lock): # 目标函数 global number for i in range(5): lock.acquire() time.sleep(0.5) number += 10 print('func 4', '\t', number) lock.release()def use_lock(): lock = threading.Lock() # 定义一个线程锁 t_1 = threading.Thread(target=func_print_3, args=(lock, )) t_2 = threading.Thread(target=func_print_4, args=(lock, )) t_1.start() t_2.start() t_2.join() t_1.join()if __name__ == '__main__': # 不使用线程锁 number = 0 print('no use lock') time_1 = time.time() no_use_lock() time_2 = time.time() print('time:', '\t', time_2-time_1) # 使用线程锁 number = 0 print('use lock') time_1 = time.time() use_lock() time_2 = time.time() print('time:', '\t', time_2-time_1)# result# no use lock# func 1 1# func 2 11# func 1 12# func 2 22# func 1 23# func 2 33# func 1 34# func 2 44# func 2 54# func 1 55# time: 2.514285087585449# use lock# func 3 1# func 3 2# func 4 12# func 4 22# func 4 32# func 4 42# func 4 52# func 3 53# func 3 54# func 3 55# time: 5.021925926208496 使用线程锁必然效率会比较低 多进程使用1234567891011121314# coding=utf-8import multiprocessing as mp # 引入进程def func(str_1, str_2): # 目标函数 print(str_1, str_2)if __name__ == '__main__': p_ = mp.Process(target=func, args=('hello', 'world')) # 定义一个进程 p_.start() # 启动进程 p_.join() 储存进程结果12345678910111213141516171819202122232425262728293031323334# coding=utf-8import multiprocessing as mp # 引入进程from queue import Queuedef func(list_): # 目标函数 res = 0 for i, val in enumerate(list_): res += val print(res) queue_.put(res)if __name__ == '__main__': queue_ = mp.Queue() data_ = [1, 2, 3, 4, 5] result = [] p_ = mp.Process(target=func, args=(data_, )) # 定义一个进程 p_.start() # 启动进程 p_.join() print('process', '\t', p_) print('result', '\t', queue_.get())# result# 1# 3# 6# 10# 15# process &lt;Process(Process-1, stopped)&gt;# result 15 进程池1234567891011121314151617181920# coding=utf-8import multiprocessing as mpdef job(x): return x*x, x/2if __name__ == '__main__': pool = mp.Pool(processes=10) # 定义一个进程池；processes是来定义工作进程的数量，不定义则默认为电脑的核数 res = pool.map(job, range(10)) # 使用map获取结果；在map中放入函数和需要迭代计算的值，map会自动分配给cpu核 print(res) res = pool.apply_async(job, (2,)) # 只能传递一个值，可以接收多个值 print(res.get()) # 使用get方法获取返回值 multi_res = [pool.apply_async(job, (i, )) for i in range(10)] # apply_async只能输入一个或一组参数，所以将apply_async放入迭代器中 print([res.get() for res in multi_res]) # 逐个提取出来 共享内存123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# coding=utf-8import multiprocessing as mpimport timevalue1 = mp.Value('d', 3.14) # 通过Value将一个值存储到内存中；Value只能是一维的数据，而且必须先定义数据类型array = mp.Array('i', [1, 2, 3, 4]) # 通过Array将多个数据存放到内存中，但要求数据类型一致；Array只能是一维的数据，而且必须先定义数据类型# 参数表类型# | Type code | C Type | Python Type | Minimum size in bytes |# | --------- | ------------------ | ----------------- | --------------------- |# | `'b'` | signed char | int | 1 |# | `'B'` | unsigned char | int | 1 |# | `'u'` | Py_UNICODE | Unicode character | 2 |# | `'h'` | signed short | int | 2 |# | `'H'` | unsigned short | int | 2 |# | `'i'` | signed int | int | 2 |# | `'I'` | unsigned int | int | 2 |# | `'l'` | signed long | int | 4 |# | `'L'` | unsigned long | int | 4 |# | `'q'` | signed long long | int | 8 |# | `'Q'` | unsigned long long | int | 8 |# | `'f'` | float | float | 4 |# | `'d'` | double | float | 8 |# 样例def add_1(n, sleep_time): for i in range(n): number.value += 1 print('add_1', '\t', i, '\t', number.value) time.sleep(sleep_time)def add_2(n, sleep_time): for i in range(n): number.value += 2 print('add_2', '\t', i, '\t', number.value) time.sleep(sleep_time)if __name__ == '__main__': number = mp.Value('b', 0) # 内存共享的变量 p_1 = mp.Process(target=add_1, args=(5, 0.01)) p_2 = mp.Process(target=add_2, args=(5, 0.01)) p_1.start() p_2.start() p_2.join() p_1.join() print(number.value)# result# 正确情况# # add_1 0 1# add_2 0 3# add_1 1 4# add_2 1 6# add_1 2 7# add_2 2 9# add_1 3 10# add_2 3 12# add_1 4 13# add_2 4 15# 15# # 坏情况# # add_1 0 1# add_2 0 3# add_1 1 4# add_2 1 6# add_1 2 8# add_2 2 8# add_2 3 9# add_1 3 9# add_1 4 11# add_2 4 11# 11 共享内存时，内存中的数据同时被多个进程操作的话就会出现错误 进程锁12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# coding=utf-8import multiprocessing as mpimport timedef no_lock_add(n, sleep_time, add_n): for i in range(n): number.value += add_n print('add_1', '\t', i, '\t', number.value) time.sleep(sleep_time)def use_lock_add(n, sleep_time, add_n): for i in range(n): lock.acquire() number.value += add_n lock.release() print('add_1', '\t', i, '\t', number.value) time.sleep(sleep_time) if __name__ == '__main__': # 不加进程锁 number = mp.Value('b', 0) # 内存共享的变量 p_1 = mp.Process(target=no_lock_add, args=(5, 0.01, 1)) p_2 = mp.Process(target=no_lock_add, args=(5, 0.01, 2)) p_1.start() p_2.start() p_2.join() p_1.join() print(number.value) # 输出结果 # 加进程锁 number = mp.Value('b', 0) # 内存共享的变量 lock = mp.Lock() # 定义一个进程锁 p_1 = mp.Process(target=use_lock_add, args=(5, 0.01, 1)) p_2 = mp.Process(target=use_lock_add, args=(5, 0.01, 2)) p_1.start() p_2.start() p_2.join() p_1.join() print(number.value) # 输出结果# result## add_1 0 1# add_1 0 3# add_1 1 4# add_1 1 4# add_1 2 5# add_1 2 5# add_1 3 6# add_1 3 6# add_1 4 8# add_1 4 8# 8# add_1 0 1# add_1 0 3# add_1 1 4# add_1 1 6# add_1 2 8# add_1 2 9# add_1 3 11# add_1 3 12# add_1 4 13# add_1 4 15# 15 多线程、多进程、单线程性能对比123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155# coding=utf-8import multiprocessing as mp # 引入进程import threading as td # 引入线程import timeimport osdef func_add(n, sleep_time): res = 0 for i in range(n): res += i time.sleep(sleep_time)def process_(n, sleep_time): # 多进程 p_1 = mp.Process(target=func_add, args=(n, sleep_time)) p_2 = mp.Process(target=func_add, args=(n, sleep_time)) p_3 = mp.Process(target=func_add, args=(n, sleep_time)) p_4 = mp.Process(target=func_add, args=(n, sleep_time)) p_1.start() p_2.start() p_3.start() p_4.start() p_4.join() p_3.join() p_2.join() p_1.join()def threading_(n, sleep_time): # 多线程 t_1 = td.Thread(target=func_add, args=(n, sleep_time)) t_2 = td.Thread(target=func_add, args=(n, sleep_time)) t_3 = td.Thread(target=func_add, args=(n, sleep_time)) t_4 = td.Thread(target=func_add, args=(n, sleep_time)) t_1.start() t_2.start() t_3.start() t_4.start() t_4.join() t_3.join() t_2.join() t_1.join()def thread_process(n, sleep_time): # 多进程多线程 def two_thread(n, sleep_time): # 创建两个线程并行计算 t_1 = td.Thread(target=func_add, args=(n, sleep_time)) t_2 = td.Thread(target=func_add, args=(n, sleep_time)) t_1.start() t_2.start() t_2.join() t_1.join() p_1 = mp.Process(target=two_thread, args=(n, sleep_time)) p_2 = mp.Process(target=two_thread, args=(n, sleep_time)) p_1.start() p_2.start() p_2.join() p_1.join()def usual(n, sleep_time): # 正常的单线程 for _ in range(4): res = 0 for i in range(n): res += i time.sleep(sleep_time)def func_one(): # 并行计算比单进程快的情况 # 重复计算n以内的整数的和4次 n = 10 # n以内数的积 thread_n = 2 # 线程数量 process_n = 2 # 进程数量 sleep_time = 0.1 # 每计算一步的睡眠时间，用来调节单步计算量大小 print('thread', '\t', 'process', '\t', 'time') # 多进程并行计算 time_1 = time.time() process_(n, sleep_time) time_2 = time.time() print(' 1 \t 4 \t', time_2 - time_1) # 多线程并行计算 time_1 = time.time() threading_(n, sleep_time) time_2 = time.time() print(' 4 \t 1 \t', time_2 - time_1) # 多线程多进程并行计算 time_1 = time.time() thread_process(n, sleep_time) time_2 = time.time() print(' 4 \t 2 \t', time_2 - time_1) # 正常 单线程流程计算 time_1 = time.time() usual(n, sleep_time) time_2 = time.time() print(' 1 \t 1 \t', time_2 - time_1)def func_two(): # 并行计算比单进程慢的情况 # 重复计算n以内的整数的和4次 n = 50000 # n以内数的积 thread_n = 2 # 线程数量 process_n = 2 # 进程数量 sleep_time = 0 # 每计算一步的睡眠时间，用来调节单步计算量大小 print('thread', '\t', 'process', '\t', 'time') # 多进程并行计算 time_1 = time.time() process_(n, sleep_time) time_2 = time.time() print(' 1 \t 4 \t', time_2 - time_1) # 多线程并行计算 time_1 = time.time() threading_(n, sleep_time) time_2 = time.time() print(' 4 \t 1 \t', time_2 - time_1) # 多线程多进程并行计算 time_1 = time.time() thread_process(n, sleep_time) time_2 = time.time() print(' 4 \t 2 \t', time_2 - time_1) # 正常 单线程流程计算 time_1 = time.time() usual(n, sleep_time) time_2 = time.time() print(' 1 \t 1 \t', time_2 - time_1)if __name__ == '__main__': func_one() print() func_two()# result# thread process time# 1 4 1.0103631019592285# 4 1 1.0256071090698242# 4 2 1.0340228080749512# 1 1 4.0948076248168945# # thread process time# 1 4 0.05113697052001953# 4 1 1.1482949256896973# 4 2 0.2586557865142822# 1 1 0.1494588851928711 结论 多线程和多进程在并行的时候未必就比单线程快，首先自己要衡量线程、进程之间切换的时间和计算的时间大小关系。 如果切换时间远大于计算时间，那么单线程是最好的选择 如果切换时间远小于计算时间，那么多线程、多进程是最好的选择（前提是这里的处理步骤可以使并行的） 如果是非并行运算，那么直接使用单线程就好 要创建多个进程或者多个线程的时候，不要使用循环，那样的话就是假并行计算，还会加大处理的时间。 没有进程间、线程间通信的话，多线程和多进程进行并行计算的时间是相近的 提高性能时，建议直接使用多进程+协程，抛弃多线程]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex-vscode settings]]></title>
    <url>%2F2018%2F10%2F02%2Flatex-vscode%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[using latex in mac and winwindows10 安装vscode 安装ctex 添加你安装的ctex的路径到windows的环境变量中 user settings如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&quot;latex-workshop.latex.tools&quot;: [ &#123; &quot;name&quot;: &quot;latexmk&quot;, &quot;command&quot;: &quot;latexmk&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;-pdf&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;xelatex&quot;, &quot;command&quot;: &quot;xelatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex&quot;, &quot;command&quot;: &quot;pdflatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;bibtex&quot;, &quot;command&quot;: &quot;bibtex&quot;, &quot;args&quot;: [ &quot;%DOCFILE%&quot; ] &#125;],&quot;latex-workshop.latex.recipes&quot;: [ &#123; &quot;name&quot;: &quot;xelatex&quot;, &quot;tools&quot;: [ &quot;xelatex&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot; ] &#125;, &#123; &quot;name&quot;: &quot;latexmk&quot;, &quot;tools&quot;: [ &quot;latexmk&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot;, &quot;bibtex&quot;, &quot;pdflatex&quot;, &quot;pdflatex&quot; ] &#125;],&quot;files.associations&quot;: &#123; &quot;*.tex&quot;: &quot;latex&quot;&#125;,&quot;latex-workshop.latex.clean.onFailBuild.enabled&quot;: true, 支持中文 12345678910111213%-- coding: UTF-8 --\documentclass[12pt]&#123;article&#125;\usepackage[UTF8]&#123;ctex&#125;\title&#123;模板&#125;\author&#123;Kevin&#125;\date&#123;2008/10/12&#125;\begin&#123;document&#125;\maketitle\tableofcontentshello world\end&#123;document&#125; Mac os 安装vscode 安装mactex user settings 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&quot;latex-workshop.latex.tools&quot;: [ &#123; &quot;name&quot;: &quot;latexmk&quot;, &quot;command&quot;: &quot;latexmk&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;-pdf&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;xelatex&quot;, &quot;command&quot;: &quot;xelatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex&quot;, &quot;command&quot;: &quot;pdflatex&quot;, &quot;args&quot;: [ &quot;-synctex=1&quot;, &quot;-interaction=nonstopmode&quot;, &quot;-file-line-error&quot;, &quot;%DOC%&quot; ] &#125;, &#123; &quot;name&quot;: &quot;bibtex&quot;, &quot;command&quot;: &quot;bibtex&quot;, &quot;args&quot;: [ &quot;%DOCFILE%&quot; ] &#125;],&quot;latex-workshop.latex.recipes&quot;: [ &#123; &quot;name&quot;: &quot;xelatex&quot;, &quot;tools&quot;: [ &quot;xelatex&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot; ] &#125;, &#123; &quot;name&quot;: &quot;latexmk&quot;, &quot;tools&quot;: [ &quot;latexmk&quot; ] &#125;, &#123; &quot;name&quot;: &quot;pdflatex -&gt; bibtex -&gt; pdflatex*2&quot;, &quot;tools&quot;: [ &quot;pdflatex&quot;, &quot;bibtex&quot;, &quot;pdflatex&quot;, &quot;pdflatex&quot; ] &#125;],&quot;files.associations&quot;: &#123; &quot;*.tex&quot;: &quot;latex&quot;&#125;,&quot;latex-workshop.latex.clean.onFailBuild.enabled&quot;: true, 支持中文 12345678910111213%!TEX program = pdflatex\documentclass&#123;article&#125;\usepackage&#123;CJKutf8&#125;\usepackage&#123;color&#125;\begin&#123;document&#125;\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;bsmi&#125;hello world\end&#123;CJK&#125;\end&#123;document&#125;]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Writing Solid Python Code]]></title>
    <url>%2F2018%2F09%2F19%2Fwriting-Solid-Python-Code%2F</url>
    <content type="text"><![CDATA[编写高质量代码（Python）改善Python程序的91个建议 编辑时间：2018.9.14 本文为原书的摘抄，原书中有些问题在后来Python的新版本中已经得到改善，所以本文中出现与原书不同之处为我自己测试样例后改动的，重在学习思想即可。本文具有一定的时限性，我是针对于Python3.6.6做的样例测试。原书中某些内容本文中没有出现的原因是太简单、本人觉得不妥或者上一篇Effective Python中有，另外原书中也有一些错误，但不妨碍它成为一本好书。 引论 ~ 避免只用大小写来区分不同的对象 避免使用容易引起混淆的名称 不要害怕过长的变量名 ~ 三元操作符?: C?X:Y等价于X if C else Y 示例：1234567891011121314if x == 1: print(&apos;1&apos;)elif x == 2: print(&apos;2&apos;)else: print(&apos;None&apos;)# 改善后的代码def search(x): return &#123; 1: &apos;1,&apos;, 2: &apos;2&apos;, &#125;.get(x, &apos;None&apos;) ~ 给外部可访问的函数和方法添加文档注释（描述方法的功能，输入输出） copyright声明、模块描述等（作者、变更描述） 注释应该是用来解释代码的功能、原因及想法的，而不是对代码本身的解释 ~ 在一组代码表达完一个完整的思路后，应该用空白行进行间隔 避免过长的代码行，每行最好不超过80个字符 空格 二元操作符的左右应该有空格，比较（==、&lt;、&gt;、!=、&lt;&gt;、&lt;=、&gt;=、in、not in、is、is not）、布尔运算（and、or、not） 逗号和分号前不要使用空格 ~ 将常量集中到一个文件（整个项目的全局变量）、加载文件引用全局变量 编程惯用法 ~ 利用assert语句发现问题 示例：123456789101112a = 1b = 2assert a == b, &apos;this is False&apos;# result# # Traceback (most recent call last):# File &quot;test.py&quot;, line 4, in &lt;module&gt;# assert a == b, &apos;this is False&apos;# AssertionError: this is False ~ 数据值交换的时候不推荐使用中间变量 示例：12# 交换a、b的值a, b = b, a ~ Lazy evaluation，惰性计算或者延迟计算，指的是仅仅在真正需要执行的时候才计算表达式的值 避免不必要的计算，带来计算性能上的提升 if x and y，如果x为False应当直接返回而不是继续计算y if x or y，当x为True使直接返回而不是计算y的值 节省空间，使得无限循环的数据结构称为可能 生成器表达式yield ~ 使用类属性 示例：123456789class age: mary = 0 allen = 1 james = 2# 简化后class age: mary, allen, james = range(3) ~ 不建议使用type进行类型检查 type() 判断一个变量是不是list类型使用代码：if type(a) is types.ListType: 例： types.BooleanType types.IntType types.StringType types.DictType 基于用户内建类型扩展的用户自定义类型，type函数并不能准确返回结果 建议使用isinstance()函数来检测 isinstance(object, classinfo) 示例:1234&gt;&gt;&gt; isinstance(2, float)False&gt;&gt;&gt; isinstance((2, 3), (str, list, tuple)) # 支持多种类型列表True ~ 尽量转换为浮点数后再去做出发 ~ 警惕eval()的安全漏洞 eval语法 eval(expression[, globals[, locals]]) expression – 表达式。 globals – 变量作用域，全局命名空间，如果被提供，则必须是一个字典对象。 locals – 变量作用域，局部命名空间，如果被提供，可以是任何映射对象。 示例：12345678910&gt;&gt;&gt;x = 7&gt;&gt;&gt; eval( &apos;3 * x&apos; )21&gt;&gt;&gt; eval(&apos;pow(2,2)&apos;)4&gt;&gt;&gt; eval(&apos;2 + 2&apos;)4&gt;&gt;&gt; n=81&gt;&gt;&gt; eval(&quot;n + 4&quot;)85 ~ 使用enumerate()函数序列迭代的索引和值 方法一 123456789101112list_ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]index = 0for i in list_: print(&apos;index:&apos;, index, &apos;element:&apos;, i) index += 1# result# # index: 0 element: a# index: 1 element: b# index: 2 element: c 方法二 12345678910list_ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]for i in range(len(list_)): print(&apos;index:&apos;, i, &apos;element:&apos;, list_[i])# result# # index: 0 element: a# index: 1 element: b# index: 2 element: c 方法三 123456789101112list_ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]index = 0while index &lt; len(list_): print(&apos;index:&apos;, index, &apos;element:&apos;, list_[index]) index += 1# result## index: 0 element: a# index: 1 element: b# index: 2 element: c 方法四 12345678910list_ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]for i, e in zip(range(len(list_)), list_): print(&apos;index:&apos;, i, &apos;element:&apos;, e)# result## index: 0 element: a# index: 1 element: b# index: 2 element: c 方法五 12345678910list_ = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]for i, e in enumerate(list_): print(&apos;index:&apos;, i, &apos;element:&apos;, e)# result## index: 0 element: a# index: 1 element: b# index: 2 element: c ~ 分清==与is的适用场景 is表示的是对象标示符，不仅地址要相同而且值要相同 ==表示的是意思相等，只需要值相同就好而不需要地址相同 示例：12345678910111213141516171819202122232425a = &apos;hello&apos;b = &apos;hello&apos;print(a is b)print(a == b)print(&apos;id:&apos;, id(a), &apos;value:&apos;, a)print(&apos;id:&apos;, id(b), &apos;value:&apos;, b)c = &apos;hello world&apos;d = &apos;hello &apos; + &apos;world&apos;print(c is d)print(c == d)print(&apos;id:&apos;, id(c), &apos;value:&apos;, c)print(&apos;id:&apos;, id(d), &apos;value:&apos;, d)# result## True# True# id: 2776211607480 value: hello# id: 2776211607480 value: hello# False# True# id: 2776212605936 value: hello world# id: 2776212684272 value: hello world ~ 考虑兼容性，注意编码格式 unicode utf-8 使用utf-8格式的话，要在程序文件头部加上: -*- coding:utf-8 -8- ~ 构建合理的包层次来管理module 使用多级文件来封装模块，分级使用和调用模块以及所在层内的全局变量 上层模块对自身的下一层模块跨文件互调 示例：12345678Module/ __init__.py module_1.py module_2.py SubPackage/ __init__.py module_1.py module_2.py 基础语法 ~ 有节制使用from···import语句 原因 因为调用其他模块的时候必然会有文件读写操作，这个效率是比较低的 尽量优先使用import ···，访问对象时使用···.··· 有节制使用from ··· import ··· 避免使用from ··· import *，这回污染命名空间，并且无法清晰地表示导入了哪个对象 ~ i++不等于++i 123456789101112print(++1)print(--1)i = 1i += 1print(i)# result# 1# 1# 2 ~ 使用with自动关闭资源 1234567891011f = open(path, &apos;w&apos;)f.write(opterate)# 往往会只集中于操作上，忘记关闭文件# with 表达式 [as 目标]:# 代码块with open(path, &apos;w&apos;) as f: f.write(opterate)# 代码块的操作执行完毕后，会自动关闭文件 ~ 异常处理 123456try: operateexcept Exception as err: return err# 这里执行try，捕捉的错误是python中定义的所有错误中的任意一种（你也可以直接定义是某类错误，这里是当你不确定是那种错误的时候） ~ 避免finally中可能发生的异常 无论try语句中是否有异常抛出，finally语句总会被执行 ~ 深入理解Nnoe，正确判断对象为空 123456789101112131415161718192021222324252627282930print(id(None))print(None == 0) # None不为0print(None == &apos;&apos;) # None不等于空字符串a = Noneb = Noneprint(a == b) # 所有的None对象都相同print(id(a), id(b))list_ = []print(list_, list_ is None) # 空的list对象不为空tuple_ = ()print(tuple_, tuple_ is None) # 空的tuple对象不为空dict_ = []print(dict_, dict_ is None) # 空的dict对象不为空# result# 1479058576# False# False# True# 1479058576 1479058576# [] False# () False# [] False ~ 链接字符串优先使用join为不是+ +每次相加都要申请新的内存，n个字符串相加就要申请n-1个内存 使用join时，会事先计算需要多少内存，然后只申请一次 123456789101112131415161718192021222324252627282930313233343536373839404142434445import timea = &apos;hello &apos;b = &apos;world &apos;c = &apos;!&apos;# 短连接time_1 = time.time()for i in range(1000000): d = a + b + ctime_2 = time.time()for i in range(1000000): d = &apos;&apos;.join([a, b, c])time_3 = time.time()print(&apos;+\t:&apos;, str(time_2-time_1))print(&apos;.join\t:&apos;, str(time_3-time_2))# 长连接time_4 = time.time()e = &apos;&apos;for i in range(1000000): e = e + (a + b + c)time_5 = time.time()temp_list = [&apos;hello world !&apos; for j in range(1000000)]e = &apos;&apos;.join(temp_list)time_6 = time.time()print(&apos;+\t:&apos;, str(time_5-time_4))print(&apos;.join\t:&apos;, str(time_6-time_5))# result# + : 0.15621352195739746# .join : 0.21873021125793457# + : 8.51358938217163# .join : 0.062482595443725586 这里的测试可以看出，join在短字符串拼接速度未必有+快，毕竟join是Python的解释器封装了一层。当长字符串拼接的时候二者相差速度很大，个人要根据情况使用。 ~ 格式化字符串尽量使用.format方式而不是% 理由： .format方式更加灵活，参数顺序没有要求 方便参数传递 官方推荐.format，%只是为了向后兼容而保留 性能上我这里没有测试，个人觉得%效率更高，毕竟.format是封装了一层 123456789101112131415161718192021222324252627282930313233343536373839# %操作格式化字符串print(&apos;value is %06.1f&apos; % 9.5) # 直接格式化字符print(&apos;a %s c %s&apos; % (&apos;b&apos;, &apos;d&apos;)) # 使用元组形式格式化print(&apos;a:%(a)s \t b:%(b)s&apos; % &#123;&apos;a&apos;: 1, &apos;b&apos;: 2&#125;) # 使用字典形式格式化# result# value is 0009.5# a b c d# a:1 b:2# .format操作格式化字符串print(&apos;a &#123;0&#125; c &#123;1&#125;&apos;.format(&apos;b&apos;, &apos;d&apos;)) # 使用位置符号print(&apos;1 &#123;0:,&#125; 3 &#123;1&#125;&apos;.format(2, 4))print(&apos;1 &#123;a&#125; 3 &#123;b&#125;&apos;.format(a=2, b=4)) # 使用名称class People: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return &apos;Information of people is &#123;self.name&#125;\t&#123;self.age&#125;&apos;.format(self=self)print(People(&apos;tom&apos;, 18)) # 通过属性tuple_ = (1, 2)print(&apos;&#123;0[0]&#125;\t&#123;0[1]&#125;&apos;.format(tuple_)) # 使用元组# result# a b c d# 1 2 3 4# 1 2 3 4# Information of people is tom 18# 1 2 ~ 函数既不是传参也不是穿引用 可变对象传应用，不可变对象传值 ~ 慎用变长参数 使用*args来实现可变参数列表：*args用于接受一个包装为元组形式的参数列表来传递非关键字参数，参数个数可以任意。 使用**kwargs接受字典形式的关键字参数列表，其中字典的键值对分别表示不可变参数的的参数名和值。 1234567891011121314151617181920212223242526# *argsdef sum_(*args): result = 0 for x in args[0:]: result += x return resultprint(sum_(1, 2, 3))# **kwargsdef print_(**kwargs): for name, value in kwargs.items(): print(&apos;&#123;0&#125;\t&#123;1&#125;&apos;.format(name, value))print(print_(a=1, b=2, c=3))# result# 6# a 1# b 2# c 3# None ~ str()和repr()的区别 作用： str()和repr()都可以将Python对象转换为字符串 区别： str主要是面向用户，其目的的可读性 repr是面向解释器，编程人员debug用途，repr可以用eval()函数还原对象 12345678910111213141516s = &apos;hello\tworld&apos;print(str(s))print(repr(s))print(eval(repr(s)), end=&apos;\t&apos;)print(eval(repr(s)) == s)try: print(eval(str(s)), end=&apos;\t&apos;)except Exception as err: print(err)# result# hello world# &apos;hello\tworld&apos;# hello world True# unexpected EOF while parsing (&lt;string&gt;, line 1) ~ 分清staticmethod和classmethod的使用场景 静态方法装饰器下定义的方法属于函数，存放逻辑性代码，主要是一些逻辑属于类，但是和类本身没有交互，即在静态方法中，不会涉及到类中的方法和属性的操作。可以理解为将静态方法存在此类的名称空间中。 类方法装饰器下定义的方法属于方法，类方法是将类本身作为对象进行操作的方法。他和静态方法的区别在于：不管这个方式是从实例调用还是从类调用，它都用第一个参数把类传递过来。 1234567891011121314151617181920212223class Method: def print_(self, x): print(self, x) @classmethod def class_print(cls, x): print(cls, x) @staticmethod def static_print(x): print(x)if __name__ == &apos;__main__&apos;: Method.print_(None, &apos;hello&apos;) Method.class_print(&apos;hello&apos;) Method.static_print(&apos;hello&apos;)# result# None hello# &lt;class &apos;__main__.Method&apos;&gt; hello# hello 库 ~ 按需选择sort()和sorted() 使用形式： 表示 sorted(iterable[, cmp[, key[, reverse]]]) s.sort([cmp[, key[, reverse]]]) 解释： cmp为用户定义的比较函数，函数的参数为两个可比较的元素 key是带一个参数的函数，用来为每个元素提取比较值 reverse表示排序结果是否反转 相比于sort()，sorted()函数使用范围更广 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 对字典排序dict_ = &#123; &apos;tom&apos;: &apos;14212&apos;, &apos;jury&apos;: &apos;41221&apos;, &apos;mark&apos;: &apos;21243&apos;&#125;from operator import itemgetterdict_1 = sorted(dict_.items(), key=itemgetter(1))print(dict_1)# 多维list排序list_ = [ [&apos;tom&apos;, &apos;male&apos;, &apos;14212&apos;], [&apos;jury&apos;, &apos;female&apos;, &apos;41221&apos;], [&apos;mark&apos;, &apos;male&apos;, &apos;21243&apos;]]from operator import itemgetterlist_1 = sorted(list_, key=itemgetter(2, 1))print(list_1)# 字典中混合list排序dict_list_ = &#123; &apos;tom&apos;: [&apos;male&apos;, 14212], &apos;jury&apos;: [&apos;female&apos;, 41211], &apos;mark&apos;: [&apos;male&apos;, 21243]&#125;from operator import itemgetterdict_list_1 = sorted(dict_list_.items(), key=lambda v: itemgetter(0)(v))print(dict_list_1)# list中混合字典排序list_dict_ = [ &#123;&apos;name&apos;: &apos;bob&apos;, &apos;age&apos;: 10&#125;, &#123;&apos;name&apos;: &apos;david&apos;, &apos;age&apos;: 3&#125;, &#123;&apos;name&apos;: &apos;patty&apos;, &apos;age&apos;: 4&#125;, &#123;&apos;name&apos;: &apos;carol&apos;, &apos;age&apos;: 9&#125;]from operator import itemgetterlist_dict_1 = sorted(list_dict_, key=itemgetter(&apos;age&apos;))print(list_dict_1)# result# [(&apos;tom&apos;, &apos;14212&apos;), (&apos;mark&apos;, &apos;21243&apos;), (&apos;jury&apos;, &apos;41221&apos;)]# [[&apos;tom&apos;, &apos;male&apos;, &apos;14212&apos;], [&apos;mark&apos;, &apos;male&apos;, &apos;21243&apos;], [&apos;jury&apos;, &apos;female&apos;, &apos;41221&apos;]]# [(&apos;jury&apos;, [&apos;female&apos;, 41211]), (&apos;mark&apos;, [&apos;male&apos;, 21243]), (&apos;tom&apos;, [&apos;male&apos;, 14212])]# [&#123;&apos;name&apos;: &apos;david&apos;, &apos;age&apos;: 3&#125;, &#123;&apos;name&apos;: &apos;patty&apos;, &apos;age&apos;: 4&#125;, &#123;&apos;name&apos;: &apos;carol&apos;, &apos;age&apos;: 9&#125;, &#123;&apos;name&apos;: &apos;bob&apos;, &apos;age&apos;: 10&#125;] ~ 使用copy模块深拷贝对象 123456789101112131415161718192021222324# -*-coding:utf-8 -*-import copya = [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;]] # 原始对象b = a # 赋值，传对象的引用c = copy.copy(a) # 对象拷贝，浅拷贝d = copy.deepcopy(a) # 对象拷贝，深拷贝a.append(5) # 修改对象aa[4].append(&apos;c&apos;) # 修改对象a中的[&apos;a&apos;, &apos;b&apos;]数组对象print(&apos;a = &apos;, a)print(&apos;b = &apos;, b)print(&apos;c = &apos;, c)print(&apos;d = &apos;, d)# result# a = [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], 5]# b = [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], 5]# c = [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]]# d = [1, 2, 3, 4, [&apos;a&apos;, &apos;b&apos;]] ~ 使用pandas处理大型csv文件 ~ 使用ElementTree解析XML xml.dom.minidom和xom.sax解析XML文件 ~ session和cache存储 序列化，就是把内存中的数据结构在不丢失其身份和类型信息的情况下转换为二进制表示的过程 调用 pickle.dump(obj, file[, protocol])：序列化数据到一个文件描述符（一个打开的文件、套接字等） load(file)：反序列化 特性： 不能保证操作的原子性 存在安全性问题 pickle是Python独有的，不同语言之间可能不兼容 ~ 序列化的另一个选择–JSON ~ 使用traceback获取栈信息 ~ 使用logging记录日志信息 日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL 对象： logger：程序在运行的时候记录相应的信息，并根据设置的日志级别或filter来决定哪些信息需要输出 Handler：处理信息的输出 Formatter：log信息的格式 Filter：用来决定哪些信息需要输出 ~ 使用threading模块编写多线程程序 thread模块提供了多线程底层支持模块，以低级原始的方式来处理和控制线程，使用起来比较复杂 threading模块基于thread进行包装，将线程的操作对象化 thread模块不支持守护进程 Python3已经不存在thread模块 ~ 使用Queue使多线程编程更安全 Queue.Queue(maxsize)：先进先出，maxsize为队列大小，其值为非正数的时候为无限循环队列 Queue.LifoQueue(maxsize)：后进后出 Queue.PriorityQueue(maxsize)：优先队列 Queue.qsize()：返回近似的队列大小 Queue.empty()：队列判空 Queue.full()：队列的判满 Queue.put(item[, block[, timeout]])：往队列中添加元素tiem，block设置为False的时候，如果队列满则抛出Full异常~~~ Queue.put_nowait(item)：等价于put(item, False).block设置为False的时候 Queue.get([block[, timeout]])：从队列中删除元素并返回该元素的值 Queue.get_nowait()：等价于get(False) Queue.task_done()：发送信号表明入列元素已经完成 Queue.join()：阻塞直至队列中所有的元素处理完毕 设计模式内部机制 ~ 理解名字查找机制 局部作用域 全局作用域 嵌套作用域 内置作用域 ~ 使用更为安全的property 用来实现属性可管理的built-in数据类型 property(fget=None, fset=None, fdel=None, doc=None) -&gt; property attribute 代码简洁 控制属性权限访问，提高数据的安全性 代码维护性更好 123456789101112131415161718192021class People(object): def __init__(self): self.age = 18 @property def main(self): return self.agepl = People()print(pl.main)try: pl.main = 12 print(pl.main)except Exception as err: print(err)# result# 18# can&apos;t set attribute ~ 掌握metaclass 元类是关于类的类，是类的模板 元类是用来控制创建类的 元类的实例为类 ~ 协程，又称微线程 线程缺点： 对队列的操作需要有显式/隐式（使用线程安全的队列）的加锁操作 “谦让式”的多进程CPU资源分配方式，性能很低 ~ 理解GIL的局限性 多线程Python程序运行的速度比只有一个线程的时候还要慢 GIL称为全局解释器锁，是Python虚拟机上用作互斥线程的一种机制，它的作用是保证任何情况下虚拟机中只会有一个线程被运行，其它线程都处于等待GIL锁被释放的状态。 ~ 对象的管理与垃圾回收 显式调用gc.collect()进行垃圾回收 创建新的对象为其分配内存的时候，检查threshold阈值，当对象的数量超过threshold的时候便自动进行垃圾回收 1234567891011121314151617181920# 类的内存管理import gcprint(gc.isenabled())print(gc.get_threshold())# 变量的内存管理a = 1print(a)del atry: print(a)except Exception as err: print(&apos;a is already delete&apos;)# result# True# (700, 10, 10)# 1# a is already delete 使用工具辅助项目开发 ~ 做paster创建包 ~ 理解单元测试概念 ~ 将包发布到PyPI 性能刨析与优化 ~ 借助性能优化工具 Psyco Pypy ~ 利用cProfile定位性能瓶颈 ~ memory_profile Objgraph ~ 充分利用set的优势 Python中集合是通过Hash算法实现的无序不重复的元素集 操作 s.union(t)：s和t的并集 s.itersection(t)：s和t的交集 s.difference(t)：s和t的差集 s.symmetric_difference(t)：s和t的并集减去s和t的交集 12345678910set_1 = set(&apos;hello&apos;)print(set_1)set_2_ = [1, 2, &apos;34&apos;, (5, 6)]set_2 = set(set_2_)print(set_2)# result# &#123;&apos;h&apos;, &apos;l&apos;, &apos;o&apos;, &apos;e&apos;&#125;# &#123;(5, 6), 1, 2, &apos;34&apos;&#125; ~ 使用multiprocessing克服GIL的缺陷 ~ 使用线程池提高效率 创建、就绪、运行、阻塞和终止 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import Queue # 队列模块import sys # 系统调用模块import threading # 多线程模块import urllib # 网络访问请求模块import os # 系统操作模块# 处理request的工作进程class Worker(threading.Thread): def __init__(self, work_queue, result_queue, **kwds): threading.Thread.__init__(self, **kwds) self.setDaemon(True) self.workQueue = work_queue self.resultQueue = result_queue def run(self): while True: try: callable, args, kwds = self.workQueue.get(False) # 从一个队列中取出一个任务 res = callable(*args, **kwds) self.resultQueue.put(res) # 存放处理结果到队列中 except Queue.Empty: breakclass WorkerManager: # 线程池管理器 def __init__(self, num_of_workers=10): self.work_queue = Queue.Queue() # 请求队列 self.result_queue = Queue.Queue() # 输出结果的队列 self.works = [] self._recuritThreads(num_of_workers) def _recuritThreads(self, num_of_workers): for i in range(num_of_workers): worker = Worker(self.work_queue, self.result_queue) self.works.append(worker) def start(self): # 启动线程 for w in self.works: w.start() def wait_for_complete(self): while len(self.works): worker = self.works.pop() # 从池中取出一个线程处理请求 worker.join() if worker.isAlive() and not self.work_queue.empty(): self.workers.append(wroker) # 重新加入线程池中 print(&apos;All job were completed.&apos;) def add_job(self, callable, *args, **kwds): self.work_queue.put((callable, args, kwds)) # 往工作队列中加入请求 def get_result(self, *args, **kwds): # 获取处理结果 return self.result_queue.get(*args, **kwds) def download_file(url): print(&apos;start download&apos;, url) urlhandler = urllib.urlopen(url) f_name = op.path.base_name(url)+&apos;.html&apos; with open(f_name, &apos;wb&apos;) as f: while True: chunk = urlhandler.read(1024) if not chunk: break f.write(chunk)urls = [&apos;http://baidu.com&apos;, &apos;http://google.com&apos; ]wm = WorkerManager(2) # 创建线程池for i in urls: wm.add_job(download_file, i) # 将所有请求加入队列中wm.start()wm.wait_for_complete() 上边的代码没有测试，慎用 ~ 使用C/C++模块扩展提高性能 使用ctype模块调用C语言 使用SWIG调用C代码 ~ 使用Cython编写扩展模块 Cython对性能并没有成倍的提升 Cython就相当于一门新的语言，和python关系不大 不建议使用Cython 安装 pip install -U cython]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Code-Skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Effective Python]]></title>
    <url>%2F2018%2F09%2F18%2FEffective-Python%2F</url>
    <content type="text"><![CDATA[编辑时间：2018.9.18 有一部分是原书的内容，但测试样例全都是使用Python3.6.6自己测试的，Python的版本在不断的优化，因此语法在变化优化，本文有时效性大概半年吧。 一、用Python方式来思考pep8切割序列12345678910111213141516list_ = [1, 2, 3, 4, 5]print(list_[:2]) # 前两个print(list_[:-2]) # 去掉后两个print(list_[-2:]) # 后两个print(list_[2:]) # 去掉前两个print(list_[-3:3]) # 前三个和后三个的交集print(list_[2:-2]) # 去掉前两个和后三个assert list_[2:] == list_[2:len(list_)] # assert是断言的意思，这里是断言两者相等为真，如果不相等就会报错# result# [1, 2]# [1, 2, 3]# [4, 5]# [3, 4, 5]# [3]# [3] 在单次切片操作内，不要同时指定start、end和stride somelist[start : end : stride] 123456list_ = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]print(list_[::2]) # 序列长度为n时，取n/2个值，每次取未取序列中的第2个值print(list_[::1]) # 序列长度为n时，取n/1个值，每次取未取序列中的第1个值print(list_[::-1]) # 序列长度为n时，取n/1个值，序列先倒序，然后每次取未取序列中的第1个值print(list_[::-2]) # 序列长度为n时，取n/2个值，序列先倒序，然后每次取未取序列中的第2个值print(list_[2:6:2]) # 序列长度为n时，先取出序列的第3个到第7个值，然后每次取未取序列中的第2个值 用列表推导来取代map和filter12345678910111213141516171819202122232425262728293031323334list_ = [1, 2, 3, 4, 5]list_1 = [i**2 for i in list_] # 列表推导print(list_1)list_2 = map(lambda x: x**2, list_) # map函数print(list_2)print(type(list_2))print(list_)list_3 = [x**2 for x in list_ if x % 2 == 0]print(list_3)list_4 = map(lambda x: x**2, filter(lambda x: x % 2 == 0, list_))print(list_4)dict_ = &#123;&apos;a&apos;: 1, &apos;b&apos;: 2, &apos;c&apos;: 3&#125;dict_1 = &#123;rank: name for name, rank in dict_.items()&#125;print(dict_1) # 字典中，值和键交换dict_2 = &#123;1: &apos;hello&apos;, 2: &apos;world&apos;, 3: &apos;!&apos;&#125;set_1 = &#123;len(strings) for strings in dict_2.values()&#125;print(set_1)# result# [1, 4, 9, 16, 25]# &lt;map object at 0x00000254CB441208&gt;# &lt;class &apos;map&apos;&gt;# [1, 2, 3, 4, 5]# [4, 16]# &lt;map object at 0x00000254CB4413C8&gt;# &#123;1: &apos;a&apos;, 2: &apos;b&apos;, 3: &apos;c&apos;&#125;# &#123;1, 5&#125; 不要使用含有两个以上表达式的列表推导，这样可行但是会减少代码的可读性。可以使用多次单个表达式的列表推导来代替多个表达式的列表推导。使用生成器来改写数据量较大的列表推导12345678910111213list_ = [1, 2, 3, 4, 5]root = ((x, x**2) for x in list_)print(next(root))print(next(root))print(next(root))# result# (1, 1)# (2, 4)# (3, 9) 尽量使用enumerate替代range1234567891011121314151617181920list_ = [1, 2, 3, 4, 5]for i in range(len(list_)): print(list_[i])for i, value in enumerate(list_): print(i, &apos;: &apos;, value)# result# 1# 2# 3# 4# 5# 0 : 1# 1 : 2# 2 : 3# 3 : 4# 4 : 5 使用zip函数同时遍历多个迭代器1234567891011121314151617181920212223242526272829list_1 = [1, 2, 3]list_2 = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]list_3 = [10, 20, 30]for number, string in zip(list_1, list_2): print(number, string)list_1.append(4)print(list_1)for number, string in zip(list_1, list_2): print(number, string)for a_, b_, c_ in zip(list_1, list_2, list_3): print(a_, b_, c_)# result# 1 a# 2 b# 3 c# [1, 2, 3, 4]# 1 a# 2 b# 3 c# 1 a 10# 2 b 20# 3 c 30 二、函数尽量使用异常来表示特殊情况，而不要返回None（捕捉异常）考虑用生成器来改写直接返回列表的函数1234567891011121314151617def add_(): a, b = 1, 1 while True: c = a + b yield c a, b = b, csum_ = add_()if __name__ == &apos;__main__&apos;: for i in range(10): print(next(sum_), end=&apos;, &apos;)# result# 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 这里的函数的返回值由yield决定 三、类与继承四、元类及属性五、并发及并行 Python的多线程、多进程、协程貌似不太好，使用多进程时要特别注意多进程结束后各个进程的执行顺序 用subprocess模块来管理子进程 可以用线程来执行阻塞式I/O，但不要用它做计算 Python的GIL是一把互斥锁，防止抢占式多进程切换操作的干扰。因此同一时刻只有一条线程在跑，解决办法就是使用其他语言来写多进程的操作，使用Python来调用。 使用Queue来协调各个线程之间的工作 Python的解释器对于线程结束后的操作一直是混乱的，这里自己可以使用Python的内置Queue函数来协调 考虑使用协程来并发地运行多个函数 Python中，核心、线程、进程、子进程和协程，协程之家切换代价最小 Python的协程没有锁，这个比进程好用，但要小心不要产生死循环（自由的代价） 考虑使用费concurrent.futures来实现并行计算 六、内置模块 使用datetime模块来处理本地时间，而不是time模块 time模块是当前时间减去很久前的一个固定时间，单位是秒 使用Python内置的算法与数据结构 双向队列 collection模块中的deque类 有序字典 collection模块中的OrderedDict类 带有默认值的字典 collection模块中的defaultdict类 堆队列（优先队列） heapq模块 二分查找 bisect模块中bisect_left等函数 与迭代器有关的工具 内置的itertools模块 七、协作开发 为自编的模块定义根异常 书中建议使用虚拟环境隔离项目，本人多次搭建虚拟环境，但觉得还是正常环境就好，或者使用docker 八、部署 repr字符串来输出调试信息 使用tracemalloc来测试内存泄漏情况 个人总结 这貌似是本人有史以来读的第一本老美比国人写的糟的书，也有可能是年代久远吧~~~ 对于性能的优化，使用CPython应该比较好吧，Cython就是一门新的语言，感觉语法很无语。或者使用SWIG或者ctype模块调用c语言代码。 Python的并行方面真的做的不够好吧]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浪潮之巅]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%B5%AA%E6%BD%AE%E4%B9%8B%E5%B7%85%2F</url>
    <content type="text"><![CDATA[原书很早前就看完了，这是别人的总结，自己没有时间重读了~~~ 定律 摩尔定律 差不多是每18个月，就是每1年半，IT产品的性能就会翻一番，价格则会降一半。摩尔定律里说的产品性能主要指3个方面，一方面是计算机的运算速度，另一方面是计算机的内存，还有就是上网的速度。摩尔定律让公司的研发必须针对多年以后的市场。例如，在Google内部，每一次基础架构的升级，都要按照目前计算和存储能力的10倍来设计。 安迪-比尔定律 整个IT生态链：以微软为首的软件开发商吃掉硬件带来的全部好处，迫使用户更换机器，让惠普，戴尔这些制造硬件的公司收益，这些公司再向英特尔这样的半导体公司订购新的芯片，同时再向外设厂商购买新的外设，在这中间，各家的利润先后得到了相应的提升，股票也随着增长，各个硬件厂商赚到钱后重新投入进行研发。如果硬件更新不了了，软件的复杂性也就有了一个终极限制。 诺维格定律 当一家公司在某个领域的市场占有率超过百分之50后，将无法再使市场占有率翻翻，就必须寻找新的市场。诺维格定律的第二层意思，一家公司永远不会有稳定的时候，永远得不断的寻找新的增长点。怎么找到新的增长点，一个是横向扩展，另一种是转型。横向扩展，就是一家公司把现有的技术和商业优势，用到相关的市场上去。可以最大限度的利用公司原来的优势和经验，在新的领域很快站住脚。 基因定律 转型说起来容易，做起来特别难，因为基因定律，就是说一家在某一个领域特别成功的公司，当开拓新领域时，会不自觉的用原来的做事方法，思维方式去应对新市场。新的领域是年轻公司唯一的机会，自然会全力以赴。而成熟的公司一旦新业务和传统业务有冲突，通常的策略是牺牲新业务，让转型又多了一个隐形的障碍。苹果从个人电脑到iPhone，再到iPad，看起来是成功转型了，但其实它的基因并没有变，还是一个相对封闭的系统，苹果的基因就决定了它必须靠硬件来赚软件的钱。 Web发展 web1.0 代表 新浪与搜狐、网易三大门户网站 释义 用户通过浏览器获取网络信息 web2.0 代表公司 论坛、博客、微博等相关社交平台 释义 用户既是网络内容的获取者，也是网络内容的制造者 web3.0 目前仍是概念阶段，没有明确的定义3.0的范围。 个人觉得前面的1.0和2.0的发展产生了过多的网络内容，2.0的后期已经显现出互联网为用户提供了过多的信息。3.0的发展趋势必然是从大量的网络信息中提取出来有用的关键信息，为用户提供精小的信息。而这就是爬虫、大数据和人工智能的一方面应用。 浪潮之巅 近百年来，总有一些公司很幸运地、有意无意地站在技术革命的浪尖上。一旦处在那个位置，即使不做任何事，也可以随着波浪顺顺当当向前漂十年甚至更长的时间。在这十几年间，它们代表着科技的浪潮，直到下一波浪潮的来临。 这些公司里的人，无论职位高低，在外人看来，都是时代的幸运儿。因为，虽然对一个公司来说，赶上一次浪潮不能保证其长盛不衰；但是，对一个人来说，一生赶上一次这样的浪潮就足够了。一个弄潮的年轻人，最幸运的，莫过于赶上一波大潮。 没有不消亡的帝国，同样也没有永不衰退的企业，浪潮不断推动企业兴衰更替，而从投资银行到求职者，都希望找到下一个浪潮，能站在下一个浪潮之巅的必将是下一个谷歌一般的企业。]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Think</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式 python]]></title>
    <url>%2F2018%2F09%2F11%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-python%2F</url>
    <content type="text"><![CDATA[Mastering Python Design Patterns 工厂模式 工厂方法： 调用一个类或者一个函数时，只需要明白输入和输出，而不必关心内部的实现过程。 抽象工厂： 单个类内包括多个函数；单个类内包括多种方法。 使用场景： 对象之间的解耦 优化性能和资源占用 单步类对象的创建 示例：123456789101112131415161718192021class Compute: def add(a, b): return a + b def reduce(a, b): return a - bif __name__ == &apos;__main__&apos;: a = 2 b = 1 sum_ab = Compute.add(a, b) print(sum_ab) reduce_ab = Compute.reduce(a, b) print(reduce_ab)# result:# 3# 1 建造者模式 一个对象的创建需要多个步骤，并且不同步骤产生不同的表现。 使用场景： 创建复杂对象 要求一个对象有不同的表现，且对象的构造与表现解耦 原型模式 创建一个对象的克隆，输入一个对象，返回一个对象的副本（改变副本不会对原型造成影响） 使用场景： 当创建一个浅副本时，副本依赖引用的原型（提升应用性能和优化内存，但需要小心修改数据） 当创建一个深副本时，副本不依赖引用的原型，并复制所有原型 示例：12345678910111213141516171819import copydict1 = &#123;&apos;num&apos;: [1, 2, 3]&#125;dict2 = copy.copy(dict1)dict3 = copy.deepcopy(dict1)dict1[&apos;num&apos;].remove(1)print(dict1)print(dict2)print(dict3)# result:# &#123;&apos;num&apos;: [2, 3]&#125;# &#123;&apos;num&apos;: [2, 3]&#125;# &#123;&apos;num&apos;: [1, 2, 3]&#125; 适配器模式 是一种结构型设计模式，实现两个不兼容接口之间的兼容 使用场景： 让新的插件用在旧的代码上或者让旧的插件用在新的代码上。 示例：12345678910111213141516171819202122def main(a, b): return a + bdef adapter(list_): return main(list_[0], list_[1])if __name__ == &apos;__main__&apos;: a = 1 b = 2 c = main(a, b) print(c) list_ = [1, 2] c = adapter(list_) print(c)# result:# 3# 3 修饰器模式 动态（运行时）扩展一个对象的功能（继承关系是静态的）。 使用场景： 扩展一个对象的行为，无需继承，一个父类可以修饰成为多个不同的子类。 示例：12345678910111213141516def fun_1(fun): fun() print(&apos;fun 1&apos;)@fun_1def fun_2(): print(&apos;fun 2&apos;)# result:# fun 2# fun 1# 执行顺序fun_1(fun_2()) 12345678910111213141516171819202122232425262728293031323334353637def fun_1(func): def main(): print(&quot;fun_1 main&quot;) func() return main # 调用main函数# 老方法def fun_2(): print(&quot;fun_2&quot;)fun_old = fun_1(fun_2) # 装饰# 新方法@fun_1def fun_3(): print(&apos;fun_3&apos;)if __name__ == &apos;__main__&apos;: fun_old() # 执行顺序fun_2(fun_1()) print(&apos;-----&apos;) fun_3() # 执行顺序fun_3(fun_1())# result:# fun_1 main# fun_2# -----# fun_1 main# fun_3 外观模式 调用一个复杂系统时，不必考虑内部的复杂实现，只需要了解使用方法和API（像是工厂模式的进阶吧） 使用场景： 隐藏系统复杂性的方式 享元模式 通过为相似对象引入数据共享来最小化内存，提升性能 使用场景： 在应用需要创建大量的计算代价大但共享许多属性的对象时 模型-视图-控制器模式 每个部分有明确的职责，模型负责数据访问，控制器负责控制过程，视图时模型的表现 使用场景： Web框架（MVC） 代理模式 使用代理对象在访问实际对象之前执行重要操作 使用场景： 远程代理 虚拟代理 防护代理 智能代理 责任链模式 发送方可直接访问链中的首个节点。若首个节点不能处理请求，则转发给下一个节点，如此直到请求被某个节点处理或者整个链遍历结束 命令模式 将一个操作（撤销、重做、复制、粘贴等）封装成一个对象 解释器模式 用于为高级用户和领域专家提供一个类编 程的框架，但没有暴露出编程语言那样的复杂性 观察者模式 描述单个对象（发布者，又称为主持者或可观察者）和一个或多个对象（订阅者， 又称为观察者）之间的发布-订阅关系 状态模式 状态机是一个抽象机器，具有两个主要部分：状态和转换。状态是指一个系统的当前状况。 一个状态机在任意时间点只会有一个激活状态；转换是指从当前状态到一个新状态的切换。 策略模式 使用多种算法来解决问题，最终采用最优的 模板模式 在实现结构相近的算法时，可以使用模板模式来消除冗 余代码。]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Code-Skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机是怎样跑起来的]]></title>
    <url>%2F2018%2F09%2F11%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B7%91%E8%B5%B7%E6%9D%A5%E7%9A%84%20%E7%A8%8B%E5%BA%8F%E6%98%AF%E6%80%8E%E6%A0%B7%E8%B7%91%E8%B5%B7%E6%9D%A5%E7%9A%84%2F</url>
    <content type="text"><![CDATA[计算机是怎样跑起来的 Hz（赫兹）是CPU频率单位，1秒发出1次时钟信号就是1Hz。100MHz=100*100万=1亿次/秒 IC是继承电路（Integrated Circuit）的简称 CPU的频率就是‘时钟信号’的电信号，可用来衡量CPU的速度 程序的流程：顺序执行、条件分支、循环 事件驱动：用户的操作等产生事件后，由事件决定程序的流程 OOP 继承、封装、多态 LAN 是Local Area Network 局域网 MAC地址就是计算机网卡中在出厂时被分配的不可更改的MAC地址（硬件的自己懂的话还是可以改的） IP地址到MAC地址的转换就是ARP 地址解析协议 程序是怎样跑起来的 CPU是由寄存器、控制器、运算器和时钟组成的 寄存器用来暂存指令、数据等处理对象、可看做内存的一部分 控制器负责把内存上的指令等读入寄存器，并根据指令的执行结果来控制整个计算机 运算器负责运算从内存读入寄存器的数据 时钟负责发出CPU开始计时的时钟信号 每个字节（1字节=8位） 每个CPU大概有20-100个寄存器，主要分为： 每个CPU中包含 名称 作用 1个 累加寄存器 存储执行运算的数据和运算后的数据 1个 标志寄存器 存储运算处理后的CPU状态 1个 程序计数器 存储下一条指令所在内存的地址 n个 基址寄存器 存储数据内存的起始地址 n个 变址寄存器 存储基址寄存器的相对地址 n个 通用寄存器 存储任意数据 1个 指令寄存器 存储指令 1个 栈寄存器 存储栈区域的起始地址 函数的调用机制 call指令会把调用函数后要执行的指令地址存储在名为栈的主存内 return命令的功能是吧保存在栈内的地址设定到程序的计数器中 通过地址和索引实现数组 通过基址寄存器和变址寄存器对主存区域进行划分 CPU会把基址寄存器+变址寄存器的值解释为实际查看的内存地址 变址寄存器的值就相当于高级变成语言程序中数组的索引功能 机器语言指令的主要类型和功能： 类型 功能 数据转送指令 寄存器和内存、内存和内存、寄存器和外围设备之间的数据读取操作 运算指令 用累加寄存器执行算数运算、逻辑运算、比较运算和移位运算 跳转指令 实现条件分支、循环、强制跳转等 call/return指令 函数的调用/返回调用前的地址 8位二进制数称为一个字节 移位运算和乘除运算的关系 左移一位就是对二进制数乘2 右移一位就是对二进制数除2 二进制数中正负的表示： 把最高位作为符号来使用，最高位就是符号位。 符号位是0表示正数，符号位是1表示负数 补数就是用正数来表示负数 原始二进制数转化其补数：所有位取反，然后加11的二进制数是00000001，取反后是11111110，加1后是11111111 逻辑右移和算数右移 逻辑右移是右移后前面空出来的高位用0补充 算数右移是右移后前面空出来的高位用右移前符号位的值来补足 浮点数是用符号、尾数、基数和指数来表示的小数 内存物理机制 DRAM SRAM ROM 磁盘缓存加快了磁盘的访问速度，指把从磁盘中读出的数据存储到内存空间中的方式 虚拟内存是指把磁盘的一部分作为假象的内存来使用 节约内存的编程方法 DLL文件是在程序运行时可以动态加载的文件 通过调用_stdcall来减小文件的大小 栈清理 磁盘的物理结构 扇区方式 把磁盘表面分成若干同心圆的空间就是磁道 把磁道按固定大小划分成的空间就是扇区 可变长方式 BIOS和引导]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Computer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mnist(softmax、NN、CNN)]]></title>
    <url>%2F2018%2F09%2F05%2Fmnist-softmax%E3%80%81NN%E3%80%81CNN%2F</url>
    <content type="text"><![CDATA[预处理 tensorflow库内包含mnist，直接加载mnist数据并转为一维数组形式。直接加载的是.gz格式。 123import tensorflow as tfimport tensorflow.examples.tutorials.mnist.input_data as input_data # 加载mnist数据mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) # one_hot为是否将标签转为一维数组形式 逻辑回归 加载数据 图片转为一维数组 建立模型：softmax回归模型 w为可变n*784二维矩阵，b为10数组 w、b变量初始化为0 y=w*x+b 损失函数：交叉熵 训练模型 模型评估 12345678910111213141516171819202122232425262728293031323334353637383940# -*- coding: utf-8 -*-# 读取数据图片，预处理import tensorflow as tfimport tensorflow.examples.tutorials.mnist.input_data as input_data # 加载mnist数据mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) # one_hot为是否将标签转为一维数组形式# 构建softmax回归模型sess = tf.InteractiveSession() # 交互作用类（不需用在计算前构建整个图）# 占位符（x和y_）x = tf.placeholder(&quot;float&quot;, shape=[None, 784]) # 浮点数，二维数组（第一维大小不定，第二维是784）y_ = tf.placeholder(&quot;float&quot;, shape=[None, 10]) # 用于代表对应某一MNIST图片的类别# 变量（w权重和b偏置）w = tf.Variable(tf.zeros([784, 10])) # 784*10的可变参数值二维矩阵b = tf.Variable(tf.zeros([10])) # 10维的向量sess.run(tf.initialize_all_variables()) # 初始化所有变量为0# 类别预测与损失函数y = tf.nn.softmax(tf.matmul(x, w) + b) # 计算每个分类的softmax概率值cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) # 损失函数（目标类别和预测类别之间的交叉熵）# 训练模型train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) # 最速下降法，步长为0.01for i in range(1000): batch = mnist.train.next_batch(50) # 每步加载50个样本 train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1]&#125;) # feed_dict被每次训练的数据替代# 模型评估correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # 检测预测值与实际值是否匹配accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;)) # 将布尔数组转化为正确率print(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) # 输出最终正确率 神经网络 加载数据集 定义函数： 初始化函数 神经元模型：mlp、逻辑回归、s函数 照片转一维数组、确定测试、训练的照片、标签 占位符：定义张量 调用神经元模型函数 计算代价函数、构造优化器、求行最值 初始化变量、迭代100次 打印正确率 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# -*- coding: utf-8 -*-# 神经网络import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_data # 加载数据def init_weight(shape): # 初始化 return tf.Variable(tf.random_normal(shape, stddev=0.01)) # (stddev是标准差，random_normal是正态分布的随机输出值)张量的可变随机值def model(X, w_h, w_o): # 神经元模型 h = tf.nn.sigmoid(tf.matmul(X, w_h)) # 这是一个基本的mlp，两个堆栈逻辑回归 # X和w_h矩阵相乘，s函数 return tf.matmul(h, w_o) # 在最后不使用softmax，因为代价函数 # 返回h和w_o的两矩阵之积mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) # 把数据转换为一位数组的格式trX = mnist.train.images # 训练的图片trY = mnist.train.labels # 训练的标签teX = mnist.test.images # 测试的图片teY = mnist.test.labels # 测试的标签# 占位符X = tf.placeholder(&quot;float&quot;, [None, 784]) # 第一维长度不定，第二维长度为784的二维矩阵 浮点数 784=28*28Y = tf.placeholder(&quot;float&quot;, [None, 10]) # 输出的10种情况w_h = init_weight([784, 625]) # 创建特征变量 # 调用上边自定义的函数，对矩阵进行初始化w_o = init_weight([625, 10]) # 初始化py_x = model(X, w_h, w_o) # 调用上边的自定义函数，cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) # 计算代价train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # 构造优化器 # 0.05的学习率使代价函数最小predict_op = tf.argmax(py_x, 1) # 求行中最大值# 在会话中启动图表with tf.Session() as sess: # 初始化所有变量 tf.global_variables_initializer().run() # 添加用于初始化变量的节点， for i in range(100): # 迭代100次 for start, end in zip(range(0, len(trX)), range(128, len(trX)+1, 128)): # zip函数是将两个列表打包为元组的列表，元素个数与最短列表一致 sess.run(train_op, feed_dict=&#123;X: trX[start: end], Y: trY[start:end]&#125;) # 跑 print(i, np.mean(np.argmax(teY, axis=1) == sess.run(predict_op, feed_dict=&#123;X: teX&#125;))) # mean函数是求平均值，打印预测和实际相同时的概率 # for i in range(100): # for start, end in zip(range(0, 1000), range(128, 1000+1, 128)): # sess.run(train_op, feed_dict=&#123;X: trX[start: end], Y: trY[start:end]&#125;) # print(i, np.mean(np.argmax(teY, axis=1) == # sess.run(predict_op, feed_dict=&#123;X: teX&#125;))) 卷积神经网络 CNN 输入层 卷积层 激活函数 池化层 全连接层 卷积就是为了降维 池化就是数据压缩，特征压缩（提取主要特征） 加载数据 定义初始化函数、定义模型函数（relu、max_pool、dropout） 图片转一维数组 张量、初始化 调用模型函数 训练的下降率 argmax 迭代10次 测试集是打乱的（np.random.shuffle） 打印准确率 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277-*- coding: utf-8 -*-# # 读取数据图片，预处理# import tensorflow as tf# import tensorflow.examples.tutorials.mnist.input_data as input_data # 加载mnist数据# mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) # one_hot为是否将标签转为一维数组形式# 构建一个多层卷积神经网络import tensorflow as tf# 权重初始化def weight_variable(shape): # 权重 initial = tf.truncated_normal(shape, stddev=0.01) return tf.Variable(initial)def bias_variable(shape): # 偏置 initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)# 卷积和池化def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;) # 1步长的卷积，0边距的模板（用0填充边界）def max_pool_2x2(x): # 池化使用2*2的模板 return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)# 第一层卷积W_conv1 = weight_variable([5, 5, 1, 32]) # 卷积在每个5*5的patch中算出32个特征b_conv1 = bias_variable([32]) # 每个输出的通道都有一个的对应的偏置量x_image = tf.reshape(x, [-1, 28, 28, 1]) # x变为一个4d向量，第2、3维是图片的宽、高，第4维是图片的颜色通道数h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # 把x_image和权值向量进行卷积，加上偏置项，然后使用RELU函数h_pool1 = max_pool_2x2(h_conv1) # 最后进行max_pooling（四个像素点中选取最大的）# 第二层卷积W_conv2 = weight_variable([5, 5, 32, 64]) # 每个5*5的patch中算出64个特征b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# 密集层网络W_fc1 = weight_variable([7*7*64, 1024]) # 图片尺寸减少到了7*7，加入一个有1024个神经元的全连接层b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) # 把池化层输出的张量reshape为一些向量h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) # 乘上权重矩阵，加上偏置，然后使用RELU# Droput(减少过拟合)keep_prob = tf.placeholder(&quot;float&quot;) # 训练中启用dropout，测试中关闭的dropouth_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# 输出层（添加一个softmax层）W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # softmax函数# 训练和模型评估（ADAM优化器做梯度下降，每100次迭代输出一次日志）cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x: batch[0], y_: batch[1], keep_prob: 1.0 &#125;) print(&apos;step %d, training accuracy %g&apos; % i, train_accuracy) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print(&apos;test accuracy %g&apos; % accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;))----------------------------------------------------------------------------------------------------------------------# # -*- coding: utf-8 -*-import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_databatch_size = 128test_size = 256def init_weights(shape): return tf.Variable(tf.random_normal(shape, stddev=0.01))def model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden): l1a = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l1 = tf.nn.dropout(l1, p_keep_conv) l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l2 = tf.nn.dropout(l2, p_keep_conv) l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]]) l3 = tf.nn.dropout(l3, p_keep_conv) l4 = tf.nn.relu(tf.matmul(l3, w4)) l4 = tf.nn.dropout(l4, p_keep_conv) pyx = tf.matmul(l4, w_o) return pyxmnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labelstrX = trX.reshape(-1, 28, 28, 1)teX = teX.reshape(-1, 28, 28, 1)X = tf.placeholder(&quot;float&quot;, [None, 28, 28, 1])Y = tf.placeholder(&quot;float&quot;, [None, 10])w = init_weights([3, 3, 1, 32])w2 = init_weights([3, 3, 32, 64])w3 = init_weights([3, 3, 64, 128])w4 = init_weights([128 * 4 * 4, 625])w_o = init_weights([625, 10])p_keep_conv = tf.placeholder(&quot;float&quot;)p_keep_hidden = tf.placeholder(&quot;float&quot;)py_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)predict_op = tf.argmax(py_x, 1)#with tf.Session() as sess: # tf.global_variables_initializer().run() for i in range(10): train_batch = zip(range(0, len(trX), batch_size), range(batch_size, len(trX) + 1, batch_size)) for start, end in train_batch: sess.run(train_op, feed_dict=&#123;X: trX[start: end], Y: trY[start: end], p_keep_conv: 0.8, p_keep_hidden: 0.5&#125;) test_indices = np.arange(len(teX)) np.random.shuffle(test_indices) test_indices = test_indices[0:test_size] print(i, np.mean(np.argmax(teY[test_indices], axis=1) == sess.run(predict_op, feed_dict=&#123;X: teY[test_indices], Y: teY[test_indices], p_keep_conv: 1.0, p_keep_hidden: 1.0&#125;)))----------------------------------------------------------------------------------------------------------------------import tensorflow as tfimport numpy as npfrom tensorflow.examples.tutorials.mnist import input_data # 加载数据batch_size = 128test_size = 256def init_weights(shape): # 初始化 return tf.Variable(tf.random_normal(shape, stddev=0.01))def model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden): # 定义的模型函数 l1a = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) # l1a shape=(?, 28, 28, 32) l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1], # l1 shape=(?, 14, 14, 32) strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l1 = tf.nn.dropout(l1, p_keep_conv) l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) # l2a shape=(?, 14, 14, 64) l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1], # l2 shape=(?, 7, 7, 64) strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l2 = tf.nn.dropout(l2, p_keep_conv) l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)) # l3a shape=(?, 7, 7, 128) l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1], # l3 shape=(?, 4, 4, 128) strides=[1, 2, 2, 1], padding=&apos;SAME&apos;) l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]]) # reshape to (?, 2048) l3 = tf.nn.dropout(l3, p_keep_conv) l4 = tf.nn.relu(tf.matmul(l3, w4)) l4 = tf.nn.dropout(l4, p_keep_hidden) pyx = tf.matmul(l4, w_o) return pyxmnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True) # 将照片转一维数组trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels # 定义训练、测试的图片、属性trX = trX.reshape(-1, 28, 28, 1) # 28x28x1 input imgteX = teX.reshape(-1, 28, 28, 1) # 28x28x1 input imgX = tf.placeholder(&quot;float&quot;, [None, 28, 28, 1]) # 张量 浮点数 4维矩阵Y = tf.placeholder(&quot;float&quot;, [None, 10])w = init_weights([3, 3, 1, 32]) # 3x3x1 conv, 32 outputsw2 = init_weights([3, 3, 32, 64]) # 3x3x32 conv, 64 outputsw3 = init_weights([3, 3, 64, 128]) # 3x3x32 conv, 128 outputsw4 = init_weights([128 * 4 * 4, 625]) # FC 128 * 4 * 4 inputs, 625 outputsw_o = init_weights([625, 10]) # FC 625 inputs, 10 outputs (labels)p_keep_conv = tf.placeholder(&quot;float&quot;) # 卷积核多项式乘法p_keep_hidden = tf.placeholder(&quot;float&quot;) # 隐藏的py_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden) # 调用模型函数cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) # 准确率train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost) # 训练的下降率predict_op = tf.argmax(py_x, 1) # 求行中最大#with tf.Session() as sess: # 初始化所有可变值 tf.global_variables_initializer().run() # 跑 for i in range(10): # 迭代10次 training_batch = zip(range(0, len(trX), batch_size), range(batch_size, len(trX)+1, batch_size)) # 训练批次 for start, end in training_batch: # 训练 sess.run(train_op, feed_dict=&#123;X: trX[start:end], Y: trY[start:end], p_keep_conv: 0.8, p_keep_hidden: 0.5&#125;) test_indices = np.arange(len(teX)) # 得到一个测试批 np.random.shuffle(test_indices) # 打乱测试集 test_indices = test_indices[0:test_size] print(i, np.mean(np.argmax(teY[test_indices], axis=1) == sess.run(predict_op, feed_dict=&#123;X: teX[test_indices], Y: teY[test_indices], p_keep_conv: 1.0, p_keep_hidden: 1.0&#125;))) # 打印预准率]]></content>
      <categories>
        <category>ComputerVision</category>
      </categories>
      <tags>
        <tag>Mnist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Ubuntu以及Web部署]]></title>
    <url>%2F2018%2F09%2F04%2F%E9%85%8D%E7%BD%AEUbuntu%E4%BB%A5%E5%8F%8AWeb%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[配置Ubuntu以及Web部署 注解 针对类似的文档有时效性，此文的编辑时间是2018/9/4 凌晨，预计半年内有效 这里的环境是Ubuntu16.04版本，使用的是腾讯云的学生机 说明： 我这里没有写使用docker部署的方法，如果你的服务器上计划只部署一个网站可以使用我的方法；如果你的服务器上计划部署多个网站，建议使用docker，不但迁移方便，配置方便，且网站之间相互独立，保证网站的稳定性 错误：Host key verification failed. 解决方式： 在客户端输入指令：vi ~/.ssh/known_hosts 删除所要连接的服务器IP的相关内容或者直接删掉known_hosts文件 原因： 使用SSH通信时，第一次客户端连接服务器，服务器会给客户端发送一个密钥，这个密钥会一直存储再服务器和客户端中，后面每次连接都需要验证密码和密钥，只有两者条件都匹配的时候，客户端才能正确连接服务器 设置root密码 方法： sudo passwd 把密码重复输入两次，看到设置成功即可 原因： 云服务器大多第一次使用或者重置操作系统后虽然在控制台已经设置了root密码，但还是要在命令行中重新设置 添加普通用户 方法： su root，进入root用户 adduser user-name，添加用户，其中user-name是要添加的用户名称 输入user-name用户的密码两次，后面的信息可以不用填写 输入Y确认即可 错误：add-apt-repository command not found 方法： sudo apt-get install python-software-properties sudo apt install software-properties-common sudo apt-get update python3.6的安装 警告： Ubuntu上面的python3.5不能删除，原因是例如yum以及其他一些组件依赖于python3.5的版本，系统内部自带的插件最好不要动 方法： sudo add-apt-repository ppa:jonathonf/python-3.6，添加第三方仓库，要输入root密码 sudo apt-get update sudo apt-get install python3.6，安装python3.6 输入python3.6 -V，查看是否安装成功 至此，Ubuntu系统中已经有了三个python版本（python2.7, python3.5, python3.6） 提高python3.6的优先级（就是输入python3会直接进入的是python3.6的环境，而不是Ubuntu自带的python3.5的环境） sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 sudo update-alternatives --config python3 输入python -V，查看是否优先级设置成功 输入 pip3，查看pip3是否正常 错误：ModuleNotFoundError: No module named &#39;gdbm&#39; 方法： sudo apt-get install python3.6-gdbm 错误：You will have to enable the component called &#39;universe&#39; 方法： apt install python3-pip 警告：12You are using pip version 8.1.1, however version 18.0 is available.You should consider upgrading via the &apos;pip install --upgrade pip&apos; command. 方法： pip3 install --upgrade pip 错误：12345Traceback (most recent call last): File &quot;/usr/bin/pip3&quot;, line 9, in &lt;module&gt; from pip import mainImportError: cannot import name &apos;main&apos; 方法： sudo vim /usr/bin/pip3，进入文件 insert键，进入vim的编辑模式原来的：123from pip import mainif __name__ == &apos;__main__&apos;: sys.exit(main()) 替换为：123from pip import __main__if __name__ == &apos;__main__&apos;: sys.exit(__main__._main()) 重新安装pip3方法： sudo python3 -m pip uninstall pip &amp;&amp; sudo apt install python3-pip --reinstall python3安装库 方法： pip3 install package-name，使用此命令安装对应的包 pip3 list，查看python3.6下安装的包 pip3 uninstall package-name，使用此命令卸载对应的包 mysql的安装 方法： sudo apt-get install mysql-server 设置mysql的密码 sudo apt-get install libmysqlclient-dev 验证安装是否成功： 输入mysql -u root -p，登陆数据库 输入密码，登陆成功就说明安装无误 python的Web部署 注解： 这里使用的是uwsgi、nginx和Django的2.1.1 方法： 安装django pip3 install django，等待安装即可 pip3 list，查看是否安装成功 安装uwsgi pip3 install uwsgi 如果出现：123In file included from plugins/python/python_plugin.c:1:0:plugins/python/uwsgi_python.h:2:20: fatal error: Python.h: No such file or directorycompilation terminated. 方法： 安装uwsgi 如果出现： 执行sudo apt-get install libpython3.6-dev，然后再安装uwsgi uwsgi --version，查看uwsgi是否安装成功 测试uwsgi cd /home，进入home的目录 vim test.py，新建文件 输入：123def application(env, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;,&apos;text/html&apos;)]) return [b&quot;Hello World&quot;] 安装： 测试uwsgi 按Esc键，输入:wq，回车 uwsgi --http :8000 --wsgi-file test.py 看到spawned uWSGI worker 1 (and the only) (pid: 22343, cores: 1)，打开浏览器访问你服务器的8000端口，页面有hello world表示uwsgi安装是正确的 安装nginx sudo apt-get install nginx 验证nginx是否安装成功 打开浏览器，输入your-server-IP:80，就是用80端口访问你的服务器 看到Welcome to nginx!说明nginx安装好了 配置nginx和uwsgi 在/etc/目录下新建uwsgi8000.ini，添加如下配置：（并把中文注释删掉）123456789101112socket = 127.0.0.1:8000master = true //主进程vhost = true //多站模式no-site = true //多站模式时不设置入口模块和文件workers = 2 //子进程数reload-mercy = 10 vacuum = true //退出、重启时清理文件max-requests = 1000 limit-as = 512buffer-size = 30000pidfile = /var/run/uwsgi9090.pid //pid文件，用于下面的脚本启动、停止该进程daemonize = /website/uwsgi9090.log 安装： 配置nginx和uwsgi 进入nginx的安装目录/etc/nginx，打开nginx.conf文件，修改server配置：（并把中文注释删掉）此配置在http大括号里面添加，注意缩进和配置12345678910111213server &#123; listen 80; server_name localhost; location / &#123; include uwsgi_params; uwsgi_pass 127.0.0.1:8000; //必须和uwsgi中的设置一致 uwsgi_param UWSGI_SCRIPT demosite.wsgi; //入口文件，即wsgi.py相对于项目根目录的位置，“.”相当于一层目录 uwsgi_param UWSGI_CHDIR /home/demosit; //项目根目录 index index.html index.htm; client_max_body_size 35m; &#125; &#125; 安装： 配置nginx和uwsgi uwsgi --ini /etc/uwsgi8000.ini &amp; /etc/nginx 新建项目 cd /home django-admin.py startproject demosite cd demosite/demosite vim settings.py，进入设置文件 找到ALLOWED_HOSTS，在方括号中加入你服务器的IP和localhost（使用单引号括起来） Esc键，:qw，保存退出 cd .. python3 manage.py runserver 0.0.0.0:8000 打开浏览器，输入your-seerver-IP+8000，能看到django的报错就好，先不管错误，这说明到此uwsgi和nginx配置成功，django部署基本完成，接下来就是调试了 错误：上边的设置无误，但就是访问不了的话 方法： 如果是云服务器的话，查看安全组的设置是否打开了这些端口让浏览器去访问 Docker部署 资源： Docker Doc 知乎的一篇教程：用Docker部署一个Web应用]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解计算机和程序]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%9B%BE%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%92%8C%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[计算机是怎样跑起来的 Hz（赫兹）是CPU频率单位，1秒发出1次时钟信号就是1Hz。100MHz=100*100万=1亿次/秒 IC是继承电路（Integrated Circuit）的简称 CPU的频率就是‘时钟信号’的电信号，可用来衡量CPU的速度 程序的流程：顺序执行、条件分支、循环 事件驱动：用户的操作等产生事件后，由事件决定程序的流程 OOP 继承、封装、多态 LAN 是Local Area Network 局域网 MAC地址就是计算机网卡中在出厂时被分配的不可更改的MAC地址（硬件的自己懂的话还是可以改的） IP地址到MAC地址的转换就是ARP 地址解析协议 程序是怎样跑起来的 CPU是由寄存器、控制器、运算器和时钟组成的 寄存器用来暂存指令、数据等处理对象、可看做内存的一部分 控制器负责把内存上的指令等读入寄存器，并根据指令的执行结果来控制整个计算机 运算器负责运算从内存读入寄存器的数据 时钟负责发出CPU开始计时的时钟信号 每个字节（1字节=8位） 每个CPU大概有20-100个寄存器，主要分为： 每个CPU中包含 名称 作用 1个 累加寄存器 存储执行运算的数据和运算后的数据 1个 标志寄存器 存储运算处理后的CPU状态 1个 程序计数器 存储下一条指令所在内存的地址 n个 基址寄存器 存储数据内存的起始地址 n个 变址寄存器 存储基址寄存器的相对地址 n个 通用寄存器 存储任意数据 1个 指令寄存器 存储指令 1个 栈寄存器 存储栈区域的起始地址 函数的调用机制 call指令会把调用函数后要执行的指令地址存储在名为栈的主存内 return命令的功能是吧保存在栈内的地址设定到程序的计数器中 通过地址和索引实现数组 通过基址寄存器和变址寄存器对主存区域进行划分 CPU会把基址寄存器+变址寄存器的值解释为实际查看的内存地址 变址寄存器的值就相当于高级变成语言程序中数组的索引功能 机器语言指令的主要类型和功能： 类型 功能 数据转送指令 寄存器和内存、内存和内存、寄存器和外围设备之间的数据读取操作 运算指令 用累加寄存器执行算数运算、逻辑运算、比较运算和移位运算 跳转指令 实现条件分支、循环、强制跳转等 call/return指令 函数的调用/返回调用前的地址 8位二进制数称为一个字节 移位运算和乘除运算的关系 左移一位就是对二进制数乘2 右移一位就是对二进制数除2 二进制数中正负的表示： 把最高位作为符号来使用，最高位就是符号位。 符号位是0表示正数，符号位是1表示负数 补数就是用正数来表示负数 原始二进制数转化其补数：所有位取反，然后加11的二进制数是00000001，取反后是11111110，加1后是11111111 逻辑右移和算数右移 逻辑右移是右移后前面空出来的高位用0补充 算数右移是右移后前面空出来的高位用右移前符号位的值来补足 浮点数是用符号、尾数、基数和指数来表示的小数 内存物理机制 DRAM SRAM ROM 磁盘缓存加快了磁盘的访问速度，指把从磁盘中读出的数据存储到内存空间中的方式 虚拟内存是指把磁盘的一部分作为假象的内存来使用 节约内存的编程方法 DLL文件是在程序运行时可以动态加载的文件 通过调用_stdcall来减小文件的大小 栈清理 磁盘的物理结构 扇区方式 把磁盘表面分成若干同心圆的空间就是磁道 把磁道按固定大小划分成的空间就是扇区 可变长方式 BIOS和引导]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Computer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[黑客与画家]]></title>
    <url>%2F2018%2F09%2F01%2F%E9%BB%91%E5%AE%A2%E4%B8%8E%E7%94%BB%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[本文大部分内容摘自书中 “书呆子”和“高智商”往往有着正相关关系 为什么书呆子不受欢迎 11岁以前，小孩子的生活往往由家长主导；11-17岁之间，孩子们逐渐把家庭生活作为寻常事了，他们在同伴中开辟了新的世界，并认为那才是最重要的。因此，11-17岁之间往往是书呆子人生中最糟糕的时期。就像小说《蝇王》中的情景，我们成年人就是这些“野蛮人”，我们自己创造的世界是一个残酷和愚蠢的世界。小孩子们之间相互欺负一方面是小孩子在产生良知之前，会觉得折磨就是一种快乐；另一方面是孩子们欺负书呆子是为了让自己感到好受一些（当你踩水的时候，你把水踩下去，你的身体就会被拖起来）。最受欢迎的小孩子并不欺负书呆子，他们不需要靠踩在书呆子身上来垫高自己；大部分的欺负来自处于下一等级的学生，那些焦虑的中间层。聪明的小孩往往在中学是不快乐的，他们有自己的兴趣，没有精力用来使自己受欢迎。 至于学校，不过是这个虚假环境中关住牲口的围栏。表面上，学校的使命是教育儿童。事实上，学校的真正目的是把儿童都关在同一个地方，以便大人们白天可以腾出手来把事情做完。我对这一点没有意见，在一个高度工业化的社会，对孩子不加管束，让他们四处乱跑，无疑是一场灾难。校园生活的真正问题是空虚。那么现行的体系中没有什么事是必然的，它现在的样子，大部分是没有人去改变他，也大概率没有人能去改变他。那么解决问题的办法就是考进一所好的大学，问题就大部分得到了解决，因为一所好的大学是思想上的“开放”。 然而直到我大学时读了此书后才明白这道理，才释然过往的岁月，痛苦是自己选择的，不是别人给的，只是和别人所想的或者追求的不同而已。 黑客与画家 黑客（这里以及本文的黑客只的都是指具有好玩、高智商和探索精神的软件从业者）与画家是有许多共同之处的。它们本质都不是在做研究，而是创作。程序员并不是黑客，它们有着本质的区别，大的公司里，程序员被当做技工，职责就是将产品经理的“构想”翻译成代码，做的再好也不过是流水线上一名熟练的操作工。而黑客是开发自己喜欢的项目并把它做到极致的优秀，并且大部分都喜欢为开源社区做贡献。因此三十万年薪的程序员不应该被称作程序员，而是码农。 不能说的话 优秀的作品往往来自其他人忽视的想法，而是最被忽视的想法就是那些被禁止的思想观点。 智力越高的人，越愿意去思考那些惊世骇俗的思想观点。这不仅仅因为聪明人本身很积极地寻找传统观念的漏洞，还因为传统观念对他们的束缚力很小，很容易摆脱。从他们的衣着上你就可以看出这一点：不受传统观念束缚的人，往往也不会穿流行的衣服。 在思想和言论之间划一条明确的界线。在心里无所不想，但是不一定要说出来。我就鼓励自己在心里默默思考那些最无法无天的想法。你的思想是一个地下组织，绝不要把那里发生的事情一股脑说给别人听。 i pensieri stretti &amp; il viso sciolto 守口如瓶，笑脸相迎 就是你要对每一个人微笑，但是不要说出自己的真实想法。 你不要让自己成为人群的一份子，而要尽可能地远离人群，观察正在发生的事情，特别注意那些被压制的思想观点。 你不仅要远距离观察人群，更要远距离观察自己。 良好的坏习惯 不服管教 另一条路 开发软件需要的程序员人数减少，不仅意味着省下更多的钱。正如《人月神话》一书中所指出的，向一个项目增加人手，往往会拖慢项目进程。随着参与人数的增加，人与人之间需要的沟通呈现指数式增长。 不要在半夜里发布代码，然后回家睡觉。 如何创造财富 如果你想致富，应该怎么做？我认为最好的办法就是自己创业，或者加入创业公司。 从经济学观点看，你可以把创业想象成一个压缩的过程，你的所有工作年份被压缩成了短短几年。你不再是低强度地工作四十年，而是以极限强度工作四年。在高技术领域，这种压缩的汇报尤其丰厚，工作效率越高，额外报酬越高。 防止垃圾邮件的一种方法 对单个词语进行贝叶斯判断。 设计者的品味 好设计是简单的设计 好设计是永不过时的设计 好设计是解决主要问题的设计 好设计是启发性的设计 好设计通常是有点趣味性的设计 好设计是艰苦的设计 好设计是看似容易的设计 好设计是对称的设计 好设计是模仿大自然的设计 好设计是一种再设计 好设计是能够复制的设计 好设计常常的独特的设计 好设计是成批出现的 好设计常常是大胆的设计 编程语言解析 你用什么语言并不重要，重要的是你对问题是否有正确的理解。代码以外的东西才是关键。 —— 一派胡言（不同语言的思想不同） 后面的没有再记录技术上的……评论： 公立学校就像监狱，他不真实，仅为了让学生待在应该待的位置。“你在学习中遇到的文字都是专门为考试设计的，目的就是为了出题，而不是为了讲清楚问题。” 程序是写出来给人看的，附带能在机器上运行。]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Think</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 使用指南]]></title>
    <url>%2F2018%2F07%2F15%2Fgit-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[创建账号创建一个Github或者Gitee账号 本地安装Git前往Git根据操作系统下载Git并安装到本地 ssh密钥本地生成秘钥右击桌面打开你的git bash,第一次使用Git时,需要先生成ssh 1ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; 生成的秘钥一般在你操作系统用户下的.ssh目录中 告诉本地系统将密钥告诉本地系统 1ssh-add ~/.ssh/id_rsa （此处如果出现Could not open a connection to your authentication agent. 则先执行 ssh-agent bash） 查看生成的公钥1cat ~/.ssh/id_rsa.pub 上传公钥 复制该公钥粘贴到你的Github或者Gitee的SSH公钥页面 使用SSH公钥可以让你在你的电脑和码云通讯的时候使用安全连接（Git的Remote要使用SSH地址） 配置User打开 git bash 需要配置: 12git config --global user.name &quot;Your Name&quot;git config --global user.email &quot;email@example.com&quot; 初始化本地Git仓库1git init 这条命令执行完毕后会多出一个.git文件夹 添加变更文件1git add . 提交文件1git commmit -am &quot;message&quot; 添加远程地址1git remote add origin git@github.com/你的github用户名/仓库名.git ssh测试1ssh -T git@gitee.com 显示Welcome to Gitee.com,你的用户名!说明ssh正确 可能出现的错误如果出现 fatal: remote origin already exists. 则执行 git remote rm origin 首次提交第一次提交可能会出现如下错误error: failed to push some refs to &#39;https://gitee.com/tomFat/interview.git&#39;所以需要在提交前执行git pull 合并两个版本库1git pull origin master --allow-unrelated-histories 向Github或Gitee推送1git push -u origin master git命令速查]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git手册]]></title>
    <url>%2F2018%2F07%2F15%2FGit%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[图形化 gitk 内建的图形化 git git config color.ui true 彩色的 git 输出 git config format.pretty oneline 显示历史记录时，只显示一行注释信息 git add -i 交互地添加文件至缓存区 远程 git remote add origin &lt;url&gt; 添加远程地址 git remote remove &lt;name&gt; 删除关联的远程版本库 git push -u origin master 将master分支推送到origin远程仓库 git push -u origin master -f 强制将master分支推送到远程仓库origin 慎用图解 git结构 工作区：就是你在电脑里能看到的目录 暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index） 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库 分支 error error: src refspec master does not match any. 空目录不能提交；至少有一次commit]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解算法]]></title>
    <url>%2F2018%2F07%2F09%2F%E5%9B%BE%E8%A7%A3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[二分查找123456789101112131415161718def binary_search(list, item): low = 0 high = len(list) - 1 while low &lt;= high: mid = int((low + high) / 2) # 中间的 guess = list[mid] if guess == item: return mid if guess &gt; item: high = mid - 1 else: low = mid + 1 return Nonemy_list = [1, 3, 5, 7, 9]print(binary_search(my_list, 3))print(binary_search(my_list, -1)) 最短路径问题：使用广度优先搜索，使用图来建立问题模型 假设你经营着一个芒果农场,需要寻找芒果销售商,以便将芒果卖给他。在Facebook,你与芒果销售商有联系吗?为此,你可在朋友中查找。 查找最短路径，广度优先搜索可回答两类问题 第一类问题:从节点A出发,有前往节点B的路径吗?(在你的人际关系网中,有芒果销 售商吗?) 第二类问题:从节点A出发,前往节点B的哪条路径最短?(哪个芒果销售商与你的关系 最近?) 123456789graph = &#123;&#125;graph["you"] = ["alice", "bob", "claire"]graph["bob"] = ["anuj", "peggy"]graph["alice"] = ["peggy"]graph["claire"] = ["thom", "jonny"]graph["anuj"] = []graph["peggy"] = []graph["thom"] = []graph["jonny"] = [] 123456789101112131415from collection import dequesearch_queue = deque() # 创建一个队列search_queue += graph['you'] # 将你的邻居都加入到这个搜索队列中while search_queue: # 只要队列不为空 person = search_queue.popleft() # 就取出其中的第一个人 if person_is_seller(person): # 检查这个人是否是芒果商 print('&#123;&#125; is a mango seller!'.format('person') # 是芒果商 return True else: search_queue += graph[person] # 不是芒果销售商。将这个人的朋友都加入搜索队列return False # 如果到达了这里，就说明队列中没人是芒果商def person_is_seller(name): return name[-1] == 'm' 将一个人加入对列的时间是固定的O(1)，总时间就为O(人数)。所以广度优先搜索的运行时间为O(人数+边数) 狄克斯特拉算法 问题：其中每个数字表示的都是时间,单位分钟。为找出从起点到终点耗时最短的路径,你将使用 狄克斯特拉算法。 123456graph TD A[起点] --&gt;|6| B(A) A[起点] --&gt;|2| C(B) C --&gt;|3| B B --&gt;|1| D(终点) C --&gt;|5| D(终点) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657graph = &#123;&#125;graph["you"] = ["alice", "bob", "claire"]# 使用散列表graph["start"] = &#123;&#125;graph["start"]["a"] = 6graph["start"]["b"] = 2graph["a"] = &#123;&#125;graph["a"]["fin"] = 1graph["b"] = &#123;&#125;graph["b"]["a"] = 3graph["b"]["fin"] = 5graph["fin"] = &#123;&#125; # 终点没有任何邻居infinity = float("inf")# 创建开销表infinity = float("inf")costs = &#123;&#125;costs["a"] = 6costs["b"] = 2costs["fin"] = infinity# 创建这个散列表parents = &#123;&#125;parents["a"] = "start"parents["b"] = "start"parents["fin"] = None# 存储处理过的节点processed = []# 代码node = find_lowest_cost_node(costs) # 在未处理的节点中找出开销量最小的节点while node is not None: # 这个while循环在所有节点都被处理过后结束 cost = costs[node] neighbors = graph[node] for n in neighbors.key(): # 遍历当前节点 new_cost = cost + neighbors[n] if costs[n] &gt; new_costs: # 如果经过当前节点前往该邻居更近 costs[n] = new_costs # 就更新该邻居的开销 parents[n] = node # 同时将该邻居的父节点设置为当前节点 processed.append(node) # 将当前节点标记为处理过 node = find_lowest_cost_node(costs) # 找出接下来要处理的节点def find_lowest_cost_node(costs): lowest_cost = float("inf") lowest_cost_node = None for node in costs: # 遍历所有的节点 cost = costs[node] if cost &lt; lowest_cost and node not in processed: # 如果当前节点的开销更低且未处理过 lowest_cost = cost # 就将其视为开销最低的节点 lowest_cost_node = node return lowest_cost_node 第一步：找出最便宜的节点 第二步：计算经起点的下一节点前往其各个节点所需的时间 第三步：重复 重复第一步:找出可在最短时间内前往的节点。你对节点B执行了第二步,除节点B外,可在最短时间内前往的节点是节点A。 重复第二步:更新节点A的所有邻居的开销。 算法异同： 使用广度优先搜索来查找两点之间的最短路径，这里的“最短路径”的意思是段数最少 在狄克斯特拉算法中,你给每段都分配了一个数字或权重,因此狄克斯特拉算法找出的是总权重最小的路径。 使用： 要计算非加权图中的最短路径,可使用广度优先搜索。要计算加权图中的最短路径,可使用狄克斯特拉算法。 狄克斯特拉算法只适用于有向无环图 贪婪算法 贪婪算法并不能得到最优解，因为每步都是局部最优解 动态规划 使用动态规划时，要么考虑拿走整件商品，要么考虑不拿，而没法判断该不该拿走商品的一部分。 问题： 假设你要去伦敦度假，假期两天，但你想去游览的地方很多。你没法前往每个地方游览，因此你列个单子。 名胜 时间 评分 威斯敏斯特教堂 0.5天 7 环球剧场 0.5天 6 英国国家美术馆 1天 9 大英博物馆 2天 9 圣保罗大教堂 0.5天 8 思路： 使用网格，先建立空网格 0.5 1 1.5 2 威斯敏斯特教堂 环球剧场 英国国家美术馆 大英博物馆 圣保罗大教堂 网格填充结果 0.5 1 1.5 2 威斯敏斯特教堂（w） 7（w） 7（w） 7（w） 7（w） 环球剧场（g） 7（w） 13（wg） 13（wg） 13（wg） 英国国家美术馆（n） 7（w） 13（wg） 16（wn） 22（wgn） 大英博物馆（b） 7（w） 13（wg） 16（wn） 22（wgn） 圣保罗大教堂（s） 8（s） 15（ws） 21（wgs） 24（wns） 填充表格从左到右，然后下一行，重复。 最长公共子串 问题：HISH和FISH的最长公共子串是？ 思路： 画表格 H I S H F I S H 填充表格 H I S H F 0 0 0 0 I 0 1 0 0 S 0 0 2 0 H 0 0 0 3 若果横纵字母相同则对应空格为1加上空格左上角邻居的值 12345# 实现这个公式的伪代码if word_a[i] == word_b[j]: # 两个字母相同 cell[i][j] = cell[i-1][j-1] + 1else: # 两个字母不同 cell[i][j] = 0 最长公共子序列之解决方案 最长公共子序列：两个单词都有的序列包含的字母数 问题：求fish和fosh的最长公共子序列 思路： 画网格 H O S H F I S H 填充表格 H O S H F 1 1 1 1 I 1 1 1 1 S 1 1 2 2 H 1 1 2 3 如果两个字母不同，就选择上方和左邻居中较大的那个如果两个字母相同，就将当前单元格的值设为左上方单元格的值加1 1234if word_a[i] == word_b[j]: # 两个字母相同 cell[i][j] = cell[i-1][j-1] + 1else: # 两个字母不同 cell[i][j] = max(cell[i-1][j], cell[i][j-1])]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序]]></title>
    <url>%2F2018%2F07%2F09%2F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[算法性能分析 分类 内排序：带排序的记录全部保存在内存中 外排序：针对外存（磁盘等）数据的排序 基本操作： 比较关键码的操作，通过这种操作确定数据的顺序关系 移动数据记录的操作，用于调整记录的位置顺序 基于关键码的比较的排序问题，时间复杂度是O(n log n)，也就是说任何算法都不能优于O(n log n) 稳定的排序算法：对于待排序的序列里任何一对排序码相同的记录Ri和Rj，在排序后后的序列中Ri和Rj的前后顺序不变。也就是说，稳定的排序算法能够维持序列中所有排序码相同记录的相对位置。 插入排序 思想：1.对一段连续表进行排序时，可以把序列看为两段，左边为排好序的，右边为待排序的。2.每次仅考虑右边序列的第一个元素在左边序列的插入位置。 12345678def insert_sort(lst): for i in range(1, len(lst)): # 开始时片段[0:1]已排好序 x = lst[i] j = i while j &gt; 0 and lst[j-1] &gt; x: lst[j] = lst[j-1] # 反序逐个后移元素，确定插入位置 j -= 1 lst[j] = x 空间复杂度：O(1)，计算中只用了两个辅助变量 时间复杂度：这个算法具有适应性。 最坏情况：O(n^2)，关键码的比较次数由内层循环while决定，为n-1次，while循环总执行n(n-1)/2次，记录移动次数包括内层循环外边的两次和循环中的一些次，为2(n-1)次，最终总共2(n-1)+n(n-1)/2次。 最好情况：O(n) 平均情况：O(n^2)，一个元素可能插入一排序序列里的任何位置，假设插入个位置的概率相等，内层循环每次的迭代次数就是j/2，求和后就是n*(n-1)/4次。 选择排序 思想：1.维护需要考虑的所有记录中最小的i个记录的已排序序列；2.每次从剩余未排序的记录中选取关键码最小的记录，将其放在已排序序列记录的后面，作为序列的第i+1个记录；3.执行到尚未排序序列里只有一个元素时，最后一个元素必然是整个序列中最大的，只需将其放在已排序的记录后排序完成。（这里以升序为例） 12345678def select_sort(lst): for i in range(len(lst)-1): # 只需循环len(lst)-1次 k = i for j in range(i, len(lst)): # k是已知最小元素的位置 if lst[j] &lt; lst[k]: k = j if i != k: # lst[k]是确定的最小元素，检查是否需要交换 lst[i], lst[k] = lst[k], lst[i] 空间复杂度：O(1)，计算中只用了几个辅助变量 时间复杂度：两个for循环按固定方式重复执行固定次，比较的次数总是1+2+3+…=n(n-1)/2。移动次数为2(n-1)次。此算法没有适应性。 最坏情况：O(n^2) 最好情况：O(n^2) 平均情况：O(n^2) 交换排序（又名起泡排序） 思想：1.一个序列中的记录没排好序，那么其中一定有逆序存在。2.通过不断减少序列中的逆序，最终得到排序序列。基本操作就是发现相邻的逆序对时就交换他们，通过反复比较和交换，最终完成整个序列的排序。 1234567891011121314151617def bubble_sort(lst): for i in range(len(lst)): for j in range(1, len(lst)-i): if lst[j-1] &gt; lst[j]: lst[j-1], lst[j] = lst[j], lst[j-1]# 改进后（当序列中没有逆序对时，立刻结束循环） def bubble_sort(lst): for i in range(len(lst)): found = False for j in range(1, len(lst)-i): if lst[j-1] &gt; lst[j]: lst[j-1], lst[j] = lst[j], lst[j-1] found = True if not found: break 空间复杂度：O(1)，计算中只用了几个辅助变量 时间复杂度：此算法没有适应性。改进后的算法具有适应性。 最坏情况：O(n^2) 最好情况：O(n^2) 平均情况：改进后的为 O(n) 快速排序 思想（基本过程）：1.选择一种标准，把被排序列中的记录按某种标准分为大小两组，较小一组的记录应该排在前面2.采用同样的方式，递归的分别划分得到这两组记录，并继续递归地划分下去3.划分得到越来越小的组，直到每个记录组中最多包含一个记录时，整个序列的排序就完成了。（一次）划分的实现：取出第一个记录作为标准，设其为R，已知的小记录积累在左边，大记录积累在右边，中间是尚未检查的记录。取出记录R使表左边出现了一个空位。这时从右端开始检查，就可以利用这个空位，把发现的第一个小记录（小于R的记录）移到左边。这一迁移操作也导致右边留下一个空位，可供存放在左边发现的一个大记录。算法：1.利用两个下标变量i和j，其初值分别是序列中第一个和最后一个记录的位置。然后取出第一个和记录R，设其排序码为K，作为划分标准。2.交替执行下面两套操作：2.1.从右向左逐一检查j一边的记录，检查中j值不断减一，直至找到第一个关键字小于K的记录，将其存入i所指的空位。注意，移动记录后位置j变成空位，i值加一后只想下一需要检查的记录。2.2.从左向右逐一检查i一边的记录，检查i值不断加一，直至找到第一个关键字大于K的记录并将其存入j所指的空位。转做上面操作。3.重复交替上述两套操作，直到i不再小于j为止。由于第一种操作中j值不断减小，第二种操作中i值不断增大，划分一定能完成。一次划分完成后对两边子序列按照同样方式递归处理。快速排序算法的执行形成了一种二叉树形式的递归调用。 123456789101112131415161718192021222324def quick_sort(lst): qsort_rec(lst, 0, len(lst)-1)def qsort_rec(lst, l, r): if l &gt;= r: # 分段无记录或只有一个记录 return i = l j = r pivot = lst[i] # lst[i] 是初始空位 while i &lt; j: # 找pivot的最终位置 while i &lt; j and lst[j] &gt;= pivot: j -= 1 # 用j向左扫描找小于pivot的记录 if i &lt; j: lst[i] = lst[j] i += 1 # 小记录移到左边 while i &lt; j and lst[i] &lt;= pivot: i += 1 # 用i向右扫描找大于pivot的记录 if i &lt; j: lst[j] = lst[i] j -= 1 # 大记录移到右边 lst[i] = pivot # 将pivot存入其最终位置 qsort_rec(lst, l, i-1) # 递归处理左半区间 qsort_rec(lst, i+1, r) # 递归处理右半区间 空间复杂度：最坏情况O(n)，与递归深度有关 时间复杂度： 最坏情况：O(n^2) 平均情况：O(n log n) 另外一种简单实现 123456789101112131415def quick_sort(lst): def qsort(lst, begin, end): if begin &gt;= end: return pivot = lst[begin] i = begin for j in range(begin+1, end+1): if lst[j] &lt; pivot: # 发现一个小元素 i += 1 lst[i], lst[j] = lst[j], lst[i] # 小元素换位 lst[begin], lst[i] = lst[i], lst[begin] # 枢纽元换位 qsort(lst, begin, i-1) qsort(lst, i+1, end) qsort(lst, 0, len(lst)-1) 归并排序 归并是一种典型的序列操作，其工作是把两个或更多有序序列合并为一个有序序列。思想：1.初始时，把待排序序列中的n个记录看成n个有序子序列（因为一个记录的序列总是排好序的），每个子序列的长度均为12.把当时序列组里的有序子序列两两归并，完成一遍后序列组里的排序序列个数减半，每个子序列的长度加倍3.对加长的有序子序列重复上面的操作，最终得到一个长度为n的有序序列这种方法为二路归并法，也可使用多路归并法。操作：1.最下层：实现表中相邻的一对有序序列的归并操作，将归并的结果存入另一个顺序表里的相同位置2.中间层：基于操作1（一对序列的归并操作），实现对整个表里顺序各对有序序列的归并，完成一遍归并，各对序列的归并结果顺序存入另一个表里的相同位置分段3.最高层：在两个顺序表之间往复执行操作2，完成一遍归并后交换两个表的地位，然后再重复操作2的工作，直至整个表里只有一个有序序列时排序完成 12345678910111213141516171819202122232425262728293031323334353637383940def merge(lfrom, lto, low, mid, high): i, j, k = low, mid, low while i &lt; mid and j &lt; high: # 反复复制两分段首记录中较小的 if lfrom[i] &lt;= lfrom[j]: lto[k] = lfrom[i] i += 1 else: lto[k] = lfrom[j] j += 1 k += 1 while i &lt; mid: # 复制第一段剩余记录 lto[k] = lfrom[i] i += 1 k += 1 while j &lt; high: # 复制第二段剩余记录 lto[k] = lfrom[j] j += 1 k += 1def merge_pass(lfrom, lto, llen, slen): i = 0 while i + 2 * slen &lt; llen: # 归并长slen的两段 merge(lfrom, lto, i, i + slen, i + 2 * slen) i += 2 if i + slen &lt; llen: # 剩下的两段，后段长度小于slen merge(lfrom, lto, i, i + slen, llen) else: # 只剩下一段，复制到表lto for j in range(i, llen): lto[j] = lfrom[j]def merge_sort(lst): slen, llen = 1, len(lst) templst = [None] * llen while slen &lt; llen: merge_pass(lst, templst, llen, slen) slen *= 2 merge_pass(templst, lst, llen, slen) # 结果存回原位 slen *= 2 空间复杂度：O(n) 时间复杂度：O(n log n)，有序子序列的长度将为2^k，完成整个排序需要做的归并遍数不会多于log 2 ^n + 1，总的比较次数和移动次数都为O(n log n) 蒂姆排序 Python中的内置函数sort蒂姆排序是一种基于归并技术的稳定排序算法，结合了归并排序和插入排序技术，该算法具有适应性。时间复杂度：最坏情况是O(n log n)，空间复杂度：最坏情况是O(n)，最坏情况下需要n/2各工作空间 几种排序的比较 排序算法 最坏情况时间复杂度 平均情况时间复杂度 最好情况时间复杂度 空间复杂度 稳定性 适应性 简单插入排序 O(n^2) O(n^2) O(n) O(1) 是 是 二分插入排序 O(n^2) O(n^2) O(n log n) O(1) 是 是 表插入排序 O(n^2) O(n^2) O(n) O(1) 是 是 直接选择排序 O(n^2) O(n^2) O(n^2) O(1) 否 否 堆选择排序 O(n log n) O(n log n) O(n log n) O(1) 否 否 起泡排序 O(n^2) O(n^2) O(n) O(1) 是 是 快速排序 O(n^2) O(n log n) O(n log n) O(log n) 否 否 归并排序 O(n log n) O(n log n) O(n log n) O(n) 是 否 蒂姆排序(sort函数) O(n log n) O(n log n) O(n) O(n) 是 是]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2014%2F12%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
